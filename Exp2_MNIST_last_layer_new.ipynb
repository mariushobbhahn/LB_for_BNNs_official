{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi\n",
    "#using a GeForce GTX1080 Ti for reproducibility for all timing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from math import *\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import KFAC, DiagHessian, DiagGGNMC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy\n",
    "from tqdm import tqdm, trange\n",
    "import pytest\n",
    "from LB_utils import * \n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s = 123\n",
    "np.random.seed(s)\n",
    "torch.manual_seed(s)\n",
    "torch.cuda.manual_seed(s)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "cuda status:  True\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cuda_status = torch.cuda.is_available()\n",
    "print(\"device: \", device)\n",
    "print(\"cuda status: \", cuda_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define network\n",
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.Conv2d(16, 32, 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(4 * 4 * 32, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TRAIN_MNIST = 128\n",
    "BATCH_SIZE_TEST_MNIST = 128\n",
    "MAX_ITER_MNIST = 6\n",
    "LR_TRAIN_MNIST = 10e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/marius/data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91fbaa0b9864247bc36d876a3abc6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marius/data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /home/marius/data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/marius/data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3a47b7d64b443fafbda786d8d4f537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marius/data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /home/marius/data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/marius/data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7a09b9ee72493f8ca2a41aa719d6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marius/data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/marius/data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/marius/data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfaab607516c4d1bb7b1f67478127fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marius/data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/marius/data/mnist/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MNIST_transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "MNIST_train = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=MNIST_transform)\n",
    "\n",
    "MNIST_train_loader = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_train,\n",
    "    batch_size=BATCH_SIZE_TRAIN_MNIST,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "MNIST_test = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform)\n",
    "\n",
    "MNIST_test_loader = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = ConvNet().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mnist_train_optimizer = torch.optim.Adam(mnist_model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "MNIST_PATH = \"pretrained_weights/MNIST_pretrained_10_classes_last_layer_s{}.pth\".format(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training routine\n",
    "\n",
    "def train(model, train_loader, optimizer, max_iter, path, verbose=True):\n",
    "    max_len = len(train_loader)\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            output = model(x)\n",
    "\n",
    "            accuracy = get_accuracy(output, y)\n",
    "\n",
    "            loss = loss_function(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if verbose and batch_idx % 50 == 0:\n",
    "                print(\n",
    "                    \"Iteration {}; {}/{} \\t\".format(iter, batch_idx, max_len) +\n",
    "                    \"Minibatch Loss %.3f  \" % (loss) +\n",
    "                    \"Accuracy %.0f\" % (accuracy * 100) + \"%\"\n",
    "                )\n",
    "\n",
    "    print(\"saving model at: {}\".format(path))\n",
    "    torch.save(mnist_model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0; 0/469 \tMinibatch Loss 2.316  Accuracy 8%\n",
      "Iteration 0; 50/469 \tMinibatch Loss 0.435  Accuracy 89%\n",
      "Iteration 0; 100/469 \tMinibatch Loss 0.307  Accuracy 90%\n",
      "Iteration 0; 150/469 \tMinibatch Loss 0.188  Accuracy 95%\n",
      "Iteration 0; 200/469 \tMinibatch Loss 0.215  Accuracy 94%\n",
      "Iteration 0; 250/469 \tMinibatch Loss 0.159  Accuracy 93%\n",
      "Iteration 0; 300/469 \tMinibatch Loss 0.119  Accuracy 97%\n",
      "Iteration 0; 350/469 \tMinibatch Loss 0.118  Accuracy 97%\n",
      "Iteration 0; 400/469 \tMinibatch Loss 0.044  Accuracy 100%\n",
      "Iteration 0; 450/469 \tMinibatch Loss 0.076  Accuracy 98%\n",
      "Iteration 1; 0/469 \tMinibatch Loss 0.159  Accuracy 95%\n",
      "Iteration 1; 50/469 \tMinibatch Loss 0.071  Accuracy 97%\n",
      "Iteration 1; 100/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 1; 150/469 \tMinibatch Loss 0.074  Accuracy 98%\n",
      "Iteration 1; 200/469 \tMinibatch Loss 0.140  Accuracy 96%\n",
      "Iteration 1; 250/469 \tMinibatch Loss 0.068  Accuracy 97%\n",
      "Iteration 1; 300/469 \tMinibatch Loss 0.065  Accuracy 97%\n",
      "Iteration 1; 350/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 1; 400/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 1; 450/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 2; 0/469 \tMinibatch Loss 0.018  Accuracy 100%\n",
      "Iteration 2; 50/469 \tMinibatch Loss 0.068  Accuracy 98%\n",
      "Iteration 2; 100/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 2; 150/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 2; 200/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 2; 250/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 2; 300/469 \tMinibatch Loss 0.114  Accuracy 97%\n",
      "Iteration 2; 350/469 \tMinibatch Loss 0.031  Accuracy 99%\n",
      "Iteration 2; 400/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 2; 450/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 3; 0/469 \tMinibatch Loss 0.045  Accuracy 98%\n",
      "Iteration 3; 50/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 3; 100/469 \tMinibatch Loss 0.135  Accuracy 96%\n",
      "Iteration 3; 150/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 3; 200/469 \tMinibatch Loss 0.050  Accuracy 98%\n",
      "Iteration 3; 250/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 3; 300/469 \tMinibatch Loss 0.014  Accuracy 99%\n",
      "Iteration 3; 350/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 3; 400/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 3; 450/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 4; 0/469 \tMinibatch Loss 0.123  Accuracy 98%\n",
      "Iteration 4; 50/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 4; 100/469 \tMinibatch Loss 0.071  Accuracy 98%\n",
      "Iteration 4; 150/469 \tMinibatch Loss 0.063  Accuracy 98%\n",
      "Iteration 4; 200/469 \tMinibatch Loss 0.120  Accuracy 98%\n",
      "Iteration 4; 250/469 \tMinibatch Loss 0.075  Accuracy 99%\n",
      "Iteration 4; 300/469 \tMinibatch Loss 0.011  Accuracy 100%\n",
      "Iteration 4; 350/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 4; 400/469 \tMinibatch Loss 0.038  Accuracy 98%\n",
      "Iteration 4; 450/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 5; 0/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 5; 50/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 5; 100/469 \tMinibatch Loss 0.047  Accuracy 99%\n",
      "Iteration 5; 150/469 \tMinibatch Loss 0.021  Accuracy 100%\n",
      "Iteration 5; 200/469 \tMinibatch Loss 0.029  Accuracy 98%\n",
      "Iteration 5; 250/469 \tMinibatch Loss 0.025  Accuracy 99%\n",
      "Iteration 5; 300/469 \tMinibatch Loss 0.046  Accuracy 99%\n",
      "Iteration 5; 350/469 \tMinibatch Loss 0.052  Accuracy 99%\n",
      "Iteration 5; 400/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 5; 450/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "saving model at: pretrained_weights/MNIST_pretrained_10_classes_last_layer_s123.pth\n"
     ]
    }
   ],
   "source": [
    "train(mnist_model, MNIST_train_loader, mnist_train_optimizer, MAX_ITER_MNIST, MNIST_PATH, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: pretrained_weights/MNIST_pretrained_10_classes_last_layer_s123.pth\n",
      "Batch 0/79 \tAccuracy 100%\n",
      "Batch 10/79 \tAccuracy 98%\n",
      "Batch 20/79 \tAccuracy 98%\n",
      "Batch 30/79 \tAccuracy 98%\n",
      "Batch 40/79 \tAccuracy 98%\n",
      "Batch 50/79 \tAccuracy 100%\n",
      "Batch 60/79 \tAccuracy 100%\n",
      "Batch 70/79 \tAccuracy 99%\n",
      "overall test accuracy on MNIST: 98.65 %\n"
     ]
    }
   ],
   "source": [
    "#predict in distribution\n",
    "MNIST_PATH = \"pretrained_weights/MNIST_pretrained_10_classes_last_layer_s{}.pth\".format(s)\n",
    "#MNIST_PATH = \"pretrained_weights/MNIST_pretrained_10_classes_last_layer.pth\"\n",
    "\n",
    "mnist_model = ConvNet().to(device)\n",
    "print(\"loading model from: {}\".format(MNIST_PATH))\n",
    "mnist_model.load_state_dict(torch.load(MNIST_PATH))\n",
    "mnist_model.eval()\n",
    "\n",
    "acc = []\n",
    "\n",
    "max_len = len(MNIST_test_loader)\n",
    "for batch_idx, (x, y) in enumerate(MNIST_test_loader):\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    output = mnist_model(x)\n",
    "\n",
    "    accuracy = get_accuracy(output, y)\n",
    "    if batch_idx % 10 == 0:\n",
    "        print(\n",
    "            \"Batch {}/{} \\t\".format(batch_idx, max_len) + \n",
    "            \"Accuracy %.0f\" % (accuracy * 100) + \"%\"\n",
    "        )\n",
    "    acc.append(accuracy)\n",
    "\n",
    "avg_acc = np.mean(acc)\n",
    "print('overall test accuracy on MNIST: {:.02f} %'.format(avg_acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TEST_FMNIST = 128\n",
    "BATCH_SIZE_TEST_KMNIST = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/marius/data/fmnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d6cc08a8d0433b97387afe988c1b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marius/data/fmnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/marius/data/fmnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/marius/data/fmnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24ad831bc44430b9162b48719f38d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marius/data/fmnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/marius/data/fmnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/marius/data/fmnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b1139c97944da88fcaf008ca5dddc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marius/data/fmnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/marius/data/fmnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/marius/data/fmnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928369df9c36467c950a98dc13aa162e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marius/data/fmnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/marius/data/fmnist/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FMNIST_test = torchvision.datasets.FashionMNIST(\n",
    "        '~/data/fmnist', train=False, download=True,\n",
    "        transform=MNIST_transform)   #torchvision.transforms.ToTensor())\n",
    "\n",
    "FMNIST_test_loader = torch.utils.data.DataLoader(\n",
    "    FMNIST_test,\n",
    "    batch_size=BATCH_SIZE_TEST_FMNIST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz to /home/marius/data/kmnist/KMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0697ed9bd6b84d7eab2dd0eb25e442c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18165135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marius/data/kmnist/KMNIST/raw/train-images-idx3-ubyte.gz to /home/marius/data/kmnist/KMNIST/raw\n",
      "\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz to /home/marius/data/kmnist/KMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebdc72bde3f4fd6959f6580d31a0dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marius/data/kmnist/KMNIST/raw/train-labels-idx1-ubyte.gz to /home/marius/data/kmnist/KMNIST/raw\n",
      "\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz to /home/marius/data/kmnist/KMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41025de05bdd4b1796a45d31436605c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3041136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marius/data/kmnist/KMNIST/raw/t10k-images-idx3-ubyte.gz to /home/marius/data/kmnist/KMNIST/raw\n",
      "\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz to /home/marius/data/kmnist/KMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c97d72f8d9416d94eba477a0a0d8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marius/data/kmnist/KMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/marius/data/kmnist/KMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KMNIST_test = torchvision.datasets.KMNIST(\n",
    "        '~/data/kmnist', train=False, download=True,\n",
    "        transform=MNIST_transform)\n",
    "\n",
    "KMNIST_test_loader = torch.utils.data.DataLoader(\n",
    "    KMNIST_test,\n",
    "    batch_size=BATCH_SIZE_TEST_KMNIST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load notMNIST\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from matplotlib.pyplot import imread\n",
    "from torch import Tensor\n",
    "\n",
    "\"\"\"\n",
    "Loads the train/test set. \n",
    "Every image in the dataset is 28x28 pixels and the labels are numbered from 0-9\n",
    "for A-J respectively.\n",
    "Set root to point to the Train/Test folders.\n",
    "\"\"\"\n",
    "\n",
    "# Creating a sub class of torch.utils.data.dataset.Dataset\n",
    "class notMNIST(Dataset):\n",
    "\n",
    "    # The init method is called when this class will be instantiated\n",
    "    def __init__(self, root, transform):\n",
    "        \n",
    "        #super(notMNIST, self).__init__(root, transform=transform)\n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "        Images, Y = [], []\n",
    "        folders = os.listdir(root)\n",
    "\n",
    "        for folder in folders:\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            for ims in os.listdir(folder_path):\n",
    "                try:\n",
    "                    img_path = os.path.join(folder_path, ims)\n",
    "                    Images.append(np.array(imread(img_path)))\n",
    "                    Y.append(ord(folder) - 65)  # Folders are A-J so labels will be 0-9\n",
    "                except:\n",
    "                    # Some images in the dataset are damaged\n",
    "                    print(\"File {}/{} is broken\".format(folder, ims))\n",
    "        data = [(x, y) for x, y in zip(Images, Y)]\n",
    "        self.data = data\n",
    "        self.targets = torch.Tensor(Y)\n",
    "\n",
    "    # The number of items in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # The Dataloader is a generator that repeatedly calls the getitem method.\n",
    "    # getitem is supposed to return (X, Y) for the specified index.\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index][0]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        # Input for Conv2D should be Channels x Height x Width\n",
    "        img_tensor = Tensor(img).view(1, 28, 28).float()\n",
    "        label = self.data[index][1]\n",
    "        return (img_tensor, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png is broken\n",
      "File A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png is broken\n"
     ]
    }
   ],
   "source": [
    "#root = os.path.abspath('~/data')\n",
    "root = os.path.expanduser('~/data')\n",
    "\n",
    "# Instantiating the notMNIST dataset class we created\n",
    "notMNIST_test = notMNIST(root=os.path.join(root, 'notMNIST_small'),\n",
    "                               transform=MNIST_transform)\n",
    "\n",
    "# Creating a dataloader\n",
    "notMNIST_test_loader = torch.utils.data.dataloader.DataLoader(\n",
    "                            dataset=notMNIST_test,\n",
    "                            batch_size=BATCH_SIZE_TEST_KMNIST,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplace import Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#la_full = Laplace(mnist_model, 'classification', \n",
    "#                     subset_of_weights='last_layer', \n",
    "#                     hessian_structure='full',\n",
    "#                     prior_precision=5e-4) #5e-4 # Choose prior precision according to weight decay\n",
    "#la_full.fit(MNIST_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_diag = Laplace(mnist_model, 'classification', \n",
    "                     subset_of_weights='last_layer', \n",
    "                     hessian_structure='diag',\n",
    "                     prior_precision=5e-4) # 5e-4 # Choose prior precision according to weight decay\n",
    "la_diag.fit(MNIST_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0090081e-06 1.7046767e-05 7.6196459e-04 1.8966886e-04 3.2080398e-09\n",
      " 2.3133402e-06 1.1283215e-08 9.9884719e-01 7.7906304e-07 1.7698725e-04]\n"
     ]
    }
   ],
   "source": [
    "## predict samples with diag & mc\n",
    "MNIST_test0 = next(iter(MNIST_test_loader))\n",
    "#print(MNIST_test0)\n",
    "print(la_diag(MNIST_test0[0].cuda(), link_approx='mc', n_samples=100).cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.3495666e-09 7.4650919e-10 2.3257318e-05 1.1988027e-05 8.6652498e-11\n",
      " 2.3607445e-08 1.4620609e-14 9.9994838e-01 5.4076111e-08 1.6361808e-05]\n"
     ]
    }
   ],
   "source": [
    "print(la_diag(MNIST_test0[0].cuda(), link_approx='bridge').cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2521812e-02 2.5860280e-01 5.1707494e-01 2.7376791e-03 7.6537428e-04\n",
      " 3.6871754e-02 1.6989063e-01 6.8310334e-04 8.3245308e-04 1.9439238e-05]\n"
     ]
    }
   ],
   "source": [
    "## predict samples with diag & mc\n",
    "FMNIST_test0 = next(iter(FMNIST_test_loader))\n",
    "#print(MNIST_test0)\n",
    "print(la_diag(FMNIST_test0[0].cuda(), link_approx='mc', n_samples=100).cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9054649e-03 4.8668459e-03 9.2423356e-01 1.5070109e-03 7.0980570e-04\n",
      " 1.2863494e-02 5.2665070e-02 3.0739830e-04 6.9054682e-04 2.5072202e-04]\n"
     ]
    }
   ],
   "source": [
    "print(la_diag(FMNIST_test0[0].cuda(), link_approx='bridge').cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = MNIST_test.targets.numpy()\n",
    "targets_FMNIST = FMNIST_test.targets.numpy()\n",
    "targets_notMNIST = notMNIST_test.targets.numpy().astype(int)\n",
    "targets_KMNIST = KMNIST_test.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_MAP = predict_MAP(mnist_model, MNIST_test_loader, cuda=cuda_status).cpu().numpy()\n",
    "mnist_test_out_fmnist_MAP = predict_MAP(mnist_model, FMNIST_test_loader, cuda=cuda_status).cpu().numpy()\n",
    "mnist_test_out_notMNIST_MAP = predict_MAP(mnist_model, not_mnist_test_loader, cuda=cuda_status).cpu().numpy()\n",
    "mnist_test_out_KMNIST_MAP = predict_MAP(mnist_model, KMNIST_test_loader, cuda=cuda_status).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP = get_in_dist_values(mnist_test_in_MAP, targets)\n",
    "acc_out_FMNIST_MAP, prob_correct_out_FMNIST_MAP, ent_out_FMNIST_MAP, MMC_out_FMNIST_MAP, auroc_out_FMNIST_MAP = get_out_dist_values(mnist_test_in_MAP, mnist_test_out_fmnist_MAP, targets_FMNIST)\n",
    "acc_out_notMNIST_MAP, prob_correct_out_notMNIST_MAP, ent_out_notMNIST_MAP, MMC_out_notMNIST_MAP, auroc_out_notMNIST_MAP = get_out_dist_values(mnist_test_in_MAP, mnist_test_out_notMNIST_MAP, targets_notMNIST)\n",
    "acc_out_KMNIST_MAP, prob_correct_out_KMNIST_MAP, ent_out_KMNIST_MAP, MMC_out_KMNIST_MAP, auroc_out_KMNIST_MAP = get_out_dist_values(mnist_test_in_MAP, mnist_test_out_KMNIST_MAP, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, MAP, mnist] Accuracy: 0.986; average entropy: 0.053;     MMC: 0.984; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, FMNIST] Accuracy: 0.060; Average entropy: 1.119;    MMC: 0.602; AUROC: 0.980; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, notMNIST] Accuracy: 0.134; Average entropy: 0.656;    MMC: 0.758; AUROC: 0.902; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, KMNIST] Accuracy: 0.093; Average entropy: 0.750;    MMC: 0.723; AUROC: 0.955; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP, 'mnist', 'MAP')\n",
    "print_out_dist_values(acc_out_FMNIST_MAP, prob_correct_out_FMNIST_MAP, ent_out_FMNIST_MAP, MMC_out_FMNIST_MAP, auroc_out_FMNIST_MAP, 'FMNIST', 'MAP')\n",
    "print_out_dist_values(acc_out_notMNIST_MAP, prob_correct_out_notMNIST_MAP, ent_out_notMNIST_MAP, MMC_out_notMNIST_MAP, auroc_out_notMNIST_MAP, 'notMNIST', 'MAP')\n",
    "print_out_dist_values(acc_out_KMNIST_MAP, prob_correct_out_KMNIST_MAP, ent_out_KMNIST_MAP, MMC_out_KMNIST_MAP, auroc_out_KMNIST_MAP, 'KMNIST', 'MAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diag Hessian Sampling estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_diag = Laplace(mnist_model, 'classification', \n",
    "                     subset_of_weights='last_layer', \n",
    "                     hessian_structure='diag',\n",
    "                     prior_precision=5e-4) # 5e-4 # Choose prior precision according to weight decay\n",
    "la_diag.fit(MNIST_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_samples(laplace_model, data_loader, num_samples=100, timing=False, device='cuda'):\n",
    "    \n",
    "    py = []\n",
    "    t0 = time.process_time()\n",
    "    for x, _ in data_loader:\n",
    "        x = x.to(device)\n",
    "        py.append(laplace_model(x, link_approx='mc', n_samples=100).detach())\n",
    "    t1 = time.process_time()\n",
    "    if timing:\n",
    "        print(\"time: \", t1 - t0)\n",
    "    return(torch.cat(py, dim=0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:  1.0023496989999927\n",
      "time:  0.9864516380000055\n",
      "time:  0.5735045910000025\n",
      "time:  1.0156396080000007\n"
     ]
    }
   ],
   "source": [
    "MNIST_test_in_D = predict_samples(la_diag, MNIST_test_loader, timing=True).cpu().numpy()\n",
    "MNIST_test_out_FMNIST_D = predict_samples(la_diag, FMNIST_test_loader, timing=True).cpu().numpy()\n",
    "MNIST_test_out_notMNIST_D = predict_samples(la_diag, notMNIST_test_loader, timing=True).cpu().numpy()\n",
    "MNIST_test_out_KMNIST_D = predict_samples(la_diag, KMNIST_test_loader, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0685)\n",
      "tensor(-3.8994)\n",
      "tensor(-3.8136)\n",
      "tensor(-4.9436)\n"
     ]
    }
   ],
   "source": [
    "# compute average log-likelihood for Diag\n",
    "print(torch.distributions.Categorical(torch.tensor(MNIST_test_in_D)).log_prob(torch.tensor(targets)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(MNIST_test_out_FMNIST_D)).log_prob(torch.tensor(targets_FMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(MNIST_test_out_notMNIST_D)).log_prob(torch.tensor(targets_notMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(MNIST_test_out_KMNIST_D)).log_prob(torch.tensor(targets_KMNIST)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycalib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6224/50059648.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#compute the Expected confidence estimate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpycalib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_calibration_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_test_in_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_calibration_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_FMNIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist_test_out_FMNIST_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycalib'"
     ]
    }
   ],
   "source": [
    "#compute the Expected confidence estimate\n",
    "import pycalib.scoring as scoring\n",
    "\n",
    "print(scoring.expected_calibration_error(targets, MNIST_test_in_D))\n",
    "print(scoring.expected_calibration_error(targets_FMNIST, MNIST_test_out_FMNIST_D))\n",
    "print(scoring.expected_calibration_error(targets_notMNIST, MNIST_test_out_notMNIST_D))\n",
    "print(scoring.expected_calibration_error(targets_KMNIST, MNIST_test_out_KMNIST_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D = get_in_dist_values(MNIST_test_in_D, targets)\n",
    "acc_out_FMNIST_D, prob_correct_out_FMNIST_D, ent_out_FMNIST_D, MMC_out_FMNIST_D, auroc_out_FMNIST_D = get_out_dist_values(MNIST_test_in_D, MNIST_test_out_FMNIST_D, targets_FMNIST)\n",
    "acc_out_notMNIST_D, prob_correct_out_notMNIST_D, ent_out_notMNIST_D, MMC_out_notMNIST_D, auroc_out_notMNIST_D = get_out_dist_values(MNIST_test_in_D, MNIST_test_out_notMNIST_D, targets_notMNIST)\n",
    "acc_out_KMNIST_D, prob_correct_out_KMNIST_D, ent_out_KMNIST_D, MMC_out_KMNIST_D, auroc_out_KMNIST_D = get_out_dist_values(MNIST_test_in_D, MNIST_test_out_KMNIST_D, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, Diag, MNIST] Accuracy: 0.986; average entropy: 0.176;     MMC: 0.952; Prob @ correct: 0.100\n",
      "[Out-fmnist, Diag, MNIST] Accuracy: 0.060; Average entropy: 1.439;    MMC: 0.488; AUROC: 0.977; Prob @ correct: 0.100\n",
      "[Out-notMNIST, Diag, MNIST] Accuracy: 0.134; Average entropy: 1.249;    MMC: 0.543; AUROC: 0.961; Prob @ correct: 0.100\n",
      "[Out-KMNIST, Diag, MNIST] Accuracy: 0.093; Average entropy: 1.276;    MMC: 0.531; AUROC: 0.972; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D, 'MNIST', 'Diag')\n",
    "print_out_dist_values(acc_out_FMNIST_D, prob_correct_out_FMNIST_D, ent_out_FMNIST_D, MMC_out_FMNIST_D, auroc_out_FMNIST_D, 'MNIST', test='fmnist', method='Diag')\n",
    "print_out_dist_values(acc_out_notMNIST_D, prob_correct_out_notMNIST_D, ent_out_notMNIST_D, MMC_out_notMNIST_D, auroc_out_notMNIST_D, 'MNIST', test='notMNIST', method='Diag')\n",
    "print_out_dist_values(acc_out_KMNIST_D, prob_correct_out_KMNIST_D, ent_out_KMNIST_D, MMC_out_KMNIST_D, auroc_out_KMNIST_D, 'MNIST', test='KMNIST', method='Diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diag Sampling (1000)\n",
    "#seeds are 123,124,125,126,127\n",
    "time_diag_in = [7.236505508422852, 7.125160217285156, 7.238185882568359, 7.385348320007324, 7.000167608261108]\n",
    "time_diag_out_fmnist = [7.219660520553589, 8.032358407974243, 7.777910232543945, 7.446442604064941, 7.200972318649292]\n",
    "time_diag_out_notmnist = [20.68891930580139, 19.327053546905518, 18.424328804016113, 17.584392786026, 17.085043907165527]\n",
    "time_diag_out_kmnist = [9.627353191375732, 9.133541345596313, 9.307446718215942, 9.410037755966187, 9.100234746932983]\n",
    "\n",
    "acc_in = [0.989, 0.989, 0.990, 0.988, 0.989]\n",
    "mmc_in = [0.980, 0.976, 0.979, 0.980, 0.979]\n",
    "mmc_out_fmnist = [0.558, 0.570, 0.511, 0.566, 0.550]\n",
    "mmc_out_notmnist = [0.695, 0.730, 0.721, 0.735, 0.731]\n",
    "mmc_out_kmnist = [0.653, 0.666, 0.654, 0.664, 0.654]\n",
    "\n",
    "auroc_out_fmnist = [0.985, 0.980, 0.988, 0.983, 0.985]\n",
    "auroc_out_notmnist = [0.944, 0.907, 0.935, 0.927, 0.921]\n",
    "auroc_out_kmnist = [0.968, 0.960, 0.967, 0.966, 0.966]\n",
    "\n",
    "print(\"Diagonal Sampling time in: {:.03f} with std {:.03f}\".format(np.mean(time_diag_in), np.std(time_diag_in)))\n",
    "print(\"Diagonal Sampling time out fmnist: {:.03f} with std {:.03f}\".format(np.mean(time_diag_out_fmnist), np.std(time_diag_out_fmnist)))\n",
    "print(\"Diagonal Sampling time out notmnist: {:.03f} with std {:.03f}\".format(np.mean(time_diag_out_notmnist), np.std(time_diag_out_notmnist)))\n",
    "print(\"Diagonal Sampling time out kmnist: {:.03f} with std {:.03f}\".format(np.mean(time_diag_out_kmnist), np.std(time_diag_out_kmnist)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_fmnist), np.std(mmc_out_fmnist)))\n",
    "print(\"MMC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_notmnist), np.std(mmc_out_notmnist)))\n",
    "print(\"MMC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_kmnist), np.std(mmc_out_kmnist)))\n",
    "\n",
    "print(\"AUROC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_fmnist), np.std(auroc_out_fmnist)))\n",
    "print(\"AUROC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_notmnist), np.std(auroc_out_notmnist)))\n",
    "print(\"AUROC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_kmnist), np.std(auroc_out_kmnist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFAC Laplace Approximation (sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_KFAC = predict_KFAC_sampling(mnist_model, mnist_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True, n_samples=num_samples).cpu().numpy()\n",
    "mnist_test_out_FMNIST_KFAC = predict_KFAC_sampling(mnist_model, FMNIST_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True, n_samples=num_samples).cpu().numpy()\n",
    "mnist_test_out_notMNIST_KFAC = predict_KFAC_sampling(mnist_model, not_mnist_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True, n_samples=num_samples).cpu().numpy()\n",
    "mnist_test_out_KMNIST_KFAC = predict_KFAC_sampling(mnist_model, KMNIST_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True, n_samples=num_samples).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average log-likelihood for KFAC\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_in_KFAC)).log_prob(torch.tensor(targets)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_FMNIST_KFAC)).log_prob(torch.tensor(targets_FMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_notMNIST_KFAC)).log_prob(torch.tensor(targets_notMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_KMNIST_KFAC)).log_prob(torch.tensor(targets_KMNIST)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ECE for KFAC\n",
    "print(scoring.expected_calibration_error(targets, mnist_test_in_KFAC))\n",
    "print(scoring.expected_calibration_error(targets_FMNIST, mnist_test_out_FMNIST_KFAC))\n",
    "print(scoring.expected_calibration_error(targets_notMNIST, mnist_test_out_notMNIST_KFAC))\n",
    "print(scoring.expected_calibration_error(targets_KMNIST, mnist_test_out_KMNIST_KFAC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_KFAC, prob_correct_in_KFAC, ent_in_KFAC, MMC_in_KFAC = get_in_dist_values(mnist_test_in_KFAC, targets)\n",
    "acc_out_FMNIST_KFAC, prob_correct_out_FMNIST_KFAC, ent_out_FMNIST_KFAC, MMC_out_FMNIST_KFAC, auroc_out_FMNIST_KFAC = get_out_dist_values(mnist_test_in_KFAC, mnist_test_out_FMNIST_KFAC, targets_FMNIST)\n",
    "acc_out_notMNIST_KFAC, prob_correct_out_notMNIST_KFAC, ent_out_notMNIST_KFAC, MMC_out_notMNIST_KFAC, auroc_out_notMNIST_KFAC = get_out_dist_values(mnist_test_in_KFAC, mnist_test_out_notMNIST_KFAC, targets_notMNIST)\n",
    "acc_out_KMNIST_KFAC, prob_correct_out_KMNIST_KFAC, ent_out_KMNIST_KFAC, MMC_out_KMNIST_KFAC, auroc_out_KMNIST_KFAC = get_out_dist_values(mnist_test_in_KFAC, mnist_test_out_KMNIST_KFAC, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in_dist_values(acc_in_KFAC, prob_correct_in_KFAC, ent_in_KFAC, MMC_in_KFAC, 'MNIST', 'KFAC')\n",
    "print_out_dist_values(acc_out_FMNIST_KFAC, prob_correct_out_FMNIST_KFAC, ent_out_FMNIST_KFAC, MMC_out_FMNIST_KFAC, auroc_out_FMNIST_KFAC, 'MNIST', test='fmnist', method='KFAC')\n",
    "print_out_dist_values(acc_out_notMNIST_KFAC, prob_correct_out_notMNIST_KFAC, ent_out_notMNIST_KFAC, MMC_out_notMNIST_KFAC, auroc_out_notMNIST_KFAC, 'MNIST', test='notMNIST', method='KFAC')\n",
    "print_out_dist_values(acc_out_KMNIST_KFAC, prob_correct_out_KMNIST_KFAC, ent_out_KMNIST_KFAC, MMC_out_KMNIST_KFAC, auroc_out_KMNIST_KFAC, 'MNIST', test='KMNIST', method='KFAC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Bridge (Diagonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_LB = predict_LB(mnist_model, mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_FMNIST_LB = predict_LB(mnist_model, FMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_notMNIST_LB = predict_LB(mnist_model, not_mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_KMNIST_LB = predict_LB(mnist_model, KMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_LBn = mnist_test_in_LB/mnist_test_in_LB.sum(1).reshape(-1,1)\n",
    "mnist_test_out_FMNIST_LBn = mnist_test_out_FMNIST_LB/mnist_test_out_FMNIST_LB.sum(1).reshape(-1,1)\n",
    "mnist_test_out_notMNIST_LBn = mnist_test_out_notMNIST_LB/mnist_test_out_notMNIST_LB.sum(1).reshape(-1,1)\n",
    "mnist_test_out_KMNIST_LBn = mnist_test_out_KMNIST_LB/mnist_test_out_KMNIST_LB.sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute average log-likelihood for LB\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_in_LBn)).log_prob(torch.tensor(targets)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_FMNIST_LBn)).log_prob(torch.tensor(targets_FMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_notMNIST_LBn)).log_prob(torch.tensor(targets_notMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_KMNIST_LBn)).log_prob(torch.tensor(targets_KMNIST)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ECE for LB\n",
    "print(scoring.expected_calibration_error(targets, mnist_test_in_LBn))\n",
    "print(scoring.expected_calibration_error(targets_FMNIST, mnist_test_out_FMNIST_LBn))\n",
    "print(scoring.expected_calibration_error(targets_notMNIST, mnist_test_out_notMNIST_LBn))\n",
    "print(scoring.expected_calibration_error(targets_KMNIST, mnist_test_out_KMNIST_LBn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_LBn, prob_correct_in_LBn, ent_in_LBn, MMC_in_LBn = get_in_dist_values(mnist_test_in_LBn, targets)\n",
    "acc_out_FMNIST_LBn, prob_correct_out_FMNIST_LBn, ent_out_FMNIST_LBn, MMC_out_FMNIST_LBn, auroc_out_FMNIST_LBn = get_out_dist_values(mnist_test_in_LBn, mnist_test_out_FMNIST_LBn, targets_FMNIST)\n",
    "acc_out_notMNIST_LBn, prob_correct_out_notMNIST_LBn, ent_out_notMNIST_LBn, MMC_out_notMNIST_LBn, auroc_out_notMNIST_LBn = get_out_dist_values(mnist_test_in_LBn, mnist_test_out_notMNIST_LBn, targets_notMNIST)\n",
    "acc_out_KMNIST_LBn, prob_correct_out_KMNIST_LBn, ent_out_KMNIST_LBn, MMC_out_KMNIST_LBn, auroc_out_KMNIST_LBn = get_out_dist_values(mnist_test_in_LBn, mnist_test_out_KMNIST_LBn, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in_dist_values(acc_in_LBn, prob_correct_in_LBn, ent_in_LBn, MMC_in_LBn, 'MNIST', 'LBn')\n",
    "print_out_dist_values(acc_out_FMNIST_LBn, prob_correct_out_FMNIST_LBn, ent_out_FMNIST_LBn, MMC_out_FMNIST_LBn, auroc_out_FMNIST_LBn, 'MNIST', test='fmnist', method='LBn')\n",
    "print_out_dist_values(acc_out_notMNIST_LBn, prob_correct_out_notMNIST_LBn, ent_out_notMNIST_LBn, MMC_out_notMNIST_LBn, auroc_out_notMNIST_LBn, 'MNIST', test='notMNIST', method='LBn')\n",
    "print_out_dist_values(acc_out_KMNIST_LBn, prob_correct_out_KMNIST_LBn, ent_out_KMNIST_LBn, MMC_out_KMNIST_LBn, auroc_out_KMNIST_LBn, 'MNIST', test='KMNIST', method='LBn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laplace Bridge\n",
    "#seeds are 123,124,125,126,127\n",
    "time_lpb_in = [0.01302, 0.01731, 0.01489, 0.01529, 0.01574]\n",
    "time_lpb_out_fmnist = [0.01256, 0.01531, 0.01669, 0.01703, 0.01500]\n",
    "time_lpb_out_notmnist = [0.02332, 0.03115, 0.02666, 0.02756, 0.02864]\n",
    "time_lpb_out_kmnist = [0.01287, 0.01520, 0.01636, 0.01687, 0.01438]\n",
    "\n",
    "acc_in = [0.989, 0.989, 0.991, 0.988, 0.989]\n",
    "mmc_in = [0.988, 0.986, 0.988, 0.988, 0.988]\n",
    "mmc_out_fmnist = [0.493, 0.523, 0.433, 0.514, 0.488]\n",
    "mmc_out_notmnist = [0.735, 0.759, 0.756, 0.767, 0.769]\n",
    "mmc_out_kmnist = [0.699, 0.722, 0.707, 0.713, 0.703]\n",
    "\n",
    "auroc_out_fmnist = [0.991, 0.988, 0.993, 0.989, 0.990]\n",
    "auroc_out_notmnist = [0.948, 0.915, 0.938, 0.932, 0.924]\n",
    "auroc_out_kmnist = [0.970, 0.963, 0.969, 0.968, 0.968]\n",
    "\n",
    "print(\"Laplace Bridge time in: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_in), np.std(time_lpb_in)))\n",
    "print(\"Laplace Bridge time out fmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_fmnist), np.std(time_lpb_out_fmnist)))\n",
    "print(\"Laplace Bridge time out notmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_notmnist), np.std(time_lpb_out_notmnist)))\n",
    "print(\"Laplace Bridge time out kmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_kmnist), np.std(time_lpb_out_kmnist)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_fmnist), np.std(mmc_out_fmnist)))\n",
    "print(\"MMC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_notmnist), np.std(mmc_out_notmnist)))\n",
    "print(\"MMC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_kmnist), np.std(mmc_out_kmnist)))\n",
    "\n",
    "print(\"AUROC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_fmnist), np.std(auroc_out_fmnist)))\n",
    "print(\"AUROC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_notmnist), np.std(auroc_out_notmnist)))\n",
    "print(\"AUROC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_kmnist), np.std(auroc_out_kmnist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFAC Laplace Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_LB_KFAC = predict_LB_KFAC(mnist_model, mnist_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_FMNIST_LB_KFAC = predict_LB_KFAC(mnist_model, FMNIST_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_notMNIST_LB_KFAC = predict_LB_KFAC(mnist_model, not_mnist_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_KMNIST_LB_KFAC = predict_LB_KFAC(mnist_model, KMNIST_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_LB_KFACn = mnist_test_in_LB_KFAC/mnist_test_in_LB_KFAC.sum(1).reshape(-1,1)\n",
    "mnist_test_out_FMNIST_LB_KFACn = mnist_test_out_FMNIST_LB_KFAC/mnist_test_out_FMNIST_LB_KFAC.sum(1).reshape(-1,1)\n",
    "mnist_test_out_notMNIST_LB_KFACn = mnist_test_out_notMNIST_LB_KFAC/mnist_test_out_notMNIST_LB_KFAC.sum(1).reshape(-1,1)\n",
    "mnist_test_out_KMNIST_LB_KFACn = mnist_test_out_KMNIST_LB_KFAC/mnist_test_out_KMNIST_LB_KFAC.sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average log-likelihood for LB KFAC\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_in_LB_KFACn)).log_prob(torch.tensor(targets)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_FMNIST_LB_KFACn)).log_prob(torch.tensor(targets_FMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_notMNIST_LB_KFACn)).log_prob(torch.tensor(targets_notMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_KMNIST_LB_KFACn)).log_prob(torch.tensor(targets_KMNIST)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scoring.expected_calibration_error(targets, mnist_test_in_LB_KFACn))\n",
    "print(scoring.expected_calibration_error(targets_FMNIST, mnist_test_out_FMNIST_LB_KFACn))\n",
    "print(scoring.expected_calibration_error(targets_notMNIST, mnist_test_out_notMNIST_LB_KFACn))\n",
    "print(scoring.expected_calibration_error(targets_KMNIST, mnist_test_out_KMNIST_LB_KFACn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_LB_KFACn, prob_correct_in_LB_KFACn, ent_in_LB_KFACn, MMC_in_LB_KFACn = get_in_dist_values(mnist_test_in_LB_KFACn, targets)\n",
    "acc_out_FMNIST_LB_KFACn, prob_correct_out_FMNIST_LB_KFACn, ent_out_FMNIST_LB_KFACn, MMC_out_FMNIST_LB_KFACn, auroc_out_FMNIST_LB_KFACn = get_out_dist_values(mnist_test_in_LB_KFACn, mnist_test_out_FMNIST_LB_KFACn, targets_FMNIST)\n",
    "acc_out_notMNIST_LB_KFACn, prob_correct_out_notMNIST_LB_KFACn, ent_out_notMNIST_LB_KFACn, MMC_out_notMNIST_LB_KFACn, auroc_out_notMNIST_LB_KFACn = get_out_dist_values(mnist_test_in_LB_KFACn, mnist_test_out_notMNIST_LB_KFACn, targets_notMNIST)\n",
    "acc_out_KMNIST_LB_KFACn, prob_correct_out_KMNIST_LB_KFACn, ent_out_KMNIST_LB_KFACn, MMC_out_KMNIST_LB_KFACn, auroc_out_KMNIST_LB_KFACn = get_out_dist_values(mnist_test_in_LB_KFACn, mnist_test_out_KMNIST_LB_KFACn, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in_dist_values(acc_in_LB_KFACn, prob_correct_in_LB_KFACn, ent_in_LB_KFACn, MMC_in_LB_KFACn, 'MNIST', 'LB_KFACn')\n",
    "print_out_dist_values(acc_out_FMNIST_LB_KFACn, prob_correct_out_FMNIST_LB_KFACn, ent_out_FMNIST_LB_KFACn, MMC_out_FMNIST_LB_KFACn, auroc_out_FMNIST_LB_KFACn, 'MNIST', test='fmnist', method='LB_KFACn')\n",
    "print_out_dist_values(acc_out_notMNIST_LB_KFACn, prob_correct_out_notMNIST_LB_KFACn, ent_out_notMNIST_LB_KFACn, MMC_out_notMNIST_LB_KFACn, auroc_out_notMNIST_LB_KFACn, 'MNIST', test='notMNIST', method='LB_KFACn')\n",
    "print_out_dist_values(acc_out_KMNIST_LB_KFACn, prob_correct_out_KMNIST_LB_KFACn, ent_out_KMNIST_LB_KFACn, MMC_out_KMNIST_LB_KFACn, auroc_out_KMNIST_LB_KFACn, 'MNIST', test='KMNIST', method='LB_KFACn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laplace Bridge KFAC\n",
    "#seeds are 123,124,125,126,127\n",
    "time_lpb_in = [0.01302, 0.01731, 0.01489, 0.01529, 0.01574]\n",
    "time_lpb_out_fmnist = [0.01256, 0.01531, 0.01669, 0.01703, 0.01500]\n",
    "time_lpb_out_notmnist = [0.02332, 0.03115, 0.02666, 0.02756, 0.02864]\n",
    "time_lpb_out_kmnist = [0.01287, 0.01520, 0.01636, 0.01687, 0.01438]\n",
    "\n",
    "acc_in = [0.989, 0.989, 0.991, 0.988, 0.989]\n",
    "mmc_in = [0.988, 0.986, 0.988, 0.988, 0.988]\n",
    "mmc_out_fmnist = [0.493, 0.523, 0.433, 0.514, 0.488]\n",
    "mmc_out_notmnist = [0.735, 0.759, 0.756, 0.767, 0.769]\n",
    "mmc_out_kmnist = [0.699, 0.722, 0.707, 0.713, 0.703]\n",
    "\n",
    "auroc_out_fmnist = [0.991, 0.988, 0.993, 0.989, 0.990]\n",
    "auroc_out_notmnist = [0.948, 0.915, 0.938, 0.932, 0.924]\n",
    "auroc_out_kmnist = [0.970, 0.963, 0.969, 0.968, 0.968]\n",
    "\n",
    "print(\"Laplace Bridge time in: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_in), np.std(time_lpb_in)))\n",
    "print(\"Laplace Bridge time out fmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_fmnist), np.std(time_lpb_out_fmnist)))\n",
    "print(\"Laplace Bridge time out notmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_notmnist), np.std(time_lpb_out_notmnist)))\n",
    "print(\"Laplace Bridge time out kmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_kmnist), np.std(time_lpb_out_kmnist)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_fmnist), np.std(mmc_out_fmnist)))\n",
    "print(\"MMC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_notmnist), np.std(mmc_out_notmnist)))\n",
    "print(\"MMC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_kmnist), np.std(mmc_out_kmnist)))\n",
    "\n",
    "print(\"AUROC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_fmnist), np.std(auroc_out_fmnist)))\n",
    "print(\"AUROC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_notmnist), np.std(auroc_out_notmnist)))\n",
    "print(\"AUROC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_kmnist), np.std(auroc_out_kmnist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditions\n",
    "\n",
    "Test the condition derived in Proposition 1 of the paper and evaluated experimentally in Appendix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if condition holds\n",
    "\n",
    "def check_condition(alpha_vecs):\n",
    "    #note that this is vectorized\n",
    "    alpha_sum = alpha_vecs.sum(1)\n",
    "    alpha_max = alpha_vecs.max(1)\n",
    "    alpha_sum_minus = alpha_sum - alpha_max\n",
    "    right_side = 0.25 * (np.sqrt(9 * alpha_sum_minus**2 + 10 * alpha_sum_minus + 1) - alpha_sum_minus - 1)\n",
    "    cases = alpha_max > right_side\n",
    "    percentage = np.sum(cases)/len(cases)\n",
    "    return(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(check_condition(mnist_test_in_LB)))\n",
    "print(np.sum(check_condition(mnist_test_out_FMNIST_LB)))\n",
    "print(np.sum(check_condition(mnist_test_out_notMNIST_LB)))\n",
    "print(np.sum(check_condition(mnist_test_out_KMNIST_LB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to extended MacKay approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_EMK = predict_extended_MacKay(mnist_model, mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_FMNIST_EMK = predict_extended_MacKay(mnist_model, FMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_notMNIST_EMK = predict_extended_MacKay(mnist_model, not_mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_KMNIST_EMK = predict_extended_MacKay(mnist_model, KMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_EMK, prob_correct_in_EMK, ent_in_EMK, MMC_in_EMK = get_in_dist_values(mnist_test_in_EMK, targets)\n",
    "acc_out_FMNIST_EMK, prob_correct_out_FMNIST_EMK, ent_out_FMNIST_EMK, MMC_out_FMNIST_EMK, auroc_out_FMNIST_EMK = get_out_dist_values(mnist_test_in_EMK, mnist_test_out_FMNIST_EMK, targets_FMNIST)\n",
    "acc_out_notMNIST_EMK, prob_correct_out_notMNIST_EMK, ent_out_notMNIST_EMK, MMC_out_notMNIST_EMK, auroc_out_notMNIST_EMK = get_out_dist_values(mnist_test_in_EMK, mnist_test_out_notMNIST_EMK, targets_notMNIST)\n",
    "acc_out_KMNIST_EMK, prob_correct_out_KMNIST_EMK, ent_out_KMNIST_EMK, MMC_out_KMNIST_EMK, auroc_out_KMNIST_EMK = get_out_dist_values(mnist_test_in_EMK, mnist_test_out_KMNIST_EMK, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in_dist_values(acc_in_EMK, prob_correct_in_EMK, ent_in_EMK, MMC_in_EMK, 'MNIST', 'EMK')\n",
    "print_out_dist_values(acc_out_FMNIST_EMK, prob_correct_out_FMNIST_EMK, ent_out_FMNIST_EMK, MMC_out_FMNIST_EMK, auroc_out_FMNIST_EMK, 'MNIST', test='fmnist', method='EMK')\n",
    "print_out_dist_values(acc_out_notMNIST_EMK, prob_correct_out_notMNIST_EMK, ent_out_notMNIST_EMK, MMC_out_notMNIST_EMK, auroc_out_notMNIST_EMK, 'MNIST', test='notMNIST', method='EMK')\n",
    "print_out_dist_values(acc_out_KMNIST_EMK, prob_correct_out_KMNIST_EMK, ent_out_KMNIST_EMK, MMC_out_KMNIST_EMK, auroc_out_KMNIST_EMK, 'MNIST', test='KMNIST', method='EMK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extended MacKay\n",
    "#seeds are 123,124,125,126,127\n",
    "time_EMK_in = [0.059017181396484375, 0.055175065994262695, 0.04456186294555664, 0.052222251892089844, 0.05633044242858887]\n",
    "time_EMK_out_fmnist = [0.14669466018676758, 0.06929922103881836, 0.0457310676574707, 0.04770398139953613, 0.04364585876464844]\n",
    "time_EMK_out_notmnist = [0.06481504440307617, 0.08360505104064941, 0.07899594306945801, 0.0765230655670166, 0.09008359909057617]\n",
    "time_EMK_out_kmnist = [0.03671598434448242, 0.03630828857421875, 0.04480934143066406, 0.0473024845123291, 0.04279160499572754]\n",
    "\n",
    "acc_in = [0.989, 0.989, 0.991, 0.988, 0.989]\n",
    "mmc_in = [0.981, 0.978, 0.981, 0.982, 0.981]\n",
    "mmc_out_fmnist = [0.564, 0.577, 0.516, 0.573, 0.556]\n",
    "mmc_out_notmnist = [0.708, 0.741, 0.744, 0.747, 0.744]\n",
    "mmc_out_kmnist = [0.667, 0.681, 0.669, 0.678, 0.668]\n",
    "\n",
    "auroc_out_fmnist = [0.986, 0.982, 0.989, 0.984, 0.986]\n",
    "auroc_out_notmnist = [0.946, 0.909, 0.937, 0.928, 0.922]\n",
    "auroc_out_kmnist = [0.969, 0.962, 0.968, 0.967, 0.967]\n",
    "\n",
    "print(\"Extended MacKay time in: {:.03f} with std {:.03f}\".format(np.mean(time_EMK_in), np.std(time_EMK_in)))\n",
    "print(\"Extended MacKay time out fmnist: {:.03f} with std {:.03f}\".format(np.mean(time_EMK_out_fmnist), np.std(time_EMK_out_fmnist)))\n",
    "print(\"Extended MacKay time out notmnist: {:.03f} with std {:.03f}\".format(np.mean(time_EMK_out_notmnist), np.std(time_EMK_out_notmnist)))\n",
    "print(\"Extended MacKay time out kmnist: {:.03f} with std {:.03f}\".format(np.mean(time_EMK_out_kmnist), np.std(time_EMK_out_kmnist)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_fmnist), np.std(mmc_out_fmnist)))\n",
    "print(\"MMC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_notmnist), np.std(mmc_out_notmnist)))\n",
    "print(\"MMC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_kmnist), np.std(mmc_out_kmnist)))\n",
    "\n",
    "print(\"AUROC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_fmnist), np.std(auroc_out_fmnist)))\n",
    "print(\"AUROC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_notmnist), np.std(auroc_out_notmnist)))\n",
    "print(\"AUROC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_kmnist), np.std(auroc_out_kmnist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to Second-order Delta Posterior Predictive\n",
    "\n",
    "as detailed in Appendix D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_SODPP = predict_SODPP(mnist_model, mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_FMNIST_SODPP = predict_SODPP(mnist_model, FMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_notMNIST_SODPP = predict_SODPP(mnist_model, not_mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_KMNIST_SODPP = predict_SODPP(mnist_model, KMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_SODPP, prob_correct_in_SODPP, ent_in_SODPP, MMC_in_SODPP = get_in_dist_values(mnist_test_in_SODPP, targets)\n",
    "acc_out_FMNIST_SODPP, prob_correct_out_FMNIST_SODPP, ent_out_FMNIST_SODPP, MMC_out_FMNIST_SODPP, auroc_out_FMNIST_SODPP = get_out_dist_values(mnist_test_in_SODPP, mnist_test_out_FMNIST_SODPP, targets_FMNIST)\n",
    "acc_out_notMNIST_SODPP, prob_correct_out_notMNIST_SODPP, ent_out_notMNIST_SODPP, MMC_out_notMNIST_SODPP, auroc_out_notMNIST_SODPP = get_out_dist_values(mnist_test_in_SODPP, mnist_test_out_notMNIST_SODPP, targets_notMNIST)\n",
    "acc_out_KMNIST_SODPP, prob_correct_out_KMNIST_SODPP, ent_out_KMNIST_SODPP, MMC_out_KMNIST_SODPP, auroc_out_KMNIST_SODPP = get_out_dist_values(mnist_test_in_SODPP, mnist_test_out_KMNIST_SODPP, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in_dist_values(acc_in_SODPP, prob_correct_in_SODPP, ent_in_SODPP, MMC_in_SODPP, 'MNIST', 'SODPP')\n",
    "print_out_dist_values(acc_out_FMNIST_SODPP, prob_correct_out_FMNIST_SODPP, ent_out_FMNIST_SODPP, MMC_out_FMNIST_SODPP, auroc_out_FMNIST_SODPP, 'MNIST', test='fmnist', method='SODPP')\n",
    "print_out_dist_values(acc_out_notMNIST_SODPP, prob_correct_out_notMNIST_SODPP, ent_out_notMNIST_SODPP, MMC_out_notMNIST_SODPP, auroc_out_notMNIST_SODPP, 'MNIST', test='notMNIST', method='SODPP')\n",
    "print_out_dist_values(acc_out_KMNIST_SODPP, prob_correct_out_KMNIST_SODPP, ent_out_KMNIST_SODPP, MMC_out_KMNIST_SODPP, auroc_out_KMNIST_SODPP, 'MNIST', test='KMNIST', method='SODPP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SODPP\n",
    "#seeds are 123,124,125,126,127\n",
    "time_SODPP_in = [0.022600173950195312, 0.022752046585083008, 0.027388811111450195, 0.0277860164642334, 0.028126955032348633]\n",
    "time_SODPP_out_fmnist = [0.02283191680908203, 0.022715091705322266, 0.02962970733642578, 0.030440807342529297, 0.028356552124023438]\n",
    "time_SODPP_out_notmnist = [0.040780067443847656, 0.04070425033569336, 0.04935407638549805, 0.05025911331176758, 0.051819801330566406]\n",
    "time_SODPP_out_kmnist = [0.021996021270751953, 0.022888660430908203, 0.028764009475708008, 0.030774354934692383, 0.02728438377380371]\n",
    "\n",
    "acc_in = [0.989, 0.989, 0.990, 0.987, 0.990]\n",
    "mmc_in = [0.980, 0.977, 0.979, 0.980, 0.979]\n",
    "mmc_out_fmnist = [0.552, 0.565, 0.506, 0.561, 0.545]\n",
    "mmc_out_notmnist = [0.681, 0.719, 0.708, 0.724, 0.717]\n",
    "mmc_out_kmnist = [0.636, 0.648, 0.636, 0.648, 0.636]\n",
    "\n",
    "auroc_out_fmnist = [0.984, 0.979, 0.987, 0.982, 0.984]\n",
    "auroc_out_notmnist = [0.945, 0.910, 0.935, 0.928, 0.921]\n",
    "auroc_out_kmnist = [0.968, 0.960, 0.967, 0.966, 0.966]\n",
    "\n",
    "print(\"SODPP time in: {:.03f} with std {:.03f}\".format(np.mean(time_SODPP_in), np.std(time_SODPP_in)))\n",
    "print(\"SODPP time out fmnist: {:.03f} with std {:.03f}\".format(np.mean(time_SODPP_out_fmnist), np.std(time_SODPP_out_fmnist)))\n",
    "print(\"SODPP time out notmnist: {:.03f} with std {:.03f}\".format(np.mean(time_SODPP_out_notmnist), np.std(time_SODPP_out_notmnist)))\n",
    "print(\"SODPP time out kmnist: {:.03f} with std {:.03f}\".format(np.mean(time_SODPP_out_kmnist), np.std(time_SODPP_out_kmnist)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_fmnist), np.std(mmc_out_fmnist)))\n",
    "print(\"MMC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_notmnist), np.std(mmc_out_notmnist)))\n",
    "print(\"MMC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_kmnist), np.std(mmc_out_kmnist)))\n",
    "\n",
    "print(\"AUROC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_fmnist), np.std(auroc_out_fmnist)))\n",
    "print(\"AUROC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_notmnist), np.std(auroc_out_notmnist)))\n",
    "print(\"AUROC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_kmnist), np.std(auroc_out_kmnist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Rotated MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "# rotate 15 degrees\n",
    "MNIST_transform_r15 = transforms.Compose([\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "MNIST_test_r15 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r15)\n",
    "\n",
    "mnist_test_loader_r15 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r15,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 30 degrees\n",
    "MNIST_transform_r30 = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r30 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r30)\n",
    "\n",
    "mnist_test_loader_r30 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r30,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 45 degrees\n",
    "MNIST_transform_r45 = transforms.Compose([\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r45 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r45)\n",
    "\n",
    "mnist_test_loader_r45 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r45,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 60 degrees\n",
    "MNIST_transform_r60 = transforms.Compose([\n",
    "    transforms.RandomRotation(60),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r60 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r60)\n",
    "\n",
    "mnist_test_loader_r60 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r60,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 75 degrees\n",
    "MNIST_transform_r75 = transforms.Compose([\n",
    "    transforms.RandomRotation(75),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r75 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r75)\n",
    "\n",
    "mnist_test_loader_r75 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r75,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 90 degrees\n",
    "MNIST_transform_r90 = transforms.Compose([\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r90 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r90)\n",
    "\n",
    "mnist_test_loader_r90 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r90,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 105 degrees\n",
    "MNIST_transform_r105 = transforms.Compose([\n",
    "    transforms.RandomRotation(105),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r105 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r105)\n",
    "\n",
    "mnist_test_loader_r105 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r105,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 120 degrees\n",
    "MNIST_transform_r120 = transforms.Compose([\n",
    "    transforms.RandomRotation(120),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r120 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r120)\n",
    "\n",
    "mnist_test_loader_r120 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r120,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 135 degrees\n",
    "MNIST_transform_r135 = transforms.Compose([\n",
    "    transforms.RandomRotation(135),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r135 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r135)\n",
    "\n",
    "mnist_test_loader_r135 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r135,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 150 degrees\n",
    "MNIST_transform_r150= transforms.Compose([\n",
    "    transforms.RandomRotation(150),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r150 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r150)\n",
    "\n",
    "mnist_test_loader_r150 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r150,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 165 degrees\n",
    "MNIST_transform_r165 = transforms.Compose([\n",
    "    transforms.RandomRotation(165),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r165 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r165)\n",
    "\n",
    "mnist_test_loader_r165 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r165,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 180 degrees\n",
    "MNIST_transform_r180 = transforms.Compose([\n",
    "    transforms.RandomRotation(180),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r180 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r180)\n",
    "\n",
    "mnist_test_loader_r180 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r180,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "## helper function: given a dataloader compute accuracy and brier score\n",
    "def get_acc_brier(dataloader, targets, num_samples=1000):\n",
    "    \n",
    "    # compute sampling results\n",
    "    mnist_rotated_D = predict_diagonal_sampling(mnist_model, dataloader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=False, n_samples=num_samples).cpu().numpy()\n",
    "    \n",
    "    # compute LB results\n",
    "    mnist_rotated_LB = predict_LB(mnist_model, dataloader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=False).cpu().numpy()\n",
    "    mnist_rotated_LBn = mnist_rotated_LB/mnist_rotated_LB.sum(1).reshape(-1,1)\n",
    "    \n",
    "    # accuracy for sampling and LB\n",
    "    acc_D = np.mean(np.argmax(mnist_rotated_D, 1) == targets)\n",
    "    acc_LB = np.mean(np.argmax(mnist_rotated_LBn, 1) == targets)\n",
    "    \n",
    "    # get brier score for sampling and LB\n",
    "    pred_at_true_D = np.array([mnist_rotated_D[i, j] for i, j in enumerate(targets)])\n",
    "    pred_at_true_LBn = np.array([mnist_rotated_LBn[i, j] for i, j in enumerate(targets)])\n",
    "    \n",
    "    brier_D = brier_score_loss(np.ones_like(pred_at_true_D), pred_at_true_D)\n",
    "    brier_LB = brier_score_loss(np.ones_like(pred_at_true_LBn), pred_at_true_LBn)\n",
    "    \n",
    "    return(acc_D, acc_LB, brier_D, brier_LB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a test\n",
    "get_acc_brier(mnist_test_loader_r15, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict on all distributions and compute accuracy and brier score\n",
    "\n",
    "dataloader_list = [mnist_test_loader, mnist_test_loader_r15, mnist_test_loader_r30, mnist_test_loader_r45,\n",
    "                  mnist_test_loader_r60, mnist_test_loader_r75, mnist_test_loader_r90, mnist_test_loader_r105,\n",
    "                  mnist_test_loader_r120, mnist_test_loader_r135, mnist_test_loader_r150, mnist_test_loader_r165, \n",
    "                  mnist_test_loader_r180]\n",
    "\n",
    "Acc_D_list = []\n",
    "Acc_LB_list = []\n",
    "Brier_D_list = []\n",
    "Brier_LB_list = []\n",
    "\n",
    "for i, loader_ in enumerate(dataloader_list):\n",
    "    print(i)\n",
    "    Acc_D_, Acc_LB_, Brier_D_, Brier_LB_ = get_acc_brier(loader_, targets)\n",
    "    Acc_D_list.append(Acc_D_)\n",
    "    Acc_LB_list.append(Acc_LB_)\n",
    "    Brier_D_list.append(Brier_D_)\n",
    "    Brier_LB_list.append(Brier_LB_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make inline plots vector graphics\n",
    "import matplotlib\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")\n",
    "\n",
    "matplotlib.rc(\"font\", **{\"family\": \"serif\", \"serif\": [\"Computer Modern\"]})\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "plt.rcParams[\"text.latex.preamble\"] = r\"\\usepackage{amsfonts} \\usepackage{amsmath}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Acc_D_list)\n",
    "print(Acc_LB_list)\n",
    "print(Brier_D_list)\n",
    "print(Brier_LB_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### compare over 5 seeds: 123, 124, 125, 126, 127\n",
    "Acc_D_all = np.array([\n",
    "    [0.9906, 0.981, 0.9401, 0.8593, 0.7388, 0.6375, 0.5627, 0.5049, 0.4703, 0.4533, 0.4363, 0.4287, 0.4268],\n",
    "    [0.9889, 0.9814, 0.9433, 0.8518, 0.7337, 0.6403, 0.561, 0.5016, 0.4669, 0.4306, 0.4292, 0.4104, 0.4183],\n",
    "    [0.9904, 0.9829, 0.946, 0.8582, 0.747, 0.6495, 0.5602, 0.5075, 0.4684, 0.4437, 0.4334, 0.4245, 0.4217],\n",
    "    [0.9875, 0.9776, 0.9353, 0.8422, 0.7212, 0.6377, 0.55, 0.4896, 0.4497, 0.4328, 0.416, 0.4117, 0.4095],\n",
    "    [0.9894, 0.9816, 0.9451, 0.8522, 0.7403, 0.6391, 0.5565, 0.4994, 0.4656, 0.4308, 0.4284, 0.4221, 0.4152]\n",
    "])\n",
    "\n",
    "Acc_LB_all = np.array([\n",
    "    [0.9907, 0.983, 0.9394, 0.8526, 0.7419, 0.6386, 0.5703, 0.5152, 0.4702, 0.4484, 0.4335, 0.4236, 0.4182],\n",
    "    [0.9889, 0.9804, 0.9471, 0.8547, 0.7411, 0.6296, 0.5624, 0.4938, 0.4609, 0.4404, 0.4295, 0.4172, 0.4152],\n",
    "    [0.9905, 0.9831, 0.9436, 0.8616, 0.7461, 0.6517, 0.5647, 0.5127, 0.4673, 0.4454, 0.4273, 0.4291, 0.415],\n",
    "    [0.9876, 0.9799, 0.9398, 0.851, 0.7352, 0.6368, 0.5445, 0.4992, 0.4632, 0.4269, 0.42, 0.417, 0.4181],\n",
    "    [0.9893, 0.9808, 0.939, 0.8465, 0.7338, 0.6312, 0.5573, 0.505, 0.4539, 0.4458, 0.4158, 0.4189, 0.4145]\n",
    "])\n",
    "\n",
    "Brier_D_all = np.array([\n",
    "    [0.008377700125184932, 0.016529366393931315, 0.05263197122933036, 0.12622978433992207, 0.23428155265648049, 0.33001102154197504, 0.40364649390174306, 0.45833621095510935, 0.48925685879783176, 0.5084873524892697, 0.5265005890140535, 0.5351897862160594, 0.535655688821819],\n",
    "    [0.010744372283432188, 0.01861786528128727, 0.04921392785665237, 0.130553247122029, 0.23708335123859933, 0.3264322724129021, 0.4020700676185214, 0.45858313992431765, 0.49138653020376494, 0.5267105075863001, 0.5302083038117827, 0.5471840382483096, 0.5406172102829678],\n",
    "    [0.008842406659404037, 0.01589756693721347, 0.04813633596723046, 0.12450638545589118, 0.2251983306557004, 0.31596819055291797, 0.39958471677594976, 0.4511044612864316, 0.4908296869743894, 0.513979687459884, 0.5228012975599322, 0.5331324584577392, 0.5366587666968312],\n",
    "    [0.010730850229531219, 0.019535984307980356, 0.054532256239393395, 0.13633893796280153, 0.24614911889753252, 0.32741910714203704, 0.4095507809925442, 0.4699407662727923, 0.5086184210935191, 0.525650285971151, 0.544528220493102, 0.5479489062984142, 0.5516986011718078],\n",
    "    [0.009795412763015064, 0.01817015340220003, 0.04894315094823046, 0.12908701202114847, 0.23282846945381916, 0.3285278649762006, 0.4081318056690933, 0.46182442301821763, 0.4973149243671581, 0.5269344762781906, 0.5322562363481955, 0.5380640556117866, 0.5471252220414157]\n",
    "])\n",
    "\n",
    "Brier_LB_all = np.array([\n",
    "    [0.007844242911062461, 0.015166313465830961, 0.05119071505966292, 0.13220723722716274, 0.23246172960926015, 0.3326357106190896, 0.4017439850542811, 0.45393964088759164, 0.49749242841385766, 0.5211752780472316, 0.5353101111950346, 0.5431750248692432, 0.5509519585301749],\n",
    "    [0.009505463873917114, 0.016971207786010672, 0.0470778754224552, 0.12817579122361247, 0.2349776937700805, 0.339070595213455, 0.40568261118347, 0.4740733457035303, 0.5030623144832805, 0.5253332179429556, 0.5360191577762338, 0.549535112754868, 0.5511099660875561],\n",
    "    [0.0076984171595750415, 0.014756499162356471, 0.0469422462627717, 0.12101585517824154, 0.22767872259093588, 0.31762503643888157, 0.4011728288118217, 0.4516875142101383, 0.4954514924412558, 0.5178717196068767, 0.5371807295796656, 0.5349908080424384, 0.5497019253514567],\n",
    "    [0.01001831719153492, 0.016729483129191477, 0.05172484471829214, 0.13075484635707604, 0.23891760292945455, 0.3334325462508124, 0.42297503706889894, 0.4695227387634461, 0.5033610244309235, 0.5387111053297119, 0.5473753173717428, 0.5506346709903308, 0.5503556560492305],\n",
    "    [0.008755419081030589, 0.01618864623953837, 0.05156303842716825, 0.13417919609729662, 0.23963138466766964, 0.34131713744443276, 0.4110757904444741, 0.46371903749768173, 0.5107108175950072, 0.5222953217647085, 0.5486314035009504, 0.5470332878374046, 0.5531479907833964]   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_D_all_mean = np.mean(Acc_D_all, axis=0)\n",
    "Acc_D_all_std = np.std(Acc_D_all, axis=0)\n",
    "\n",
    "Acc_LB_all_mean = np.mean(Acc_LB_all, axis=0)\n",
    "Acc_LB_all_std = np.std(Acc_LB_all, axis=0)\n",
    "\n",
    "Brier_D_all_mean = np.mean(Brier_D_all, axis=0)\n",
    "Brier_D_all_std = np.std(Brier_D_all, axis=0)\n",
    "\n",
    "Brier_LB_all_mean = np.mean(Brier_LB_all, axis=0)\n",
    "Brier_LB_all_std = np.std(Brier_LB_all, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a figure for accuracy\n",
    "x_labels = ['test', '15$^{\\circ}$', '30$^{\\circ}$', '45$^{\\circ}$', '60$^{\\circ}$', '75$^{\\circ}$', '90$^{\\circ}$',\n",
    "            '105$^{\\circ}$', '120$^{\\circ}$', '135$^{\\circ}$', '150$^{\\circ}$', '165$^{\\circ}$', '180$^{\\circ}$']\n",
    "\n",
    "plt.figure(figsize=(5, 1.5))\n",
    "plt.plot(x_labels, Acc_D_all_mean, color='blue', label='MCMC')\n",
    "plt.plot(x_labels, Acc_LB_all_mean, color='firebrick', label='LB')\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('Accuracy', size=15)\n",
    "#plt.legend()\n",
    "plt.xticks(x_labels, x_labels, rotation='30', size=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/LB_vs_MCMC_Acc.pdf')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a figure for the brier score\n",
    "\n",
    "plt.figure(figsize=(5, 1.5))\n",
    "plt.plot(x_labels, Brier_D_list, color='blue', label='MCMC')\n",
    "plt.plot(x_labels, Brier_LB_list, color='firebrick', label='LB')\n",
    "plt.ylim(0,0.8)\n",
    "plt.ylabel('Brier', size=15)\n",
    "plt.legend()\n",
    "plt.xticks(x_labels, x_labels, rotation='30', size=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/LB_vs_MCMC_Brier.pdf')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a subplot\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(5, 3))\n",
    "\n",
    "ax[0].plot(x_labels, Acc_D_all_mean, color='blue', label='MCMC')\n",
    "ax[0].plot(x_labels, Acc_LB_all_mean, color='firebrick', label='LB')\n",
    "ax[0].set_ylim(0,1)\n",
    "ax[0].set_ylabel('Accuracy', size=15)\n",
    "ax[0].set_xticklabels([])\n",
    "\n",
    "ax[1].plot(x_labels, Brier_D_list, color='blue', label='MCMC')\n",
    "ax[1].plot(x_labels, Brier_LB_list, color='firebrick', label='LB')\n",
    "ax[1].set_ylim(0,0.8)\n",
    "ax[1].set_ylabel('Brier', size=15)\n",
    "ax[1].legend()\n",
    "ax[1].set_xticklabels(x_labels, rotation=30, size=13)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/LB_vs_MCMC_Acc_Brier.pdf')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
