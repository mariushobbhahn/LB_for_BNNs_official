{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi\n",
    "#using a GeForce GTX1080 Ti for reproducibility for all timing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from math import *\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import KFAC, DiagHessian, DiagGGNMC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy\n",
    "from tqdm import tqdm, trange\n",
    "import pytest\n",
    "from LB_utils import * \n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s = 123\n",
    "np.random.seed(s)\n",
    "torch.manual_seed(s)\n",
    "torch.cuda.manual_seed(s)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "cuda status:  False\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cuda_status = torch.cuda.is_available()\n",
    "print(\"device: \", device)\n",
    "print(\"cuda status: \", cuda_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define network\n",
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.Conv2d(16, 32, 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(4 * 4 * 32, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TRAIN_MNIST = 128\n",
    "BATCH_SIZE_TEST_MNIST = 128\n",
    "MAX_ITER_MNIST = 6\n",
    "LR_TRAIN_MNIST = 10e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "MNIST_train = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=MNIST_transform)\n",
    "\n",
    "MNIST_train_loader = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_train,\n",
    "    batch_size=BATCH_SIZE_TRAIN_MNIST,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "MNIST_test = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform)\n",
    "\n",
    "MNIST_test_loader = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = ConvNet().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "mnist_train_optimizer = torch.optim.Adam(mnist_model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "MNIST_PATH = \"pretrained_weights/MNIST_pretrained_10_classes_last_layer_s{}.pth\".format(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training routine\n",
    "\n",
    "def train(model, train_loader, optimizer, max_iter, path, verbose=True):\n",
    "    max_len = len(train_loader)\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            output = model(x)\n",
    "\n",
    "            accuracy = get_accuracy(output, y)\n",
    "\n",
    "            loss = loss_function(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if verbose and batch_idx % 50 == 0:\n",
    "                print(\n",
    "                    \"Iteration {}; {}/{} \\t\".format(iter, batch_idx, max_len) +\n",
    "                    \"Minibatch Loss %.3f  \" % (loss) +\n",
    "                    \"Accuracy %.0f\" % (accuracy * 100) + \"%\"\n",
    "                )\n",
    "\n",
    "    print(\"saving model at: {}\".format(path))\n",
    "    torch.save(mnist_model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0; 0/469 \tMinibatch Loss 2.316  Accuracy 8%\n",
      "Iteration 0; 50/469 \tMinibatch Loss 0.435  Accuracy 89%\n",
      "Iteration 0; 100/469 \tMinibatch Loss 0.307  Accuracy 90%\n",
      "Iteration 0; 150/469 \tMinibatch Loss 0.188  Accuracy 95%\n",
      "Iteration 0; 200/469 \tMinibatch Loss 0.215  Accuracy 94%\n",
      "Iteration 0; 250/469 \tMinibatch Loss 0.159  Accuracy 93%\n",
      "Iteration 0; 300/469 \tMinibatch Loss 0.119  Accuracy 97%\n",
      "Iteration 0; 350/469 \tMinibatch Loss 0.118  Accuracy 97%\n",
      "Iteration 0; 400/469 \tMinibatch Loss 0.043  Accuracy 100%\n",
      "Iteration 0; 450/469 \tMinibatch Loss 0.075  Accuracy 98%\n",
      "Iteration 1; 0/469 \tMinibatch Loss 0.160  Accuracy 95%\n",
      "Iteration 1; 50/469 \tMinibatch Loss 0.070  Accuracy 97%\n",
      "Iteration 1; 100/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 1; 150/469 \tMinibatch Loss 0.072  Accuracy 98%\n",
      "Iteration 1; 200/469 \tMinibatch Loss 0.139  Accuracy 96%\n",
      "Iteration 1; 250/469 \tMinibatch Loss 0.067  Accuracy 97%\n",
      "Iteration 1; 300/469 \tMinibatch Loss 0.064  Accuracy 97%\n",
      "Iteration 1; 350/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 1; 400/469 \tMinibatch Loss 0.034  Accuracy 98%\n",
      "Iteration 1; 450/469 \tMinibatch Loss 0.057  Accuracy 98%\n",
      "Iteration 2; 0/469 \tMinibatch Loss 0.020  Accuracy 100%\n",
      "Iteration 2; 50/469 \tMinibatch Loss 0.070  Accuracy 97%\n",
      "Iteration 2; 100/469 \tMinibatch Loss 0.056  Accuracy 98%\n",
      "Iteration 2; 150/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 2; 200/469 \tMinibatch Loss 0.076  Accuracy 98%\n",
      "Iteration 2; 250/469 \tMinibatch Loss 0.053  Accuracy 98%\n",
      "Iteration 2; 300/469 \tMinibatch Loss 0.116  Accuracy 97%\n",
      "Iteration 2; 350/469 \tMinibatch Loss 0.030  Accuracy 99%\n",
      "Iteration 2; 400/469 \tMinibatch Loss 0.039  Accuracy 98%\n",
      "Iteration 2; 450/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 3; 0/469 \tMinibatch Loss 0.044  Accuracy 98%\n",
      "Iteration 3; 50/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 3; 100/469 \tMinibatch Loss 0.131  Accuracy 96%\n",
      "Iteration 3; 150/469 \tMinibatch Loss 0.040  Accuracy 98%\n",
      "Iteration 3; 200/469 \tMinibatch Loss 0.049  Accuracy 98%\n",
      "Iteration 3; 250/469 \tMinibatch Loss 0.028  Accuracy 99%\n",
      "Iteration 3; 300/469 \tMinibatch Loss 0.013  Accuracy 99%\n",
      "Iteration 3; 350/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 3; 400/469 \tMinibatch Loss 0.022  Accuracy 100%\n",
      "Iteration 3; 450/469 \tMinibatch Loss 0.043  Accuracy 98%\n",
      "Iteration 4; 0/469 \tMinibatch Loss 0.124  Accuracy 96%\n",
      "Iteration 4; 50/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 4; 100/469 \tMinibatch Loss 0.073  Accuracy 98%\n",
      "Iteration 4; 150/469 \tMinibatch Loss 0.059  Accuracy 98%\n",
      "Iteration 4; 200/469 \tMinibatch Loss 0.119  Accuracy 98%\n",
      "Iteration 4; 250/469 \tMinibatch Loss 0.080  Accuracy 99%\n",
      "Iteration 4; 300/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 4; 350/469 \tMinibatch Loss 0.027  Accuracy 99%\n",
      "Iteration 4; 400/469 \tMinibatch Loss 0.041  Accuracy 98%\n",
      "Iteration 4; 450/469 \tMinibatch Loss 0.017  Accuracy 100%\n",
      "Iteration 5; 0/469 \tMinibatch Loss 0.012  Accuracy 100%\n",
      "Iteration 5; 50/469 \tMinibatch Loss 0.046  Accuracy 98%\n",
      "Iteration 5; 100/469 \tMinibatch Loss 0.051  Accuracy 99%\n",
      "Iteration 5; 150/469 \tMinibatch Loss 0.019  Accuracy 100%\n",
      "Iteration 5; 200/469 \tMinibatch Loss 0.032  Accuracy 98%\n",
      "Iteration 5; 250/469 \tMinibatch Loss 0.026  Accuracy 99%\n",
      "Iteration 5; 300/469 \tMinibatch Loss 0.042  Accuracy 99%\n",
      "Iteration 5; 350/469 \tMinibatch Loss 0.050  Accuracy 99%\n",
      "Iteration 5; 400/469 \tMinibatch Loss 0.016  Accuracy 100%\n",
      "Iteration 5; 450/469 \tMinibatch Loss 0.035  Accuracy 98%\n",
      "saving model at: pretrained_weights/MNIST_pretrained_10_classes_last_layer_s123.pth\n"
     ]
    }
   ],
   "source": [
    "train(mnist_model, MNIST_train_loader, mnist_train_optimizer, MAX_ITER_MNIST, MNIST_PATH, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: pretrained_weights/MNIST_pretrained_10_classes_last_layer_s123.pth\n",
      "Batch 0/79 \tAccuracy 100%\n",
      "Batch 10/79 \tAccuracy 98%\n",
      "Batch 20/79 \tAccuracy 98%\n",
      "Batch 30/79 \tAccuracy 98%\n",
      "Batch 40/79 \tAccuracy 98%\n",
      "Batch 50/79 \tAccuracy 100%\n",
      "Batch 60/79 \tAccuracy 100%\n",
      "Batch 70/79 \tAccuracy 99%\n",
      "overall test accuracy on MNIST: 98.60 %\n"
     ]
    }
   ],
   "source": [
    "#predict in distribution\n",
    "MNIST_PATH = \"pretrained_weights/MNIST_pretrained_10_classes_last_layer_s{}.pth\".format(s)\n",
    "#MNIST_PATH = \"pretrained_weights/MNIST_pretrained_10_classes_last_layer.pth\"\n",
    "\n",
    "mnist_model = ConvNet().to(device)\n",
    "print(\"loading model from: {}\".format(MNIST_PATH))\n",
    "mnist_model.load_state_dict(torch.load(MNIST_PATH))\n",
    "mnist_model.eval()\n",
    "\n",
    "acc = []\n",
    "\n",
    "max_len = len(MNIST_test_loader)\n",
    "for batch_idx, (x, y) in enumerate(MNIST_test_loader):\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    output = mnist_model(x)\n",
    "\n",
    "    accuracy = get_accuracy(output, y)\n",
    "    if batch_idx % 10 == 0:\n",
    "        print(\n",
    "            \"Batch {}/{} \\t\".format(batch_idx, max_len) + \n",
    "            \"Accuracy %.0f\" % (accuracy * 100) + \"%\"\n",
    "        )\n",
    "    acc.append(accuracy)\n",
    "\n",
    "avg_acc = np.mean(acc)\n",
    "print('overall test accuracy on MNIST: {:.02f} %'.format(avg_acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TEST_FMNIST = 128\n",
    "BATCH_SIZE_TEST_KMNIST = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FMNIST_test = torchvision.datasets.FashionMNIST(\n",
    "        '~/data/fmnist', train=False, download=True,\n",
    "        transform=MNIST_transform)   #torchvision.transforms.ToTensor())\n",
    "\n",
    "FMNIST_test_loader = torch.utils.data.DataLoader(\n",
    "    FMNIST_test,\n",
    "    batch_size=BATCH_SIZE_TEST_FMNIST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMNIST_test = torchvision.datasets.KMNIST(\n",
    "        '~/data/kmnist', train=False, download=True,\n",
    "        transform=MNIST_transform)\n",
    "\n",
    "KMNIST_test_loader = torch.utils.data.DataLoader(\n",
    "    KMNIST_test,\n",
    "    batch_size=BATCH_SIZE_TEST_KMNIST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load notMNIST\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from matplotlib.pyplot import imread\n",
    "from torch import Tensor\n",
    "\n",
    "\"\"\"\n",
    "Loads the train/test set. \n",
    "Every image in the dataset is 28x28 pixels and the labels are numbered from 0-9\n",
    "for A-J respectively.\n",
    "Set root to point to the Train/Test folders.\n",
    "\"\"\"\n",
    "\n",
    "# Creating a sub class of torch.utils.data.dataset.Dataset\n",
    "class notMNIST(Dataset):\n",
    "\n",
    "    # The init method is called when this class will be instantiated\n",
    "    def __init__(self, root, transform):\n",
    "        \n",
    "        #super(notMNIST, self).__init__(root, transform=transform)\n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "        Images, Y = [], []\n",
    "        folders = os.listdir(root)\n",
    "\n",
    "        for folder in folders:\n",
    "            folder_path = os.path.join(root, folder)\n",
    "            for ims in os.listdir(folder_path):\n",
    "                try:\n",
    "                    img_path = os.path.join(folder_path, ims)\n",
    "                    Images.append(np.array(imread(img_path)))\n",
    "                    Y.append(ord(folder) - 65)  # Folders are A-J so labels will be 0-9\n",
    "                except:\n",
    "                    # Some images in the dataset are damaged\n",
    "                    print(\"File {}/{} is broken\".format(folder, ims))\n",
    "        data = [(x, y) for x, y in zip(Images, Y)]\n",
    "        self.data = data\n",
    "        self.targets = torch.Tensor(Y)\n",
    "\n",
    "    # The number of items in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # The Dataloader is a generator that repeatedly calls the getitem method.\n",
    "    # getitem is supposed to return (X, Y) for the specified index.\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index][0]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        # Input for Conv2D should be Channels x Height x Width\n",
    "        img_tensor = Tensor(img).view(1, 28, 28).float()\n",
    "        label = self.data[index][1]\n",
    "        return (img_tensor, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mariushobbhahn/data/notMNIST_small'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a203950644e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Instantiating the notMNIST dataset class we created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m notMNIST_test = notMNIST(root=os.path.join(root, 'notMNIST_small'),\n\u001b[0m\u001b[1;32m      6\u001b[0m                                transform=MNIST_transform)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d939b7004af7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mImages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfolders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mariushobbhahn/data/notMNIST_small'"
     ]
    }
   ],
   "source": [
    "#root = os.path.abspath('~/data')\n",
    "root = os.path.expanduser('~/data')\n",
    "\n",
    "# Instantiating the notMNIST dataset class we created\n",
    "notMNIST_test = notMNIST(root=os.path.join(root, 'notMNIST_small'),\n",
    "                               transform=MNIST_transform)\n",
    "\n",
    "# Creating a dataloader\n",
    "not_mnist_test_loader = torch.utils.data.dataloader.DataLoader(\n",
    "                            dataset=notMNIST_test,\n",
    "                            batch_size=BATCH_SIZE_TEST_KMNIST,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laplace import Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#la_full = Laplace(mnist_model, 'classification', \n",
    "#                     subset_of_weights='last_layer', \n",
    "#                     hessian_structure='full',\n",
    "#                     prior_precision=5e-4) #5e-4 # Choose prior precision according to weight decay\n",
    "#la_full.fit(MNIST_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_diag = Laplace(mnist_model, 'classification', \n",
    "                     subset_of_weights='last_layer', \n",
    "                     hessian_structure='diag',\n",
    "                     prior_precision=5e-4) # 5e-4 # Choose prior precision according to weight decay\n",
    "la_diag.fit(MNIST_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.1969937e-06 4.2444722e-06 2.7965385e-04 3.6197715e-04 1.0240941e-08\n",
      " 4.3042241e-06 6.9125378e-10 9.9918574e-01 8.1338493e-07 1.6004850e-04]\n"
     ]
    }
   ],
   "source": [
    "## predict samples with diag & mc\n",
    "MNIST_test0 = next(iter(MNIST_test_loader))\n",
    "#print(MNIST_test0)\n",
    "print(la_diag(MNIST_test0[0], link_approx='mc', n_samples=100).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_mu:  tensor([[-1.8807e+00,  8.4699e-01,  2.3559e+00,  2.4790e+00, -6.8265e+00,\n",
      "         -2.3344e+00, -7.1885e+00,  1.2932e+01, -2.7832e+00,  2.3994e+00],\n",
      "        [ 5.1213e+00,  7.2997e+00,  1.3773e+01, -2.5994e+00, -3.2040e+00,\n",
      "         -6.5344e+00,  8.5208e-01, -4.6815e+00,  2.0473e-01, -1.0231e+01],\n",
      "        [-1.2979e+00,  8.2169e+00, -1.4028e+00, -3.9788e+00,  1.7516e+00,\n",
      "         -1.8739e+00,  1.3801e+00,  1.6642e-02, -9.9837e-02, -2.7120e+00],\n",
      "        [ 1.1283e+01,  7.5270e+00, -1.1381e-01, -8.3491e+00, -2.8228e+00,\n",
      "         -2.6742e+00,  1.7146e+00, -3.8572e+00, -2.3863e+00, -3.2105e-01],\n",
      "        [-4.4779e+00,  6.1918e+00, -4.3677e+00, -5.2117e+00,  1.3180e+01,\n",
      "         -4.0963e+00, -3.2147e+00,  4.7680e-01, -2.8595e+00,  4.3790e+00],\n",
      "        [-1.2512e+00,  1.0401e+01, -2.3227e+00, -4.6815e+00,  2.1979e+00,\n",
      "         -3.6221e+00,  2.9012e-01,  9.6658e-01,  2.3698e-01, -2.2156e+00],\n",
      "        [-1.0586e+01,  7.3139e+00, -3.1495e+00, -6.7014e+00,  1.1252e+01,\n",
      "         -2.4059e+00, -1.6526e+00,  4.7521e-01,  3.7169e+00,  1.7379e+00],\n",
      "        [-1.2334e+00,  2.8127e+00, -8.0851e-01, -6.3250e-01,  1.5633e+00,\n",
      "         -6.3368e-01, -7.7366e+00, -2.5732e+00,  1.2221e+00,  8.0198e+00],\n",
      "        [-3.7906e+00,  6.9975e-01, -1.0637e+00, -3.7911e+00, -5.2206e+00,\n",
      "          1.0099e+01,  2.8555e+00, -1.9692e+00,  3.0727e+00, -8.9159e-01],\n",
      "        [-2.1456e+00, -3.0992e-01, -2.5857e+00, -4.6671e-01,  2.0859e+00,\n",
      "         -5.4038e+00, -6.0334e+00,  2.6565e+00,  2.3697e+00,  9.8330e+00],\n",
      "        [ 1.3426e+01,  1.5987e+01,  1.3856e+00, -6.3776e+00, -1.0571e+01,\n",
      "         -3.9917e+00, -2.2397e+00, -3.3588e+00, -3.8105e+00, -4.4946e-01],\n",
      "        [ 4.3970e+00,  9.8935e+00, -5.0927e+00, -5.5525e+00, -5.0086e+00,\n",
      "          3.3681e-01,  1.2855e+01, -6.4222e+00,  7.0382e-01, -6.1097e+00],\n",
      "        [-3.4383e+00, -1.0896e+00, -4.5238e+00, -6.1204e-03,  5.1582e+00,\n",
      "         -2.1666e+00, -7.4845e+00,  1.5543e+00,  1.5122e+00,  1.0484e+01],\n",
      "        [ 1.0828e+01,  1.3921e+01, -4.0930e+00, -1.2146e+01, -3.0480e+00,\n",
      "         -2.2436e+00, -1.9292e+00,  7.5367e-02, -2.5512e+00,  1.1860e+00],\n",
      "        [-1.5734e+00,  9.0171e+00, -2.7154e+00, -3.1100e-01,  1.7543e+00,\n",
      "         -1.3197e+00, -1.4557e+00, -1.6866e+00,  7.2051e-01, -2.4301e+00],\n",
      "        [-3.3505e+00,  7.5664e+00, -3.6918e-01,  2.3724e+00, -5.3157e+00,\n",
      "          8.1434e+00,  1.7883e+00, -2.2406e+00, -1.4022e+00, -7.1922e+00],\n",
      "        [-3.5423e+00,  2.2611e+00, -4.3073e+00, -2.3944e+00,  2.8194e+00,\n",
      "         -2.1782e+00, -8.5016e+00,  3.9813e+00,  2.9614e-01,  1.1566e+01],\n",
      "        [ 3.5039e-01,  4.3373e+00,  3.0628e+00,  2.7407e+00, -1.0773e+01,\n",
      "         -1.3279e+00, -1.0002e+01,  1.4815e+01, -7.2283e+00,  4.0257e+00],\n",
      "        [-2.1157e+00,  1.0062e+01,  2.6379e+00,  5.3467e+00, -4.3716e+00,\n",
      "          4.6358e-01, -1.0156e+00, -7.7285e+00,  9.2406e-01, -4.2030e+00],\n",
      "        [-7.2784e+00,  4.1412e+00, -1.5878e+00, -6.1690e+00,  1.1726e+01,\n",
      "          4.4590e-01, -2.8028e+00,  2.4708e+00, -1.5012e+00,  5.5552e-01],\n",
      "        [-1.9799e+00, -8.9771e-01, -2.3799e+00, -6.3657e-01,  2.7559e+00,\n",
      "         -9.0413e-01, -5.2845e+00,  2.4778e+00, -1.0938e+00,  7.9428e+00],\n",
      "        [-3.8731e-01,  1.0096e+00, -1.5707e+00, -2.0829e+00, -8.7689e-01,\n",
      "          4.9825e+00,  1.1334e+01, -8.9180e+00,  2.1030e+00, -5.5928e+00],\n",
      "        [ 2.5575e+00,  8.1478e+00, -9.7238e+00, -7.2187e+00,  9.0792e-01,\n",
      "          1.2227e+00,  1.2461e+01, -3.8701e+00, -3.7102e-01, -4.1129e+00],\n",
      "        [-7.7580e-01, -1.2066e+00, -4.7634e+00,  1.8643e+00, -4.6596e+00,\n",
      "          1.2261e+01,  3.0673e+00, -4.6993e+00, -9.9895e-01, -8.8588e-02],\n",
      "        [-4.0942e+00,  3.8636e+00, -2.2819e+00, -4.9999e+00,  1.0151e+01,\n",
      "         -7.4954e-01, -3.5366e+00,  6.4630e-01, -1.4230e+00,  2.4240e+00],\n",
      "        [ 1.1873e+01,  8.9468e+00,  4.8732e-01, -9.3535e+00, -4.6479e+00,\n",
      "         -6.6331e+00,  1.5349e+00, -3.0983e+00,  1.0035e-01,  7.9074e-01],\n",
      "        [-1.2457e+00,  5.8281e+00,  1.5996e+00,  1.1691e-01, -3.3565e+00,\n",
      "         -9.1950e-01, -1.0656e+01,  1.0595e+01, -5.6213e+00,  3.6596e+00],\n",
      "        [-6.8612e+00,  5.4912e+00, -3.4060e+00, -5.6658e+00,  1.6295e+01,\n",
      "         -8.9835e-01, -4.5215e+00, -1.6464e-02, -2.1721e+00,  1.7553e+00],\n",
      "        [ 1.2454e+01,  1.1824e+01,  2.5671e+00, -6.3221e+00, -5.8296e+00,\n",
      "         -2.0808e+00, -4.2655e+00, -6.4609e-01, -3.7203e+00, -3.9808e+00],\n",
      "        [-2.6388e+00,  7.1946e+00, -1.1533e+00, -8.4932e-01,  2.0485e+00,\n",
      "         -1.3025e+00, -1.8643e+00, -2.9703e-01,  1.4081e+00, -2.5459e+00],\n",
      "        [-1.8382e+00, -1.3483e+00, -2.7109e+00,  1.5333e+01, -5.7665e+00,\n",
      "          1.4486e+00, -4.2466e+00, -6.5601e-01, -2.8767e+00,  2.6614e+00],\n",
      "        [-2.6948e+00,  6.9894e+00, -1.5004e+00, -1.5319e+00,  2.3368e+00,\n",
      "         -1.4100e+00, -3.8162e+00,  5.6383e-02,  1.4166e+00,  1.5419e-01],\n",
      "        [-6.4275e+00,  6.0886e+00, -6.8950e-01,  1.4609e+01, -3.2792e+00,\n",
      "          1.1677e+00, -6.0087e+00, -4.0528e-01, -9.3826e-01, -4.1167e+00],\n",
      "        [ 1.8411e+00,  1.3108e+01, -1.1872e+00, -9.5428e+00,  1.3322e+01,\n",
      "         -6.2175e+00,  1.3192e+00, -3.0655e+00, -4.9871e+00, -4.5900e+00],\n",
      "        [-2.7241e+00,  2.9886e+00,  4.3444e+00,  3.1280e+00, -7.8373e+00,\n",
      "         -6.2822e+00, -7.3589e+00,  1.4045e+01,  1.1922e-01, -4.2235e-01],\n",
      "        [-1.7076e+00,  1.3535e+01,  1.4559e+01,  1.9436e-01, -9.0654e+00,\n",
      "         -8.0518e+00, -3.6352e+00, -1.5962e+00, -3.0757e-01, -3.9247e+00],\n",
      "        [-3.0017e+00,  4.3674e+00,  7.0790e+00,  1.4432e+00, -7.0739e+00,\n",
      "         -5.9668e+00, -5.0179e+00,  1.0667e+01, -1.4484e+00, -1.0477e+00],\n",
      "        [-2.0954e+00,  7.9017e+00, -2.1208e+00, -2.1608e+00,  1.8147e+00,\n",
      "         -1.7367e+00, -2.6082e+00, -1.1239e-01,  1.6681e+00, -5.5025e-01],\n",
      "        [ 4.0293e+00,  5.5637e+00,  9.6006e+00,  2.1067e-01, -8.4400e+00,\n",
      "         -3.8891e+00, -3.5840e+00,  1.9751e+00,  2.2885e-01, -5.6951e+00],\n",
      "        [ 6.9916e-01,  8.6439e+00, -2.3331e+00, -1.4761e+00,  3.4691e-01,\n",
      "         -3.4363e+00, -1.4705e-01, -1.2935e+00,  1.6827e+00, -2.6866e+00],\n",
      "        [-1.4727e+00,  5.2740e+00, -8.8618e-01, -1.5397e+00,  8.6072e-01,\n",
      "         -1.5917e+00, -7.0222e-01,  4.1408e-01,  5.1807e-01, -8.7429e-01],\n",
      "        [-2.6644e+00,  3.7327e+00,  6.1086e+00,  2.0491e+00, -4.2039e+00,\n",
      "         -4.0753e+00, -4.7709e+00,  8.8921e+00, -2.1683e+00, -2.8997e+00],\n",
      "        [-5.9168e+00,  6.8453e+00, -1.1262e+01, -7.2682e+00,  1.3756e+01,\n",
      "         -2.8212e-01, -3.1520e+00,  1.9458e+00, -1.9036e-01,  5.5236e+00],\n",
      "        [-4.2206e+00,  7.9129e+00,  6.7959e+00, -2.1655e+00,  3.8544e+00,\n",
      "         -2.0091e+00, -2.0149e+00,  3.9219e-01, -1.2499e+00, -7.2953e+00],\n",
      "        [-2.8621e+00, -9.2980e-02, -6.5135e-01,  9.6668e+00, -1.6545e+00,\n",
      "          3.0303e+00, -2.0494e-02, -3.5823e+00, -2.1992e+00, -1.6342e+00],\n",
      "        [ 7.6869e-01,  4.6013e+00, -1.7679e+00, -2.7420e+00, -6.2127e+00,\n",
      "          8.1784e+00,  2.5746e+00, -4.3183e+00,  1.2444e+00, -2.3265e+00],\n",
      "        [-3.5772e+00,  8.3539e+00, -3.4012e-01, -3.9250e-01,  2.1546e+00,\n",
      "         -7.8731e-01, -3.4446e+00, -1.6134e+00,  2.2858e+00, -2.6392e+00],\n",
      "        [ 2.4100e-01,  1.7624e+00,  1.2897e+01, -5.0741e-02,  1.0543e-01,\n",
      "         -2.8627e+00, -1.6401e+00, -2.1726e+00, -3.9639e-01, -7.8829e+00],\n",
      "        [-8.1592e+00,  4.9080e+00, -6.4331e+00, -3.2782e+00,  1.4420e+01,\n",
      "         -8.9781e-01, -5.3709e+00,  6.7665e-02, -5.5502e-01,  5.2981e+00],\n",
      "        [-4.3519e+00,  3.9690e+00, -6.8612e+00, -5.9272e+00,  1.3247e+01,\n",
      "          6.2514e-01, -2.8808e+00, -3.2509e-01, -2.5620e+00,  5.0665e+00]])\n",
      "f_var:  tensor([[[ 9.4461, -2.1435, -0.3102,  ..., -0.2527, -0.4284, -0.3108],\n",
      "         [-2.1435, 16.7257, -0.6194,  ..., -0.5045, -0.8554, -0.6205],\n",
      "         [-0.3102, -0.6194,  2.9500,  ..., -0.0730, -0.1238, -0.0898],\n",
      "         ...,\n",
      "         [-0.2527, -0.5045, -0.0730,  ...,  2.4166, -0.1008, -0.0731],\n",
      "         [-0.4284, -0.8554, -0.1238,  ..., -0.1008,  4.0270, -0.1240],\n",
      "         [-0.3108, -0.6205, -0.0898,  ..., -0.0731, -0.1240,  2.9555]],\n",
      "\n",
      "        [[ 6.8414, -1.2418, -0.3704,  ..., -1.1313, -0.3437, -0.7792],\n",
      "         [-1.2418, 11.3078, -0.6658,  ..., -2.0336, -0.6178, -1.4007],\n",
      "         [-0.3704, -0.6658,  3.8396,  ..., -0.6065, -0.1843, -0.4177],\n",
      "         ...,\n",
      "         [-1.1313, -2.0336, -0.6065,  ..., 10.4820, -0.5628, -1.2759],\n",
      "         [-0.3437, -0.6178, -0.1843,  ..., -0.5628,  3.5763, -0.3876],\n",
      "         [-0.7792, -1.4007, -0.4177,  ..., -1.2759, -0.3876,  7.6167]],\n",
      "\n",
      "        [[ 4.6780, -0.2919, -0.3471,  ..., -0.3358, -0.2197, -0.3425],\n",
      "         [-0.2919,  1.2490, -0.0757,  ..., -0.0733, -0.0480, -0.0747],\n",
      "         [-0.3471, -0.0757,  1.4709,  ..., -0.0871, -0.0570, -0.0889],\n",
      "         ...,\n",
      "         [-0.3358, -0.0733, -0.0871,  ...,  1.4261, -0.0552, -0.0860],\n",
      "         [-0.2197, -0.0480, -0.0570,  ..., -0.0552,  0.9522, -0.0563],\n",
      "         [-0.3425, -0.0747, -0.0889,  ..., -0.0860, -0.0563,  1.4525]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.0723, -1.5851, -0.2975,  ..., -0.6603, -0.3367, -0.5331],\n",
      "         [-1.5851,  6.9351, -0.3547,  ..., -0.7872, -0.4014, -0.6356],\n",
      "         [-0.2975, -0.3547,  1.5898,  ..., -0.1478, -0.0753, -0.1193],\n",
      "         ...,\n",
      "         [-0.6603, -0.7872, -0.1478,  ...,  3.3481, -0.1672, -0.2648],\n",
      "         [-0.3367, -0.4014, -0.0753,  ..., -0.1672,  1.7892, -0.1350],\n",
      "         [-0.5331, -0.6356, -0.1193,  ..., -0.2648, -0.1350,  2.7544]],\n",
      "\n",
      "        [[ 9.6402, -3.8642, -0.7455,  ..., -0.6939, -0.4422, -0.3078],\n",
      "         [-3.8642, 18.6463, -1.9080,  ..., -1.7759, -1.1317, -0.7877],\n",
      "         [-0.7455, -1.9080,  5.1372,  ..., -0.3426, -0.2183, -0.1520],\n",
      "         ...,\n",
      "         [-0.6939, -1.7759, -0.3426,  ...,  4.8053, -0.2032, -0.1414],\n",
      "         [-0.4422, -1.1317, -0.2183,  ..., -0.2032,  3.1360, -0.0901],\n",
      "         [-0.3078, -0.7877, -0.1520,  ..., -0.1414, -0.0901,  2.2100]],\n",
      "\n",
      "        [[12.8758, -6.0555, -0.7576,  ..., -0.9350, -0.6038, -0.4433],\n",
      "         [-6.0555, 29.6263, -2.6184,  ..., -3.2314, -2.0868, -1.5319],\n",
      "         [-0.7576, -2.6184,  5.9975,  ..., -0.4043, -0.2611, -0.1917],\n",
      "         ...,\n",
      "         [-0.9350, -3.2314, -0.4043,  ...,  7.3069, -0.3222, -0.2365],\n",
      "         [-0.6038, -2.0868, -0.2611,  ..., -0.3222,  4.8329, -0.1528],\n",
      "         [-0.4433, -1.5319, -0.1917,  ..., -0.2365, -0.1528,  3.5884]]])\n",
      "f_mu[1200:1250]:  tensor([], size=(0, 10))\n",
      "f_var[1200:1250]:  tensor([], size=(0, 10, 10))\n",
      "f_mu[2450:2500]:  tensor([], size=(0, 10))\n",
      "f_var[2450:2500]:  tensor([], size=(0, 10, 10))\n",
      "alpha[0]:  tensor([9.3667e-01, 1.1100e+00, 1.7934e+00, 1.6730e+00, 8.2332e-01, 8.9482e-01,\n",
      "        8.5878e-01, 1.0343e+03, 8.6296e-01, 1.8176e+00])\n",
      "alpha[0:50]:  tensor([[9.3667e-01, 1.1100e+00, 1.7934e+00, 1.6730e+00, 8.2332e-01, 8.9482e-01,\n",
      "         8.5878e-01, 1.0343e+03, 8.6296e-01, 1.8176e+00],\n",
      "        [5.6270e+00, 6.7717e+00, 7.6972e+02, 1.0398e+00, 1.0361e+00, 8.4389e-01,\n",
      "         1.7196e+00, 9.6046e-01, 1.5592e+00, 8.1672e-01],\n",
      "        [9.8034e-01, 5.1338e+02, 9.0336e-01, 8.2634e-01, 2.5819e+00, 9.0702e-01,\n",
      "         1.4010e+00, 1.1332e+00, 1.0967e+00, 8.3463e-01],\n",
      "        [6.1580e+01, 2.1851e+00, 1.1695e+00, 8.1667e-01, 9.3696e-01, 9.2261e-01,\n",
      "         1.5041e+00, 9.0802e-01, 9.3408e-01, 1.1362e+00],\n",
      "        [8.6692e-01, 1.9926e+00, 8.4087e-01, 8.4702e-01, 1.0507e+03, 8.7323e-01,\n",
      "         9.0338e-01, 1.1918e+00, 8.8076e-01, 5.3037e+00],\n",
      "        [9.8759e-01, 6.8373e+02, 8.6439e-01, 8.2689e-01, 2.5946e+00, 8.5182e-01,\n",
      "         1.1417e+00, 1.3913e+00, 1.1728e+00, 8.6756e-01],\n",
      "        [8.2135e-01, 2.8224e+00, 9.0919e-01, 8.3271e-01, 2.9267e+02, 9.6148e-01,\n",
      "         1.0755e+00, 1.3161e+00, 4.0154e+00, 1.9561e+00],\n",
      "        [9.5380e-01, 1.2615e+00, 9.4386e-01, 9.5960e-01, 1.3714e+00, 9.5684e-01,\n",
      "         8.1915e-01, 8.6382e-01, 1.2836e+00, 4.1019e+01],\n",
      "        [8.6905e-01, 1.0223e+00, 9.2912e-01, 8.5523e-01, 8.2836e-01, 1.3055e+01,\n",
      "         1.2706e+00, 9.0892e-01, 1.5563e+00, 9.3898e-01],\n",
      "        [9.3114e-01, 1.0518e+00, 8.6712e-01, 1.0148e+00, 1.7334e+00, 8.2120e-01,\n",
      "         8.5273e-01, 2.0083e+00, 2.0221e+00, 3.7441e+02],\n",
      "        [6.1188e+01, 3.1523e+00, 1.3677e+00, 8.6626e-01, 8.2203e-01, 8.9941e-01,\n",
      "         9.8895e-01, 9.4632e-01, 8.8575e-01, 1.0888e+00],\n",
      "        [7.1644e+00, 5.7250e+00, 8.5813e-01, 8.6820e-01, 8.6850e-01, 1.6904e+00,\n",
      "         5.0635e+02, 8.5550e-01, 1.9301e+00, 8.3017e-01],\n",
      "        [9.0198e-01, 1.0796e+00, 8.2907e-01, 1.1750e+00, 1.6941e+01, 9.2307e-01,\n",
      "         8.2543e-01, 1.7171e+00, 1.8556e+00, 2.4047e+03],\n",
      "        [2.9392e+01, 3.4714e+00, 9.0795e-01, 8.1490e-01, 9.8905e-01, 1.0070e+00,\n",
      "         1.0700e+00, 1.2637e+00, 9.7063e-01, 1.5278e+00],\n",
      "        [9.2159e-01, 2.6957e+02, 8.3465e-01, 9.9013e-01, 1.7632e+00, 9.0733e-01,\n",
      "         9.2928e-01, 8.7583e-01, 1.2418e+00, 8.4147e-01],\n",
      "        [9.4670e-01, 2.5266e+00, 1.1738e+00, 2.2766e+00, 8.7632e-01, 4.7134e+01,\n",
      "         1.4738e+00, 1.0217e+00, 1.0077e+00, 8.1531e-01],\n",
      "        [9.0128e-01, 1.3870e+00, 8.4037e-01, 9.2764e-01, 2.7216e+00, 9.4336e-01,\n",
      "         8.2198e-01, 3.2636e+00, 1.2125e+00, 1.0628e+03],\n",
      "        [1.2934e+00, 1.6468e+00, 2.6438e+00, 2.2428e+00, 8.2343e-01, 1.0729e+00,\n",
      "         8.9037e-01, 8.6604e+02, 8.2645e-01, 3.7424e+00],\n",
      "        [9.2661e-01, 1.9696e+00, 1.5160e+00, 3.2961e+00, 8.6200e-01, 1.0944e+00,\n",
      "         9.8494e-01, 8.2453e-01, 1.1765e+00, 8.4742e-01],\n",
      "        [8.2881e-01, 2.4633e+00, 9.9656e-01, 8.2227e-01, 6.6253e+03, 1.4184e+00,\n",
      "         9.7624e-01, 2.8719e+00, 9.7787e-01, 1.5599e+00],\n",
      "        [8.8603e-01, 9.3350e-01, 8.4217e-01, 9.2656e-01, 2.2763e+00, 9.1070e-01,\n",
      "         8.3785e-01, 1.8225e+00, 8.8222e-01, 1.9948e+02],\n",
      "        [1.3395e+00, 1.5955e+00, 1.0894e+00, 1.0434e+00, 1.1844e+00, 1.3375e+01,\n",
      "         2.1502e+02, 8.1860e-01, 3.3477e+00, 8.3079e-01],\n",
      "        [4.3589e+00, 6.9255e+00, 8.1349e-01, 8.7937e-01, 2.8726e+00, 3.0085e+00,\n",
      "         4.6034e+02, 1.0707e+00, 1.8402e+00, 9.3775e-01],\n",
      "        [1.0797e+00, 1.0668e+00, 8.3224e-01, 1.7075e+00, 8.3190e-01, 7.8243e+02,\n",
      "         1.7732e+00, 8.5005e-01, 9.8904e-01, 1.1443e+00],\n",
      "        [8.6314e-01, 2.2394e+00, 8.8567e-01, 8.2197e-01, 7.8370e+03, 1.0644e+00,\n",
      "         8.7357e-01, 1.4483e+00, 9.4073e-01, 4.7143e+00],\n",
      "        [4.4604e+01, 2.1016e+00, 1.2673e+00, 8.2130e-01, 8.8487e-01, 8.3889e-01,\n",
      "         1.4221e+00, 9.5111e-01, 1.1991e+00, 1.3290e+00],\n",
      "        [1.0574e+00, 2.2892e+00, 1.8946e+00, 1.2563e+00, 8.8971e-01, 1.0640e+00,\n",
      "         8.3364e-01, 7.2734e+02, 8.2105e-01, 6.6460e+00],\n",
      "        [8.4309e-01, 1.6393e+00, 8.6798e-01, 8.3609e-01, 4.0463e+03, 1.0056e+00,\n",
      "         8.7361e-01, 1.0880e+00, 9.0015e-01, 1.5810e+00],\n",
      "        [4.8275e+01, 2.9350e+00, 1.7988e+00, 8.3306e-01, 8.5726e-01, 9.4503e-01,\n",
      "         9.0125e-01, 1.0813e+00, 8.6323e-01, 8.6822e-01],\n",
      "        [8.6655e-01, 9.5987e+01, 8.8828e-01, 9.1875e-01, 2.0791e+00, 8.8782e-01,\n",
      "         8.9284e-01, 9.6946e-01, 1.7069e+00, 8.2835e-01],\n",
      "        [9.6612e-01, 1.0123e+00, 8.7039e-01, 2.9002e+03, 8.2417e-01, 1.4783e+00,\n",
      "         9.0893e-01, 1.0230e+00, 8.5597e-01, 2.2584e+00],\n",
      "        [8.6293e-01, 1.0932e+02, 8.6258e-01, 8.6721e-01, 2.8982e+00, 8.7700e-01,\n",
      "         8.3319e-01, 1.0209e+00, 1.8141e+00, 1.0422e+00],\n",
      "        [8.4333e-01, 1.7038e+00, 9.8566e-01, 3.7858e+02, 8.7376e-01, 1.2519e+00,\n",
      "         8.6001e-01, 1.0197e+00, 9.5469e-01, 8.3707e-01],\n",
      "        [1.2949e+00, 2.2120e+00, 9.8321e-01, 8.3795e-01, 4.2173e+01, 8.5833e-01,\n",
      "         1.2011e+00, 9.2031e-01, 8.5203e-01, 8.5474e-01],\n",
      "        [9.7150e-01, 1.7047e+00, 5.5667e+00, 2.3117e+00, 8.1945e-01, 8.4417e-01,\n",
      "         8.8009e-01, 1.3265e+03, 1.2123e+00, 1.1098e+00],\n",
      "        [1.0251e+00, 4.1409e+00, 2.8553e+02, 1.1855e+00, 8.2303e-01, 8.4178e-01,\n",
      "         9.7672e-01, 1.0200e+00, 1.1212e+00, 8.8339e-01],\n",
      "        [9.0432e-01, 1.6114e+00, 1.6940e+01, 1.3467e+00, 8.2822e-01, 8.3454e-01,\n",
      "         8.9982e-01, 9.9257e+01, 9.4882e-01, 9.6258e-01],\n",
      "        [8.8366e-01, 1.9211e+02, 8.4297e-01, 8.5063e-01, 1.7809e+00, 8.6830e-01,\n",
      "         8.6562e-01, 9.8322e-01, 1.8561e+00, 9.2966e-01],\n",
      "        [7.6864e+00, 7.9339e+00, 1.0384e+03, 1.8713e+00, 8.1542e-01, 9.2520e-01,\n",
      "         1.0415e+00, 3.3214e+00, 1.9133e+00, 8.4880e-01],\n",
      "        [1.1528e+00, 3.0031e+02, 8.5097e-01, 9.1172e-01, 1.1487e+00, 8.3622e-01,\n",
      "         1.0439e+00, 9.1396e-01, 2.1005e+00, 8.4087e-01],\n",
      "        [8.7804e-01, 2.1743e+02, 8.6893e-01, 8.4426e-01, 1.4360e+00, 8.4726e-01,\n",
      "         9.3164e-01, 1.1213e+00, 1.2299e+00, 8.6987e-01],\n",
      "        [9.1475e-01, 1.8019e+00, 1.9406e+01, 1.7426e+00, 8.4203e-01, 8.4260e-01,\n",
      "         8.8788e-01, 9.9656e+01, 8.8509e-01, 8.5330e-01],\n",
      "        [1.0626e+00, 9.8388e+00, 8.1184e-01, 9.2006e-01, 1.5614e+04, 2.3265e+00,\n",
      "         1.3767e+00, 4.8411e+00, 2.3227e+00, 5.1251e+01],\n",
      "        [8.8800e-01, 3.6132e+00, 2.1147e+01, 9.4372e-01, 3.7309e+00, 9.5779e-01,\n",
      "         9.8581e-01, 1.2351e+00, 9.8237e-01, 8.1628e-01],\n",
      "        [8.8051e-01, 1.0174e+00, 9.5263e-01, 1.0245e+02, 8.9081e-01, 2.2369e+00,\n",
      "         1.0231e+00, 8.3685e-01, 8.4796e-01, 8.8169e-01],\n",
      "        [1.4844e+00, 2.3162e+00, 9.7172e-01, 8.9311e-01, 8.1794e-01, 1.7299e+02,\n",
      "         2.1162e+00, 8.6941e-01, 1.9931e+00, 9.0449e-01],\n",
      "        [8.5759e-01, 1.1713e+01, 9.4144e-01, 9.3797e-01, 1.4271e+00, 9.1192e-01,\n",
      "         8.5696e-01, 8.7607e-01, 1.7135e+00, 8.3861e-01],\n",
      "        [2.2390e+00, 3.3481e+00, 3.6113e+04, 2.0646e+00, 2.1987e+00, 1.0690e+00,\n",
      "         1.4307e+00, 1.1980e+00, 1.7702e+00, 8.1129e-01],\n",
      "        [8.3192e-01, 2.1769e+00, 8.2586e-01, 9.1327e-01, 4.8723e+03, 1.0928e+00,\n",
      "         8.8565e-01, 1.2557e+00, 1.1230e+00, 1.6397e+01],\n",
      "        [9.0462e-01, 1.5295e+00, 8.2136e-01, 8.4831e-01, 4.2532e+02, 1.2395e+00,\n",
      "         9.4807e-01, 1.1119e+00, 9.0970e-01, 5.9035e+00]])\n",
      "alpha[1200:1250]:  tensor([], size=(0, 10))\n",
      "alpha[2450:2500]:  tensor([], size=(0, 10))\n",
      "alpha after correction:  tensor([[9.3667e-01, 1.1100e+00, 1.7934e+00, 1.6730e+00, 8.2332e-01, 8.9482e-01,\n",
      "         8.5878e-01, 1.0343e+03, 8.6296e-01, 1.8176e+00],\n",
      "        [5.6270e+00, 6.7717e+00, 7.6972e+02, 1.0398e+00, 1.0361e+00, 8.4389e-01,\n",
      "         1.7196e+00, 9.6046e-01, 1.5592e+00, 8.1672e-01],\n",
      "        [9.8034e-01, 5.1338e+02, 9.0336e-01, 8.2634e-01, 2.5819e+00, 9.0702e-01,\n",
      "         1.4010e+00, 1.1332e+00, 1.0967e+00, 8.3463e-01],\n",
      "        [6.1580e+01, 2.1851e+00, 1.1695e+00, 8.1667e-01, 9.3696e-01, 9.2261e-01,\n",
      "         1.5041e+00, 9.0802e-01, 9.3408e-01, 1.1362e+00],\n",
      "        [8.6692e-01, 1.9926e+00, 8.4087e-01, 8.4702e-01, 1.0507e+03, 8.7323e-01,\n",
      "         9.0338e-01, 1.1918e+00, 8.8076e-01, 5.3037e+00],\n",
      "        [9.8759e-01, 6.8373e+02, 8.6439e-01, 8.2689e-01, 2.5946e+00, 8.5182e-01,\n",
      "         1.1417e+00, 1.3913e+00, 1.1728e+00, 8.6756e-01],\n",
      "        [8.2135e-01, 2.8224e+00, 9.0919e-01, 8.3271e-01, 2.9267e+02, 9.6148e-01,\n",
      "         1.0755e+00, 1.3161e+00, 4.0154e+00, 1.9561e+00],\n",
      "        [9.5380e-01, 1.2615e+00, 9.4386e-01, 9.5960e-01, 1.3714e+00, 9.5684e-01,\n",
      "         8.1915e-01, 8.6382e-01, 1.2836e+00, 4.1019e+01],\n",
      "        [8.6905e-01, 1.0223e+00, 9.2912e-01, 8.5523e-01, 8.2836e-01, 1.3055e+01,\n",
      "         1.2706e+00, 9.0892e-01, 1.5563e+00, 9.3898e-01],\n",
      "        [9.3114e-01, 1.0518e+00, 8.6712e-01, 1.0148e+00, 1.7334e+00, 8.2120e-01,\n",
      "         8.5273e-01, 2.0083e+00, 2.0221e+00, 3.7441e+02]])\n",
      "Dir variance:  tensor([], size=(0, 10))\n",
      "[8.9626561e-04 1.0621211e-03 1.7160071e-03 1.6008758e-03 7.8780839e-04\n",
      " 8.5622194e-04 8.2173402e-04 9.8969406e-01 8.2573900e-04 1.7391600e-03]\n"
     ]
    }
   ],
   "source": [
    "print(la_diag(MNIST_test0[0], link_approx='bridge').numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.9913402e-03 1.8480839e-01 5.8268136e-01 3.2135954e-03 6.4066472e-04\n",
      " 2.1357624e-02 1.9662681e-01 1.0369298e-03 6.0606119e-04 3.7219037e-05]\n"
     ]
    }
   ],
   "source": [
    "## predict samples with diag & mc\n",
    "FMNIST_test0 = next(iter(FMNIST_test_loader))\n",
    "#print(MNIST_test0)\n",
    "print(la_diag(FMNIST_test0[0], link_approx='mc', n_samples=100).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_mu:  tensor([[-1.8215e-01,  8.1830e+00,  4.4991e+00, -1.4375e+00, -2.8327e+00,\n",
      "          7.8222e-01,  3.3740e+00, -2.5723e+00, -3.1155e+00, -6.6983e+00],\n",
      "        [ 4.8294e-01,  8.0922e+00,  9.2274e-01, -8.8659e-01, -3.6061e+00,\n",
      "         -3.9020e+00,  1.6423e+00, -3.0557e+00,  1.8622e+00, -1.5520e+00],\n",
      "        [ 2.5435e+00,  2.6445e+00, -2.5704e+00, -7.3133e+00,  6.9965e+00,\n",
      "         -1.9633e+00, -1.1477e+00, -2.3834e-02,  1.8111e+00, -9.7706e-01],\n",
      "        [-1.2277e+00,  2.1735e+00,  7.4563e-01, -1.0576e+00,  1.9697e+00,\n",
      "         -1.7173e+00, -1.4112e+00, -5.0320e-01,  2.4731e+00, -1.4450e+00],\n",
      "        [ 1.1996e+00,  3.6774e+00, -5.0066e-01, -7.7762e-01,  3.9521e-01,\n",
      "         -2.6334e+00, -2.2705e-01, -1.0688e+00,  4.3559e-01, -5.0019e-01],\n",
      "        [ 1.9967e+00,  2.6846e+00, -2.5359e+00, -5.0910e+00,  4.8310e+00,\n",
      "         -2.0436e+00, -1.1928e+00,  1.3191e-01,  1.8784e+00, -6.5938e-01],\n",
      "        [-1.1577e-01,  3.4852e+00, -2.0955e+00, -3.7798e+00,  2.5990e+00,\n",
      "         -3.5052e-01,  5.2670e-01, -9.2553e-02, -3.6557e-01,  1.8876e-01],\n",
      "        [ 1.0695e-02,  3.1968e+00, -5.2633e-01, -3.4626e-01,  3.4368e-01,\n",
      "         -1.8915e+00, -5.1423e-01, -1.1046e+00,  1.1532e+00, -3.2146e-01],\n",
      "        [-3.5095e+00, -4.2864e-01,  2.9763e+00, -1.9409e-01,  1.4586e+00,\n",
      "          7.8791e-01,  2.6338e-01,  2.1429e+00, -9.6935e-01, -2.5275e+00],\n",
      "        [-7.5508e-01, -6.3410e-01,  2.5176e+00, -1.7575e+00,  3.0202e+00,\n",
      "          7.8318e-01, -8.5736e-01, -5.7728e-01, -2.1783e+00,  4.3857e-01],\n",
      "        [ 1.3489e+00,  3.7612e+00,  5.4779e-01, -2.3832e-01,  5.0214e-01,\n",
      "         -2.4946e+00,  1.4878e+00, -4.0232e+00,  2.5898e-01, -1.1508e+00],\n",
      "        [ 5.7552e-01,  8.5951e+00,  2.8449e+00, -3.3831e+00, -2.0370e+00,\n",
      "          2.0979e+00,  3.1369e+00, -3.1729e+00, -1.8712e+00, -6.7862e+00],\n",
      "        [-1.5812e-01,  2.4935e+00,  6.6112e+00, -2.8272e+00,  5.7350e-01,\n",
      "          2.2104e+00,  3.0517e+00, -2.6463e+00, -4.9794e+00, -4.3293e+00],\n",
      "        [-1.8004e-01,  3.5173e+00, -4.9555e-01, -1.9284e+00,  2.4242e+00,\n",
      "         -1.4751e+00,  1.5135e-01, -8.8021e-01,  1.9835e+00, -3.1171e+00],\n",
      "        [ 1.0815e+00,  5.1392e+00, -1.5997e+00, -1.3786e+00,  6.8756e-01,\n",
      "         -3.4025e+00, -1.1866e+00, -4.6680e-01,  1.2348e+00, -1.0866e-01],\n",
      "        [-6.9674e-01,  1.5596e+00, -2.9818e-01, -8.8399e-01,  1.6854e+00,\n",
      "         -2.8153e-01, -7.4311e-01, -9.8700e-01,  1.7224e+00, -1.0768e+00],\n",
      "        [ 1.1361e+00,  3.4752e+00,  4.5639e-01, -1.3065e+00, -2.7992e-01,\n",
      "         -1.1301e+00,  1.7407e+00, -2.0961e+00,  3.4202e-01, -2.3378e+00],\n",
      "        [ 8.5440e-01,  2.2932e+00, -1.0264e+00,  5.2531e-01,  1.1777e+00,\n",
      "         -1.2903e+00, -2.0463e+00, -8.6398e-01,  2.4581e+00, -2.0818e+00],\n",
      "        [ 4.9013e+00,  3.3226e+00,  2.6259e+00, -3.4750e+00, -5.5916e+00,\n",
      "          1.0570e+00,  3.3199e-01, -8.5487e-01,  1.1366e-01, -2.4310e+00],\n",
      "        [-3.1525e-01,  9.8846e-01, -1.8355e+00,  2.7452e-01,  1.7679e+00,\n",
      "         -3.0858e+00, -4.6176e-01,  5.7189e-01,  2.4405e+00, -3.4488e-01],\n",
      "        [ 8.4147e-03,  1.6260e+00, -2.3780e-02,  5.1051e-02,  2.0197e-01,\n",
      "         -1.4530e+00, -4.1945e-01, -4.5535e-01,  5.5141e-01, -8.7262e-02],\n",
      "        [ 1.7153e+00,  2.0666e+00,  4.9182e+00, -3.0388e+00,  6.1356e-01,\n",
      "          1.1311e+00,  1.8970e+00, -2.3801e+00, -4.8037e+00, -2.1192e+00],\n",
      "        [ 1.7793e+00,  3.4188e+00,  6.4991e+00, -3.8743e+00,  2.6126e-01,\n",
      "          1.4266e+00,  3.4857e+00, -3.0399e+00, -5.7830e+00, -4.1735e+00],\n",
      "        [ 4.6697e-01,  2.3005e+00,  4.3395e+00, -4.5546e+00, -1.0325e+00,\n",
      "          6.5132e+00,  1.0598e+00, -4.7536e+00, -2.5529e+00, -1.7864e+00],\n",
      "        [ 6.4890e-01,  2.1229e+00, -1.3496e+00, -4.5328e+00,  4.4634e+00,\n",
      "         -1.3900e+00, -1.3313e+00,  3.0469e-01,  1.5276e+00, -4.6394e-01],\n",
      "        [ 1.5667e+00,  4.2735e+00, -6.4973e-01, -1.4072e+00, -7.1163e-02,\n",
      "         -1.7989e+00,  6.4971e-01, -2.3728e+00,  1.2309e+00, -1.4209e+00],\n",
      "        [ 1.7508e+00,  5.2240e+00, -1.5519e+00, -3.4051e-01,  3.4620e-01,\n",
      "         -3.2298e+00, -3.3590e-02, -2.2422e+00,  1.6629e+00, -1.5858e+00],\n",
      "        [-3.7175e-01,  2.4219e+00, -1.0049e+00, -2.7189e+00,  4.3069e+00,\n",
      "         -2.1694e+00, -5.5482e-01, -1.3879e+00,  2.7702e+00, -1.2913e+00],\n",
      "        [ 2.4052e+00,  7.5646e+00,  4.4270e+00, -2.0147e+00, -1.4985e+00,\n",
      "          1.2407e+00,  1.9482e+00, -5.2728e+00, -1.5681e+00, -7.2317e+00],\n",
      "        [-2.2324e-01,  1.6587e+00, -6.3130e-01, -1.0900e+00,  2.5975e+00,\n",
      "         -1.8859e+00,  1.1568e-01, -3.4088e-01,  5.7813e-01, -7.7857e-01],\n",
      "        [ 1.4149e+00,  3.4162e+00,  1.8920e+00, -3.7049e+00, -2.9798e+00,\n",
      "          1.0198e-01,  1.4408e+00,  1.8919e+00, -5.7851e-01, -2.8947e+00],\n",
      "        [-3.2546e+00,  6.4119e+00,  5.0507e+00, -1.1399e+00,  7.8359e-01,\n",
      "          5.8803e-01,  3.6371e-01, -2.0529e+00, -1.0651e-01, -6.6439e+00],\n",
      "        [-8.4120e-02,  3.3763e+00, -1.9866e+00, -2.6189e+00,  2.6360e+00,\n",
      "          2.2204e-01,  2.9974e-01, -2.4131e+00,  2.0536e+00, -1.4849e+00],\n",
      "        [-4.2303e-01,  1.8547e+00, -5.4993e-01, -5.4458e-01,  1.8003e+00,\n",
      "         -3.1672e+00, -1.4318e+00,  2.0872e-01,  4.7578e+00, -2.5050e+00],\n",
      "        [-6.1107e+00,  6.5042e+00,  1.8239e+00, -3.6449e+00,  1.0356e+00,\n",
      "         -5.8296e-01,  2.8299e+00,  1.1458e+00,  1.8129e+00, -4.8138e+00],\n",
      "        [-6.0938e-01,  8.2655e-03, -8.1031e-01, -1.0363e+00, -6.1133e-01,\n",
      "          5.4607e-01,  2.9002e-01,  1.0332e+00,  2.5856e+00, -1.3958e+00],\n",
      "        [ 4.3793e-01,  1.5414e+00,  1.8717e+00, -3.5448e+00,  2.5669e+00,\n",
      "          1.3856e+00, -6.8327e-01,  2.4666e-01, -1.7787e+00, -2.0435e+00],\n",
      "        [-3.6830e+00,  1.2152e+00,  3.8789e+00, -2.5345e-01,  2.3288e+00,\n",
      "          1.1734e+00,  1.4558e+00,  3.8054e-01, -2.8822e+00, -3.6140e+00],\n",
      "        [ 1.6906e+00,  4.4715e-01,  3.1532e+00, -3.7976e+00,  3.1813e+00,\n",
      "          1.1931e+00, -1.2600e+00, -4.3958e-01, -4.1411e+00, -2.6912e-02],\n",
      "        [ 1.4647e+00,  1.1673e+01,  3.2609e+00, -8.6563e-01, -2.5002e+00,\n",
      "         -7.4169e-01,  1.3365e+00, -3.6075e+00, -2.0204e+00, -7.9999e+00],\n",
      "        [-6.3143e-01,  1.9199e+00, -2.6616e+00, -7.2565e-01,  5.1890e-01,\n",
      "         -2.4518e+00, -7.2004e-01,  5.3751e-01,  5.8927e+00, -1.6784e+00],\n",
      "        [ 1.0813e+00,  3.5835e+00, -2.2166e+00, -5.4922e+00,  6.3646e+00,\n",
      "         -2.1011e+00, -2.0414e+00,  4.0544e-01,  1.5333e+00, -1.1170e+00],\n",
      "        [-1.0152e+00,  1.6762e+00, -1.3498e+00, -1.1087e+00,  3.0635e+00,\n",
      "         -1.1132e+00,  3.4227e-01, -7.7884e-01,  8.5371e-01, -5.6997e-01],\n",
      "        [ 1.2671e+00,  1.0638e+01,  5.3154e+00, -2.0233e+00, -1.2524e+00,\n",
      "         -1.1489e+00,  7.1850e-01, -1.8349e+00, -3.7215e+00, -7.9577e+00],\n",
      "        [ 5.9307e-01,  3.8797e+00, -1.5498e+00, -6.2007e-01,  1.2838e+00,\n",
      "         -3.2676e+00,  4.2468e-02, -1.2890e+00,  1.4356e+00, -5.0818e-01],\n",
      "        [ 1.2131e+00,  5.0105e+00,  1.1444e+00, -6.3960e-01, -9.2765e-01,\n",
      "          6.0614e-01,  2.5479e+00, -3.1647e+00, -1.6151e+00, -4.1749e+00],\n",
      "        [ 4.7949e-01,  3.3333e+00, -1.1655e+00, -8.0278e-02, -5.4682e-02,\n",
      "         -1.9414e+00, -4.4864e-01, -8.5655e-01,  9.1292e-01, -1.7863e-01],\n",
      "        [ 1.2082e+00,  1.6553e+00, -1.6685e+00, -2.6140e+00,  3.0955e+00,\n",
      "         -1.0359e+00, -8.6337e-01, -1.5845e+00,  2.5532e+00, -7.4599e-01],\n",
      "        [ 5.6288e-01,  4.7343e+00, -6.7683e-01,  6.2680e-01, -8.9245e-01,\n",
      "         -6.2590e-01, -8.5846e-01, -2.4624e+00,  6.7999e-01, -1.0879e+00],\n",
      "        [ 5.8616e-01,  3.9596e+00, -1.0968e+00, -6.8643e-01,  7.1193e-01,\n",
      "         -2.8771e+00, -7.4055e-01, -4.9638e-01,  8.7431e-01, -2.3478e-01]])\n",
      "f_var:  tensor([[[ 2.2710e+00, -1.0475e+00, -8.5686e-02,  ..., -2.2669e-01,\n",
      "          -1.1166e-01, -1.7851e-01],\n",
      "         [-1.0475e+00,  7.8122e+00, -4.7377e-01,  ..., -1.2534e+00,\n",
      "          -6.1740e-01, -9.8699e-01],\n",
      "         [-8.5686e-02, -4.7377e-01,  1.0740e+00,  ..., -1.0253e-01,\n",
      "          -5.0503e-02, -8.0736e-02],\n",
      "         ...,\n",
      "         [-2.2669e-01, -1.2534e+00, -1.0253e-01,  ...,  2.6728e+00,\n",
      "          -1.3361e-01, -2.1360e-01],\n",
      "         [-1.1166e-01, -6.1740e-01, -5.0503e-02,  ..., -1.3361e-01,\n",
      "           1.3843e+00, -1.0521e-01],\n",
      "         [-1.7851e-01, -9.8699e-01, -8.0736e-02,  ..., -2.1360e-01,\n",
      "          -1.0521e-01,  2.1501e+00]],\n",
      "\n",
      "        [[ 2.7941e+00, -1.9270e+00, -7.4745e-02,  ..., -1.4254e-01,\n",
      "          -5.5867e-02, -7.2186e-02],\n",
      "         [-1.9270e+00,  1.6045e+01, -1.2169e+00,  ..., -2.3206e+00,\n",
      "          -9.0957e-01, -1.1753e+00],\n",
      "         [-7.4745e-02, -1.2169e+00,  1.7921e+00,  ..., -9.0015e-02,\n",
      "          -3.5281e-02, -4.5587e-02],\n",
      "         ...,\n",
      "         [-1.4254e-01, -2.3206e+00, -9.0015e-02,  ...,  3.3358e+00,\n",
      "          -6.7280e-02, -8.6933e-02],\n",
      "         [-5.5867e-02, -9.0957e-01, -3.5281e-02,  ..., -6.7280e-02,\n",
      "           1.3484e+00, -3.4073e-02],\n",
      "         [-7.2186e-02, -1.1753e+00, -4.5587e-02,  ..., -8.6933e-02,\n",
      "          -3.4073e-02,  1.7323e+00]],\n",
      "\n",
      "        [[ 3.4537e+00, -1.1514e+00, -2.8662e-01,  ..., -3.4731e-01,\n",
      "          -1.4924e-01, -1.9993e-01],\n",
      "         [-1.1514e+00,  8.5021e+00, -9.1511e-01,  ..., -1.1089e+00,\n",
      "          -4.7650e-01, -6.3834e-01],\n",
      "         [-2.8662e-01, -9.1511e-01,  2.8037e+00,  ..., -2.7604e-01,\n",
      "          -1.1861e-01, -1.5890e-01],\n",
      "         ...,\n",
      "         [-3.4731e-01, -1.1089e+00, -2.7604e-01,  ...,  3.3390e+00,\n",
      "          -1.4373e-01, -1.9255e-01],\n",
      "         [-1.4924e-01, -4.7650e-01, -1.1861e-01,  ..., -1.4373e-01,\n",
      "           1.5168e+00, -8.2741e-02],\n",
      "         [-1.9993e-01, -6.3834e-01, -1.5890e-01,  ..., -1.9255e-01,\n",
      "          -8.2741e-02,  2.0038e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.9209e+00, -4.3958e-01, -1.8712e-01,  ..., -2.1602e-01,\n",
      "          -9.1800e-02, -1.2916e-01],\n",
      "         [-4.3958e-01,  2.6680e+00, -2.8150e-01,  ..., -3.2498e-01,\n",
      "          -1.3810e-01, -1.9431e-01],\n",
      "         [-1.8712e-01, -2.8150e-01,  1.2974e+00,  ..., -1.3834e-01,\n",
      "          -5.8787e-02, -8.2713e-02],\n",
      "         ...,\n",
      "         [-2.1602e-01, -3.2498e-01, -1.3834e-01,  ...,  1.4764e+00,\n",
      "          -6.7867e-02, -9.5489e-02],\n",
      "         [-9.1800e-02, -1.3810e-01, -5.8787e-02,  ..., -6.7867e-02,\n",
      "           6.6643e-01, -4.0578e-02],\n",
      "         [-1.2916e-01, -1.9431e-01, -8.2713e-02,  ..., -9.5489e-02,\n",
      "          -4.0578e-02,  9.2115e-01]],\n",
      "\n",
      "        [[ 1.0801e+00, -7.7481e-01, -2.9639e-02,  ..., -4.9133e-02,\n",
      "          -2.2915e-02, -2.8539e-02],\n",
      "         [-7.7481e-01,  5.9717e+00, -5.0457e-01,  ..., -8.3642e-01,\n",
      "          -3.9009e-01, -4.8583e-01],\n",
      "         [-2.9639e-02, -5.0457e-01,  7.1370e-01,  ..., -3.1996e-02,\n",
      "          -1.4922e-02, -1.8585e-02],\n",
      "         ...,\n",
      "         [-4.9133e-02, -8.3642e-01, -3.1996e-02,  ...,  1.1621e+00,\n",
      "          -2.4737e-02, -3.0808e-02],\n",
      "         [-2.2915e-02, -3.9009e-01, -1.4922e-02,  ..., -2.4737e-02,\n",
      "           5.5516e-01, -1.4368e-02],\n",
      "         [-2.8539e-02, -4.8583e-01, -1.8585e-02,  ..., -3.0808e-02,\n",
      "          -1.4368e-02,  6.8789e-01]],\n",
      "\n",
      "        [[ 8.4996e-01, -6.2535e-01, -2.2313e-02,  ..., -3.4645e-02,\n",
      "          -1.6586e-02, -1.8927e-02],\n",
      "         [-6.2535e-01,  5.5136e+00, -4.8561e-01,  ..., -7.5399e-01,\n",
      "          -3.6096e-01, -4.1192e-01],\n",
      "         [-2.2313e-02, -4.8561e-01,  6.6501e-01,  ..., -2.6903e-02,\n",
      "          -1.2880e-02, -1.4698e-02],\n",
      "         ...,\n",
      "         [-3.4645e-02, -7.5399e-01, -2.6903e-02,  ...,  1.0177e+00,\n",
      "          -1.9998e-02, -2.2821e-02],\n",
      "         [-1.6586e-02, -3.6096e-01, -1.2880e-02,  ..., -1.9998e-02,\n",
      "           4.9762e-01, -1.0925e-02],\n",
      "         [-1.8927e-02, -4.1192e-01, -1.4698e-02,  ..., -2.2821e-02,\n",
      "          -1.0925e-02,  5.6633e-01]]])\n",
      "f_mu[1200:1250]:  tensor([], size=(0, 10))\n",
      "f_var[1200:1250]:  tensor([], size=(0, 10, 10))\n",
      "f_mu[2450:2500]:  tensor([], size=(0, 10))\n",
      "f_var[2450:2500]:  tensor([], size=(0, 10, 10))\n",
      "alpha[0]:  tensor([  1.9446,  24.9350, 100.0111,   1.2202,   0.9447,   3.1675,  11.4864,\n",
      "          1.0678,   0.8915,   0.8134])\n",
      "alpha[0:50]:  tensor([[  1.9446,  24.9350, 100.0111,   1.2202,   0.9447,   3.1675,  11.4864,\n",
      "           1.0678,   0.8915,   0.8134],\n",
      "        [  1.2251,   3.2010,   1.4344,   0.9859,   0.8351,   0.8305,   1.5175,\n",
      "           0.8598,   2.3831,   0.8979],\n",
      "        [  2.7198,   2.0099,   0.9052,   0.8139,  76.1733,   0.9474,   1.0711,\n",
      "           1.2822,   2.9258,   1.0450],\n",
      "        [  0.8825,   1.8657,   1.2406,   0.8805,   2.6093,   0.8397,   0.8731,\n",
      "           0.9221,   6.2879,   0.8408],\n",
      "        [  2.6506,   2.9678,   0.9613,   0.9271,   1.3665,   0.8160,   1.0780,\n",
      "           0.9073,   1.5246,   0.9619],\n",
      "        [  1.8325,   1.6093,   0.8579,   0.8190,  11.7611,   0.8785,   0.9618,\n",
      "           1.1392,   2.4216,   0.9889],\n",
      "        [  1.1011,   2.1901,   0.8459,   0.8174,   4.4436,   1.0361,   1.2965,\n",
      "           1.1077,   1.0239,   1.2106],\n",
      "        [  1.2053,   4.0182,   0.9501,   1.0195,   1.4974,   0.8161,   1.0129,\n",
      "           0.8783,   5.2141,   1.0123],\n",
      "        [  0.8405,   0.9417,   1.9737,   0.9420,   1.2594,   1.0780,   0.9771,\n",
      "           1.3729,   0.8775,   0.8292],\n",
      "        [  0.9073,   0.9263,   1.7458,   0.8495,   2.6234,   1.0623,   0.9044,\n",
      "           0.9048,   0.8300,   1.0140],\n",
      "        [  1.9981,   2.6911,   1.5552,   1.1375,   1.4772,   0.8386,   2.0743,\n",
      "           0.8176,   1.3630,   0.9317],\n",
      "        [  2.2159,  13.1103,  11.3482,   0.9090,   1.0275,   5.1126,   7.4812,\n",
      "           0.9595,   1.0390,   0.8134],\n",
      "        [  1.2776,   1.8874,  65.3492,   0.8978,   1.5846,   2.7959,   2.8802,\n",
      "           0.9207,   0.8207,   0.8358],\n",
      "        [  1.1433,   7.7481,   1.0415,   0.8792,   7.1318,   0.9004,   1.2367,\n",
      "           0.9770,   6.1319,   0.8161],\n",
      "        [  1.5380,   2.5377,   0.8669,   0.8971,   1.3508,   0.8182,   0.9427,\n",
      "           1.0169,   2.0490,   1.0775],\n",
      "        [  0.8905,   1.6414,   0.9191,   0.8629,   2.4898,   0.9248,   0.8887,\n",
      "           0.8544,   4.2534,   0.8401],\n",
      "        [  2.5260,   3.6490,   1.7432,   0.9028,   1.1427,   0.9168,   3.1516,\n",
      "           0.8492,   1.6411,   0.8182],\n",
      "        [  1.1353,   1.2365,   0.8747,   1.0899,   1.2413,   0.8604,   0.8646,\n",
      "           0.9029,   3.1309,   0.8299],\n",
      "        [ 38.0816,   5.1057,  14.9860,   0.8677,   0.8132,   3.7802,   2.1783,\n",
      "           1.4427,   2.0993,   0.9708],\n",
      "        [  1.0372,   1.2707,   0.8520,   1.1759,   2.1593,   0.8168,   1.0319,\n",
      "           1.2823,   6.3139,   1.0044],\n",
      "        [  0.9852,   1.3435,   0.9744,   0.9994,   1.0528,   0.8198,   0.9224,\n",
      "           0.8897,   1.3759,   0.9534],\n",
      "        [  1.7048,   1.5269,  13.9178,   0.8675,   1.4160,   1.5701,   1.7727,\n",
      "           0.8999,   0.8171,   0.8982],\n",
      "        [  1.8379,   2.0849,  28.2754,   0.8697,   1.3764,   1.8193,   2.9178,\n",
      "           0.9142,   0.8187,   0.8542],\n",
      "        [  1.0436,   1.1179,   2.4216,   0.8375,   0.9318,   4.5898,   1.1041,\n",
      "           0.8358,   0.8636,   0.8938],\n",
      "        [  1.4880,   2.5262,   0.9230,   0.8145,  72.5983,   0.9200,   0.9575,\n",
      "           1.3568,   3.7880,   1.0572],\n",
      "        [  4.5484,   4.2673,   0.9620,   0.9038,   1.1773,   0.8436,   1.6809,\n",
      "           0.8289,   3.6263,   0.8454],\n",
      "        [  2.1461,   2.5150,   0.8752,   1.0363,   1.2099,   0.8229,   1.1073,\n",
      "           0.8619,   2.6633,   0.8661],\n",
      "        [  0.9891,   1.5385,   0.9065,   0.8350,  14.2342,   0.8421,   0.9697,\n",
      "           0.8878,   6.2719,   0.8712],\n",
      "        [  9.3802,  34.3921, 103.2011,   1.1683,   1.3615,   5.4680,   6.5242,\n",
      "           0.8782,   1.2327,   0.8125],\n",
      "        [  0.9426,   1.2122,   0.8891,   0.8649,   3.5546,   0.8297,   0.9913,\n",
      "           0.9292,   1.1764,   0.8701],\n",
      "        [  1.5110,   1.6667,   2.2302,   0.8303,   0.8385,   1.1657,   1.3814,\n",
      "           2.1072,   1.0173,   0.8358],\n",
      "        [  0.9073,   4.2149,  26.6098,   1.1019,   1.6897,   1.6392,   1.4996,\n",
      "           1.0224,   1.3427,   0.8130],\n",
      "        [  1.1046,   8.2520,   0.8472,   0.8358,   7.8620,   1.1954,   1.1975,\n",
      "           0.8370,   6.1625,   0.8664],\n",
      "        [  1.0498,   1.5187,   1.0035,   1.0170,   2.4976,   0.8261,   0.9469,\n",
      "           1.1932,  71.0694,   0.8271],\n",
      "        [  0.8356,   2.0592,   1.7572,   0.8692,   1.4228,   1.0479,   1.7997,\n",
      "           1.3589,   1.8165,   0.8216],\n",
      "        [  0.9218,   1.0196,   0.8711,   0.8600,   0.9039,   1.2738,   1.0664,\n",
      "           1.5956,  18.7564,   0.8252],\n",
      "        [  0.9889,   1.0307,   1.2268,   0.8293,   1.5155,   1.1297,   0.9276,\n",
      "           0.9809,   0.8591,   0.8522],\n",
      "        [  0.8611,   1.2981,   8.6293,   1.0824,   3.2584,   1.5792,   1.5131,\n",
      "           1.2336,   0.8319,   0.8242],\n",
      "        [  1.1789,   1.0252,   1.8707,   0.8366,   2.1486,   1.1653,   0.9349,\n",
      "           0.9647,   0.8234,   1.0007],\n",
      "        [  9.7905, 169.9825,  72.8825,   2.5429,   1.4100,   2.8420,   7.4225,\n",
      "           1.2640,   1.3806,   0.8107],\n",
      "        [  0.9849,   1.3562,   0.8306,   0.9597,   1.2108,   0.8387,   0.9947,\n",
      "           1.2070, 155.6679,   0.8556],\n",
      "        [  1.4889,   2.6564,   0.8849,   0.8170,  55.5909,   0.8958,   0.9278,\n",
      "           1.2881,   2.3234,   0.9592],\n",
      "        [  0.8732,   1.2705,   0.8438,   0.8628,   7.1149,   0.8570,   1.0506,\n",
      "           0.8897,   1.3966,   0.8912],\n",
      "        [  2.0407,   9.6301,  26.2559,   1.0463,   1.1478,   1.2106,   1.7587,\n",
      "           1.1117,   0.8837,   0.8136],\n",
      "        [  1.4420,   2.4313,   0.8686,   1.0016,   2.0399,   0.8164,   1.1735,\n",
      "           0.9199,   3.0016,   1.0009],\n",
      "        [  6.4494,  27.7090,   8.5416,   1.5009,   1.4200,   4.0728,  10.1211,\n",
      "           0.9153,   0.9605,   0.8123],\n",
      "        [  1.2872,   2.3355,   0.8475,   1.0391,   1.0482,   0.8216,   0.9777,\n",
      "           0.8963,   1.9876,   1.0011],\n",
      "        [  1.3477,   1.4311,   0.8529,   0.8306,   6.5518,   0.8911,   0.9232,\n",
      "           0.8622,   6.0274,   0.9053],\n",
      "        [  1.2092,   2.4523,   0.9069,   1.2983,   0.9036,   0.9090,   0.9213,\n",
      "           0.8242,   1.3930,   0.8641],\n",
      "        [  1.4831,   2.7532,   0.8942,   0.9693,   1.5796,   0.8152,   0.9925,\n",
      "           1.0212,   2.0493,   1.0648]])\n",
      "alpha[1200:1250]:  tensor([], size=(0, 10))\n",
      "alpha[2450:2500]:  tensor([], size=(0, 10))\n",
      "alpha after correction:  tensor([[  1.9446,  24.9350, 100.0111,   1.2202,   0.9447,   3.1675,  11.4864,\n",
      "           1.0678,   0.8915,   0.8134],\n",
      "        [  1.2251,   3.2010,   1.4344,   0.9859,   0.8351,   0.8305,   1.5175,\n",
      "           0.8598,   2.3831,   0.8979],\n",
      "        [  2.7198,   2.0099,   0.9052,   0.8139,  76.1733,   0.9474,   1.0711,\n",
      "           1.2822,   2.9258,   1.0450],\n",
      "        [  0.8825,   1.8657,   1.2406,   0.8805,   2.6093,   0.8397,   0.8731,\n",
      "           0.9221,   6.2879,   0.8408],\n",
      "        [  2.6506,   2.9678,   0.9613,   0.9271,   1.3665,   0.8160,   1.0780,\n",
      "           0.9073,   1.5246,   0.9619],\n",
      "        [  1.8325,   1.6093,   0.8579,   0.8190,  11.7611,   0.8785,   0.9618,\n",
      "           1.1392,   2.4216,   0.9889],\n",
      "        [  1.1011,   2.1901,   0.8459,   0.8174,   4.4436,   1.0361,   1.2965,\n",
      "           1.1077,   1.0239,   1.2106],\n",
      "        [  1.2053,   4.0182,   0.9501,   1.0195,   1.4974,   0.8161,   1.0129,\n",
      "           0.8783,   5.2141,   1.0123],\n",
      "        [  0.8405,   0.9417,   1.9737,   0.9420,   1.2594,   1.0780,   0.9771,\n",
      "           1.3729,   0.8775,   0.8292],\n",
      "        [  0.9073,   0.9263,   1.7458,   0.8495,   2.6234,   1.0623,   0.9044,\n",
      "           0.9048,   0.8300,   1.0140]])\n",
      "Dir variance:  tensor([], size=(0, 10))\n",
      "[0.01327564 0.17022552 0.6827526  0.00833024 0.00644916 0.02162368\n",
      " 0.07841479 0.00728973 0.00608575 0.00555293]\n"
     ]
    }
   ],
   "source": [
    "print(la_diag(FMNIST_test0[0], link_approx='bridge').numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = MNIST_test.targets.numpy()\n",
    "targets_FMNIST = FMNIST_test.targets.numpy()\n",
    "#targets_notMNIST = notMNIST_test.targets.numpy().astype(int)\n",
    "targets_KMNIST = KMNIST_test.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_MAP = predict_MAP(mnist_model, MNIST_test_loader, cuda=False).cpu().numpy()\n",
    "mnist_test_out_fmnist_MAP = predict_MAP(mnist_model, FMNIST_test_loader, cuda=False).cpu().numpy()\n",
    "#mnist_test_out_notMNIST_MAP = predict_MAP(mnist_model, not_mnist_test_loader, cuda=True).cpu().numpy()\n",
    "mnist_test_out_KMNIST_MAP = predict_MAP(mnist_model, KMNIST_test_loader, cuda=False).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP = get_in_dist_values(mnist_test_in_MAP, targets)\n",
    "acc_out_FMNIST_MAP, prob_correct_out_FMNIST_MAP, ent_out_FMNIST_MAP, MMC_out_FMNIST_MAP, auroc_out_FMNIST_MAP = get_out_dist_values(mnist_test_in_MAP, mnist_test_out_fmnist_MAP, targets_FMNIST)\n",
    "#acc_out_notMNIST_MAP, prob_correct_out_notMNIST_MAP, ent_out_notMNIST_MAP, MMC_out_notMNIST_MAP, auroc_out_notMNIST_MAP = get_out_dist_values(mnist_test_in_MAP, mnist_test_out_notMNIST_MAP, targets_notMNIST)\n",
    "acc_out_KMNIST_MAP, prob_correct_out_KMNIST_MAP, ent_out_KMNIST_MAP, MMC_out_KMNIST_MAP, auroc_out_KMNIST_MAP = get_out_dist_values(mnist_test_in_MAP, mnist_test_out_KMNIST_MAP, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, MAP, mnist] Accuracy: 0.986; average entropy: 0.053;     MMC: 0.984; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, FMNIST] Accuracy: 0.063; Average entropy: 1.118;    MMC: 0.604; AUROC: 0.979; Prob @ correct: 0.100\n",
      "[Out-MAP, KFAC, KMNIST] Accuracy: 0.092; Average entropy: 0.753;    MMC: 0.722; AUROC: 0.956; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_MAP, prob_correct_in_MAP, ent_in_MAP, MMC_in_MAP, 'mnist', 'MAP')\n",
    "print_out_dist_values(acc_out_FMNIST_MAP, prob_correct_out_FMNIST_MAP, ent_out_FMNIST_MAP, MMC_out_FMNIST_MAP, auroc_out_FMNIST_MAP, 'FMNIST', 'MAP')\n",
    "#print_out_dist_values(acc_out_notMNIST_MAP, prob_correct_out_notMNIST_MAP, ent_out_notMNIST_MAP, MMC_out_notMNIST_MAP, auroc_out_notMNIST_MAP, 'notMNIST', 'MAP')\n",
    "print_out_dist_values(acc_out_KMNIST_MAP, prob_correct_out_KMNIST_MAP, ent_out_KMNIST_MAP, MMC_out_KMNIST_MAP, auroc_out_KMNIST_MAP, 'KMNIST', 'MAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAP estimate\n",
    "#seeds are 123,124,125,126,127\n",
    "acc_in = [0.989, 0.989, 0.991, 0.988, 0.989]\n",
    "mmc_in = [0.988, 0.986, 0.988, 0.988, 0.988]\n",
    "mmc_out_fmnist = [0.591, 0.604, 0.543, 0.600, 0.581]\n",
    "mmc_out_notmnist = [0.750, 0.777, 0.775, 0.782, 0.783]\n",
    "mmc_out_kmnist = [0.717, 0.732, 0.721, 0.725, 0.718]\n",
    "\n",
    "auroc_out_fmnist = [0.988, 0.984, 0.991, 0.986, 0.988]\n",
    "auroc_out_notmnist = [0.947, 0.914, 0.937, 0.931, 0.923]\n",
    "auroc_out_kmnist = [0.969, 0.962, 0.968, 0.968, 0.968]\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_fmnist), np.std(mmc_out_fmnist)))\n",
    "print(\"MMC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_notmnist), np.std(mmc_out_notmnist)))\n",
    "print(\"MMC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_kmnist), np.std(mmc_out_kmnist)))\n",
    "\n",
    "print(\"AUROC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_fmnist), np.std(auroc_out_fmnist)))\n",
    "print(\"AUROC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_notmnist), np.std(auroc_out_notmnist)))\n",
    "print(\"AUROC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_kmnist), np.std(auroc_out_kmnist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diag Hessian Sampling estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_D = predict_diagonal_sampling(mnist_model, mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True, n_samples=num_samples).cpu().numpy()\n",
    "mnist_test_out_FMNIST_D = predict_diagonal_sampling(mnist_model, FMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True, n_samples=num_samples).cpu().numpy()\n",
    "mnist_test_out_notMNIST_D = predict_diagonal_sampling(mnist_model, not_mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True, n_samples=num_samples).cpu().numpy()\n",
    "mnist_test_out_KMNIST_D = predict_diagonal_sampling(mnist_model, KMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True, n_samples=num_samples).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average log-likelihood for Diag\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_in_D)).log_prob(torch.tensor(targets)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_FMNIST_D)).log_prob(torch.tensor(targets_FMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_notMNIST_D)).log_prob(torch.tensor(targets_notMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_KMNIST_D)).log_prob(torch.tensor(targets_KMNIST)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the Expected confidence estimate\n",
    "import pycalib.scoring as scoring\n",
    "\n",
    "print(scoring.expected_calibration_error(targets, mnist_test_in_D))\n",
    "print(scoring.expected_calibration_error(targets_FMNIST, mnist_test_out_FMNIST_D))\n",
    "print(scoring.expected_calibration_error(targets_notMNIST, mnist_test_out_notMNIST_D))\n",
    "print(scoring.expected_calibration_error(targets_KMNIST, mnist_test_out_KMNIST_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D = get_in_dist_values(mnist_test_in_D, targets)\n",
    "acc_out_FMNIST_D, prob_correct_out_FMNIST_D, ent_out_FMNIST_D, MMC_out_FMNIST_D, auroc_out_FMNIST_D = get_out_dist_values(mnist_test_in_D, mnist_test_out_FMNIST_D, targets_FMNIST)\n",
    "acc_out_notMNIST_D, prob_correct_out_notMNIST_D, ent_out_notMNIST_D, MMC_out_notMNIST_D, auroc_out_notMNIST_D = get_out_dist_values(mnist_test_in_D, mnist_test_out_notMNIST_D, targets_notMNIST)\n",
    "acc_out_KMNIST_D, prob_correct_out_KMNIST_D, ent_out_KMNIST_D, MMC_out_KMNIST_D, auroc_out_KMNIST_D = get_out_dist_values(mnist_test_in_D, mnist_test_out_KMNIST_D, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in_dist_values(acc_in_D, prob_correct_in_D, ent_in_D, MMC_in_D, 'MNIST', 'Diag')\n",
    "print_out_dist_values(acc_out_FMNIST_D, prob_correct_out_FMNIST_D, ent_out_FMNIST_D, MMC_out_FMNIST_D, auroc_out_FMNIST_D, 'MNIST', test='fmnist', method='Diag')\n",
    "print_out_dist_values(acc_out_notMNIST_D, prob_correct_out_notMNIST_D, ent_out_notMNIST_D, MMC_out_notMNIST_D, auroc_out_notMNIST_D, 'MNIST', test='notMNIST', method='Diag')\n",
    "print_out_dist_values(acc_out_KMNIST_D, prob_correct_out_KMNIST_D, ent_out_KMNIST_D, MMC_out_KMNIST_D, auroc_out_KMNIST_D, 'MNIST', test='KMNIST', method='Diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diag Sampling (1000)\n",
    "#seeds are 123,124,125,126,127\n",
    "time_diag_in = [7.236505508422852, 7.125160217285156, 7.238185882568359, 7.385348320007324, 7.000167608261108]\n",
    "time_diag_out_fmnist = [7.219660520553589, 8.032358407974243, 7.777910232543945, 7.446442604064941, 7.200972318649292]\n",
    "time_diag_out_notmnist = [20.68891930580139, 19.327053546905518, 18.424328804016113, 17.584392786026, 17.085043907165527]\n",
    "time_diag_out_kmnist = [9.627353191375732, 9.133541345596313, 9.307446718215942, 9.410037755966187, 9.100234746932983]\n",
    "\n",
    "acc_in = [0.989, 0.989, 0.990, 0.988, 0.989]\n",
    "mmc_in = [0.980, 0.976, 0.979, 0.980, 0.979]\n",
    "mmc_out_fmnist = [0.558, 0.570, 0.511, 0.566, 0.550]\n",
    "mmc_out_notmnist = [0.695, 0.730, 0.721, 0.735, 0.731]\n",
    "mmc_out_kmnist = [0.653, 0.666, 0.654, 0.664, 0.654]\n",
    "\n",
    "auroc_out_fmnist = [0.985, 0.980, 0.988, 0.983, 0.985]\n",
    "auroc_out_notmnist = [0.944, 0.907, 0.935, 0.927, 0.921]\n",
    "auroc_out_kmnist = [0.968, 0.960, 0.967, 0.966, 0.966]\n",
    "\n",
    "print(\"Diagonal Sampling time in: {:.03f} with std {:.03f}\".format(np.mean(time_diag_in), np.std(time_diag_in)))\n",
    "print(\"Diagonal Sampling time out fmnist: {:.03f} with std {:.03f}\".format(np.mean(time_diag_out_fmnist), np.std(time_diag_out_fmnist)))\n",
    "print(\"Diagonal Sampling time out notmnist: {:.03f} with std {:.03f}\".format(np.mean(time_diag_out_notmnist), np.std(time_diag_out_notmnist)))\n",
    "print(\"Diagonal Sampling time out kmnist: {:.03f} with std {:.03f}\".format(np.mean(time_diag_out_kmnist), np.std(time_diag_out_kmnist)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_fmnist), np.std(mmc_out_fmnist)))\n",
    "print(\"MMC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_notmnist), np.std(mmc_out_notmnist)))\n",
    "print(\"MMC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_kmnist), np.std(mmc_out_kmnist)))\n",
    "\n",
    "print(\"AUROC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_fmnist), np.std(auroc_out_fmnist)))\n",
    "print(\"AUROC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_notmnist), np.std(auroc_out_notmnist)))\n",
    "print(\"AUROC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_kmnist), np.std(auroc_out_kmnist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFAC Laplace Approximation (sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_KFAC = predict_KFAC_sampling(mnist_model, mnist_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True, n_samples=num_samples).cpu().numpy()\n",
    "mnist_test_out_FMNIST_KFAC = predict_KFAC_sampling(mnist_model, FMNIST_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True, n_samples=num_samples).cpu().numpy()\n",
    "mnist_test_out_notMNIST_KFAC = predict_KFAC_sampling(mnist_model, not_mnist_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True, n_samples=num_samples).cpu().numpy()\n",
    "mnist_test_out_KMNIST_KFAC = predict_KFAC_sampling(mnist_model, KMNIST_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True, n_samples=num_samples).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average log-likelihood for KFAC\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_in_KFAC)).log_prob(torch.tensor(targets)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_FMNIST_KFAC)).log_prob(torch.tensor(targets_FMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_notMNIST_KFAC)).log_prob(torch.tensor(targets_notMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_KMNIST_KFAC)).log_prob(torch.tensor(targets_KMNIST)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ECE for KFAC\n",
    "print(scoring.expected_calibration_error(targets, mnist_test_in_KFAC))\n",
    "print(scoring.expected_calibration_error(targets_FMNIST, mnist_test_out_FMNIST_KFAC))\n",
    "print(scoring.expected_calibration_error(targets_notMNIST, mnist_test_out_notMNIST_KFAC))\n",
    "print(scoring.expected_calibration_error(targets_KMNIST, mnist_test_out_KMNIST_KFAC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_KFAC, prob_correct_in_KFAC, ent_in_KFAC, MMC_in_KFAC = get_in_dist_values(mnist_test_in_KFAC, targets)\n",
    "acc_out_FMNIST_KFAC, prob_correct_out_FMNIST_KFAC, ent_out_FMNIST_KFAC, MMC_out_FMNIST_KFAC, auroc_out_FMNIST_KFAC = get_out_dist_values(mnist_test_in_KFAC, mnist_test_out_FMNIST_KFAC, targets_FMNIST)\n",
    "acc_out_notMNIST_KFAC, prob_correct_out_notMNIST_KFAC, ent_out_notMNIST_KFAC, MMC_out_notMNIST_KFAC, auroc_out_notMNIST_KFAC = get_out_dist_values(mnist_test_in_KFAC, mnist_test_out_notMNIST_KFAC, targets_notMNIST)\n",
    "acc_out_KMNIST_KFAC, prob_correct_out_KMNIST_KFAC, ent_out_KMNIST_KFAC, MMC_out_KMNIST_KFAC, auroc_out_KMNIST_KFAC = get_out_dist_values(mnist_test_in_KFAC, mnist_test_out_KMNIST_KFAC, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in_dist_values(acc_in_KFAC, prob_correct_in_KFAC, ent_in_KFAC, MMC_in_KFAC, 'MNIST', 'KFAC')\n",
    "print_out_dist_values(acc_out_FMNIST_KFAC, prob_correct_out_FMNIST_KFAC, ent_out_FMNIST_KFAC, MMC_out_FMNIST_KFAC, auroc_out_FMNIST_KFAC, 'MNIST', test='fmnist', method='KFAC')\n",
    "print_out_dist_values(acc_out_notMNIST_KFAC, prob_correct_out_notMNIST_KFAC, ent_out_notMNIST_KFAC, MMC_out_notMNIST_KFAC, auroc_out_notMNIST_KFAC, 'MNIST', test='notMNIST', method='KFAC')\n",
    "print_out_dist_values(acc_out_KMNIST_KFAC, prob_correct_out_KMNIST_KFAC, ent_out_KMNIST_KFAC, MMC_out_KMNIST_KFAC, auroc_out_KMNIST_KFAC, 'MNIST', test='KMNIST', method='KFAC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace Bridge (Diagonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_LB = predict_LB(mnist_model, mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_FMNIST_LB = predict_LB(mnist_model, FMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_notMNIST_LB = predict_LB(mnist_model, not_mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_KMNIST_LB = predict_LB(mnist_model, KMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_LBn = mnist_test_in_LB/mnist_test_in_LB.sum(1).reshape(-1,1)\n",
    "mnist_test_out_FMNIST_LBn = mnist_test_out_FMNIST_LB/mnist_test_out_FMNIST_LB.sum(1).reshape(-1,1)\n",
    "mnist_test_out_notMNIST_LBn = mnist_test_out_notMNIST_LB/mnist_test_out_notMNIST_LB.sum(1).reshape(-1,1)\n",
    "mnist_test_out_KMNIST_LBn = mnist_test_out_KMNIST_LB/mnist_test_out_KMNIST_LB.sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute average log-likelihood for LB\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_in_LBn)).log_prob(torch.tensor(targets)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_FMNIST_LBn)).log_prob(torch.tensor(targets_FMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_notMNIST_LBn)).log_prob(torch.tensor(targets_notMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_KMNIST_LBn)).log_prob(torch.tensor(targets_KMNIST)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute ECE for LB\n",
    "print(scoring.expected_calibration_error(targets, mnist_test_in_LBn))\n",
    "print(scoring.expected_calibration_error(targets_FMNIST, mnist_test_out_FMNIST_LBn))\n",
    "print(scoring.expected_calibration_error(targets_notMNIST, mnist_test_out_notMNIST_LBn))\n",
    "print(scoring.expected_calibration_error(targets_KMNIST, mnist_test_out_KMNIST_LBn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_LBn, prob_correct_in_LBn, ent_in_LBn, MMC_in_LBn = get_in_dist_values(mnist_test_in_LBn, targets)\n",
    "acc_out_FMNIST_LBn, prob_correct_out_FMNIST_LBn, ent_out_FMNIST_LBn, MMC_out_FMNIST_LBn, auroc_out_FMNIST_LBn = get_out_dist_values(mnist_test_in_LBn, mnist_test_out_FMNIST_LBn, targets_FMNIST)\n",
    "acc_out_notMNIST_LBn, prob_correct_out_notMNIST_LBn, ent_out_notMNIST_LBn, MMC_out_notMNIST_LBn, auroc_out_notMNIST_LBn = get_out_dist_values(mnist_test_in_LBn, mnist_test_out_notMNIST_LBn, targets_notMNIST)\n",
    "acc_out_KMNIST_LBn, prob_correct_out_KMNIST_LBn, ent_out_KMNIST_LBn, MMC_out_KMNIST_LBn, auroc_out_KMNIST_LBn = get_out_dist_values(mnist_test_in_LBn, mnist_test_out_KMNIST_LBn, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in_dist_values(acc_in_LBn, prob_correct_in_LBn, ent_in_LBn, MMC_in_LBn, 'MNIST', 'LBn')\n",
    "print_out_dist_values(acc_out_FMNIST_LBn, prob_correct_out_FMNIST_LBn, ent_out_FMNIST_LBn, MMC_out_FMNIST_LBn, auroc_out_FMNIST_LBn, 'MNIST', test='fmnist', method='LBn')\n",
    "print_out_dist_values(acc_out_notMNIST_LBn, prob_correct_out_notMNIST_LBn, ent_out_notMNIST_LBn, MMC_out_notMNIST_LBn, auroc_out_notMNIST_LBn, 'MNIST', test='notMNIST', method='LBn')\n",
    "print_out_dist_values(acc_out_KMNIST_LBn, prob_correct_out_KMNIST_LBn, ent_out_KMNIST_LBn, MMC_out_KMNIST_LBn, auroc_out_KMNIST_LBn, 'MNIST', test='KMNIST', method='LBn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laplace Bridge\n",
    "#seeds are 123,124,125,126,127\n",
    "time_lpb_in = [0.01302, 0.01731, 0.01489, 0.01529, 0.01574]\n",
    "time_lpb_out_fmnist = [0.01256, 0.01531, 0.01669, 0.01703, 0.01500]\n",
    "time_lpb_out_notmnist = [0.02332, 0.03115, 0.02666, 0.02756, 0.02864]\n",
    "time_lpb_out_kmnist = [0.01287, 0.01520, 0.01636, 0.01687, 0.01438]\n",
    "\n",
    "acc_in = [0.989, 0.989, 0.991, 0.988, 0.989]\n",
    "mmc_in = [0.988, 0.986, 0.988, 0.988, 0.988]\n",
    "mmc_out_fmnist = [0.493, 0.523, 0.433, 0.514, 0.488]\n",
    "mmc_out_notmnist = [0.735, 0.759, 0.756, 0.767, 0.769]\n",
    "mmc_out_kmnist = [0.699, 0.722, 0.707, 0.713, 0.703]\n",
    "\n",
    "auroc_out_fmnist = [0.991, 0.988, 0.993, 0.989, 0.990]\n",
    "auroc_out_notmnist = [0.948, 0.915, 0.938, 0.932, 0.924]\n",
    "auroc_out_kmnist = [0.970, 0.963, 0.969, 0.968, 0.968]\n",
    "\n",
    "print(\"Laplace Bridge time in: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_in), np.std(time_lpb_in)))\n",
    "print(\"Laplace Bridge time out fmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_fmnist), np.std(time_lpb_out_fmnist)))\n",
    "print(\"Laplace Bridge time out notmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_notmnist), np.std(time_lpb_out_notmnist)))\n",
    "print(\"Laplace Bridge time out kmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_kmnist), np.std(time_lpb_out_kmnist)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_fmnist), np.std(mmc_out_fmnist)))\n",
    "print(\"MMC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_notmnist), np.std(mmc_out_notmnist)))\n",
    "print(\"MMC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_kmnist), np.std(mmc_out_kmnist)))\n",
    "\n",
    "print(\"AUROC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_fmnist), np.std(auroc_out_fmnist)))\n",
    "print(\"AUROC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_notmnist), np.std(auroc_out_notmnist)))\n",
    "print(\"AUROC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_kmnist), np.std(auroc_out_kmnist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFAC Laplace Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_LB_KFAC = predict_LB_KFAC(mnist_model, mnist_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_FMNIST_LB_KFAC = predict_LB_KFAC(mnist_model, FMNIST_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_notMNIST_LB_KFAC = predict_LB_KFAC(mnist_model, not_mnist_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_KMNIST_LB_KFAC = predict_LB_KFAC(mnist_model, KMNIST_test_loader, M_W_post_K, M_b_post_K, U_post_K, V_post_K, B_post_K, verbose=False, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_LB_KFACn = mnist_test_in_LB_KFAC/mnist_test_in_LB_KFAC.sum(1).reshape(-1,1)\n",
    "mnist_test_out_FMNIST_LB_KFACn = mnist_test_out_FMNIST_LB_KFAC/mnist_test_out_FMNIST_LB_KFAC.sum(1).reshape(-1,1)\n",
    "mnist_test_out_notMNIST_LB_KFACn = mnist_test_out_notMNIST_LB_KFAC/mnist_test_out_notMNIST_LB_KFAC.sum(1).reshape(-1,1)\n",
    "mnist_test_out_KMNIST_LB_KFACn = mnist_test_out_KMNIST_LB_KFAC/mnist_test_out_KMNIST_LB_KFAC.sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average log-likelihood for LB KFAC\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_in_LB_KFACn)).log_prob(torch.tensor(targets)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_FMNIST_LB_KFACn)).log_prob(torch.tensor(targets_FMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_notMNIST_LB_KFACn)).log_prob(torch.tensor(targets_notMNIST)).mean())\n",
    "print(torch.distributions.Categorical(torch.tensor(mnist_test_out_KMNIST_LB_KFACn)).log_prob(torch.tensor(targets_KMNIST)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scoring.expected_calibration_error(targets, mnist_test_in_LB_KFACn))\n",
    "print(scoring.expected_calibration_error(targets_FMNIST, mnist_test_out_FMNIST_LB_KFACn))\n",
    "print(scoring.expected_calibration_error(targets_notMNIST, mnist_test_out_notMNIST_LB_KFACn))\n",
    "print(scoring.expected_calibration_error(targets_KMNIST, mnist_test_out_KMNIST_LB_KFACn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_LB_KFACn, prob_correct_in_LB_KFACn, ent_in_LB_KFACn, MMC_in_LB_KFACn = get_in_dist_values(mnist_test_in_LB_KFACn, targets)\n",
    "acc_out_FMNIST_LB_KFACn, prob_correct_out_FMNIST_LB_KFACn, ent_out_FMNIST_LB_KFACn, MMC_out_FMNIST_LB_KFACn, auroc_out_FMNIST_LB_KFACn = get_out_dist_values(mnist_test_in_LB_KFACn, mnist_test_out_FMNIST_LB_KFACn, targets_FMNIST)\n",
    "acc_out_notMNIST_LB_KFACn, prob_correct_out_notMNIST_LB_KFACn, ent_out_notMNIST_LB_KFACn, MMC_out_notMNIST_LB_KFACn, auroc_out_notMNIST_LB_KFACn = get_out_dist_values(mnist_test_in_LB_KFACn, mnist_test_out_notMNIST_LB_KFACn, targets_notMNIST)\n",
    "acc_out_KMNIST_LB_KFACn, prob_correct_out_KMNIST_LB_KFACn, ent_out_KMNIST_LB_KFACn, MMC_out_KMNIST_LB_KFACn, auroc_out_KMNIST_LB_KFACn = get_out_dist_values(mnist_test_in_LB_KFACn, mnist_test_out_KMNIST_LB_KFACn, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in_dist_values(acc_in_LB_KFACn, prob_correct_in_LB_KFACn, ent_in_LB_KFACn, MMC_in_LB_KFACn, 'MNIST', 'LB_KFACn')\n",
    "print_out_dist_values(acc_out_FMNIST_LB_KFACn, prob_correct_out_FMNIST_LB_KFACn, ent_out_FMNIST_LB_KFACn, MMC_out_FMNIST_LB_KFACn, auroc_out_FMNIST_LB_KFACn, 'MNIST', test='fmnist', method='LB_KFACn')\n",
    "print_out_dist_values(acc_out_notMNIST_LB_KFACn, prob_correct_out_notMNIST_LB_KFACn, ent_out_notMNIST_LB_KFACn, MMC_out_notMNIST_LB_KFACn, auroc_out_notMNIST_LB_KFACn, 'MNIST', test='notMNIST', method='LB_KFACn')\n",
    "print_out_dist_values(acc_out_KMNIST_LB_KFACn, prob_correct_out_KMNIST_LB_KFACn, ent_out_KMNIST_LB_KFACn, MMC_out_KMNIST_LB_KFACn, auroc_out_KMNIST_LB_KFACn, 'MNIST', test='KMNIST', method='LB_KFACn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laplace Bridge KFAC\n",
    "#seeds are 123,124,125,126,127\n",
    "time_lpb_in = [0.01302, 0.01731, 0.01489, 0.01529, 0.01574]\n",
    "time_lpb_out_fmnist = [0.01256, 0.01531, 0.01669, 0.01703, 0.01500]\n",
    "time_lpb_out_notmnist = [0.02332, 0.03115, 0.02666, 0.02756, 0.02864]\n",
    "time_lpb_out_kmnist = [0.01287, 0.01520, 0.01636, 0.01687, 0.01438]\n",
    "\n",
    "acc_in = [0.989, 0.989, 0.991, 0.988, 0.989]\n",
    "mmc_in = [0.988, 0.986, 0.988, 0.988, 0.988]\n",
    "mmc_out_fmnist = [0.493, 0.523, 0.433, 0.514, 0.488]\n",
    "mmc_out_notmnist = [0.735, 0.759, 0.756, 0.767, 0.769]\n",
    "mmc_out_kmnist = [0.699, 0.722, 0.707, 0.713, 0.703]\n",
    "\n",
    "auroc_out_fmnist = [0.991, 0.988, 0.993, 0.989, 0.990]\n",
    "auroc_out_notmnist = [0.948, 0.915, 0.938, 0.932, 0.924]\n",
    "auroc_out_kmnist = [0.970, 0.963, 0.969, 0.968, 0.968]\n",
    "\n",
    "print(\"Laplace Bridge time in: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_in), np.std(time_lpb_in)))\n",
    "print(\"Laplace Bridge time out fmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_fmnist), np.std(time_lpb_out_fmnist)))\n",
    "print(\"Laplace Bridge time out notmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_notmnist), np.std(time_lpb_out_notmnist)))\n",
    "print(\"Laplace Bridge time out kmnist: {:.03f} with std {:.03f}\".format(np.mean(time_lpb_out_kmnist), np.std(time_lpb_out_kmnist)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_fmnist), np.std(mmc_out_fmnist)))\n",
    "print(\"MMC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_notmnist), np.std(mmc_out_notmnist)))\n",
    "print(\"MMC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_kmnist), np.std(mmc_out_kmnist)))\n",
    "\n",
    "print(\"AUROC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_fmnist), np.std(auroc_out_fmnist)))\n",
    "print(\"AUROC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_notmnist), np.std(auroc_out_notmnist)))\n",
    "print(\"AUROC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_kmnist), np.std(auroc_out_kmnist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditions\n",
    "\n",
    "Test the condition derived in Proposition 1 of the paper and evaluated experimentally in Appendix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if condition holds\n",
    "\n",
    "def check_condition(alpha_vecs):\n",
    "    #note that this is vectorized\n",
    "    alpha_sum = alpha_vecs.sum(1)\n",
    "    alpha_max = alpha_vecs.max(1)\n",
    "    alpha_sum_minus = alpha_sum - alpha_max\n",
    "    right_side = 0.25 * (np.sqrt(9 * alpha_sum_minus**2 + 10 * alpha_sum_minus + 1) - alpha_sum_minus - 1)\n",
    "    cases = alpha_max > right_side\n",
    "    percentage = np.sum(cases)/len(cases)\n",
    "    return(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(check_condition(mnist_test_in_LB)))\n",
    "print(np.sum(check_condition(mnist_test_out_FMNIST_LB)))\n",
    "print(np.sum(check_condition(mnist_test_out_notMNIST_LB)))\n",
    "print(np.sum(check_condition(mnist_test_out_KMNIST_LB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to extended MacKay approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_EMK = predict_extended_MacKay(mnist_model, mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_FMNIST_EMK = predict_extended_MacKay(mnist_model, FMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_notMNIST_EMK = predict_extended_MacKay(mnist_model, not_mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_KMNIST_EMK = predict_extended_MacKay(mnist_model, KMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_EMK, prob_correct_in_EMK, ent_in_EMK, MMC_in_EMK = get_in_dist_values(mnist_test_in_EMK, targets)\n",
    "acc_out_FMNIST_EMK, prob_correct_out_FMNIST_EMK, ent_out_FMNIST_EMK, MMC_out_FMNIST_EMK, auroc_out_FMNIST_EMK = get_out_dist_values(mnist_test_in_EMK, mnist_test_out_FMNIST_EMK, targets_FMNIST)\n",
    "acc_out_notMNIST_EMK, prob_correct_out_notMNIST_EMK, ent_out_notMNIST_EMK, MMC_out_notMNIST_EMK, auroc_out_notMNIST_EMK = get_out_dist_values(mnist_test_in_EMK, mnist_test_out_notMNIST_EMK, targets_notMNIST)\n",
    "acc_out_KMNIST_EMK, prob_correct_out_KMNIST_EMK, ent_out_KMNIST_EMK, MMC_out_KMNIST_EMK, auroc_out_KMNIST_EMK = get_out_dist_values(mnist_test_in_EMK, mnist_test_out_KMNIST_EMK, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in_dist_values(acc_in_EMK, prob_correct_in_EMK, ent_in_EMK, MMC_in_EMK, 'MNIST', 'EMK')\n",
    "print_out_dist_values(acc_out_FMNIST_EMK, prob_correct_out_FMNIST_EMK, ent_out_FMNIST_EMK, MMC_out_FMNIST_EMK, auroc_out_FMNIST_EMK, 'MNIST', test='fmnist', method='EMK')\n",
    "print_out_dist_values(acc_out_notMNIST_EMK, prob_correct_out_notMNIST_EMK, ent_out_notMNIST_EMK, MMC_out_notMNIST_EMK, auroc_out_notMNIST_EMK, 'MNIST', test='notMNIST', method='EMK')\n",
    "print_out_dist_values(acc_out_KMNIST_EMK, prob_correct_out_KMNIST_EMK, ent_out_KMNIST_EMK, MMC_out_KMNIST_EMK, auroc_out_KMNIST_EMK, 'MNIST', test='KMNIST', method='EMK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extended MacKay\n",
    "#seeds are 123,124,125,126,127\n",
    "time_EMK_in = [0.059017181396484375, 0.055175065994262695, 0.04456186294555664, 0.052222251892089844, 0.05633044242858887]\n",
    "time_EMK_out_fmnist = [0.14669466018676758, 0.06929922103881836, 0.0457310676574707, 0.04770398139953613, 0.04364585876464844]\n",
    "time_EMK_out_notmnist = [0.06481504440307617, 0.08360505104064941, 0.07899594306945801, 0.0765230655670166, 0.09008359909057617]\n",
    "time_EMK_out_kmnist = [0.03671598434448242, 0.03630828857421875, 0.04480934143066406, 0.0473024845123291, 0.04279160499572754]\n",
    "\n",
    "acc_in = [0.989, 0.989, 0.991, 0.988, 0.989]\n",
    "mmc_in = [0.981, 0.978, 0.981, 0.982, 0.981]\n",
    "mmc_out_fmnist = [0.564, 0.577, 0.516, 0.573, 0.556]\n",
    "mmc_out_notmnist = [0.708, 0.741, 0.744, 0.747, 0.744]\n",
    "mmc_out_kmnist = [0.667, 0.681, 0.669, 0.678, 0.668]\n",
    "\n",
    "auroc_out_fmnist = [0.986, 0.982, 0.989, 0.984, 0.986]\n",
    "auroc_out_notmnist = [0.946, 0.909, 0.937, 0.928, 0.922]\n",
    "auroc_out_kmnist = [0.969, 0.962, 0.968, 0.967, 0.967]\n",
    "\n",
    "print(\"Extended MacKay time in: {:.03f} with std {:.03f}\".format(np.mean(time_EMK_in), np.std(time_EMK_in)))\n",
    "print(\"Extended MacKay time out fmnist: {:.03f} with std {:.03f}\".format(np.mean(time_EMK_out_fmnist), np.std(time_EMK_out_fmnist)))\n",
    "print(\"Extended MacKay time out notmnist: {:.03f} with std {:.03f}\".format(np.mean(time_EMK_out_notmnist), np.std(time_EMK_out_notmnist)))\n",
    "print(\"Extended MacKay time out kmnist: {:.03f} with std {:.03f}\".format(np.mean(time_EMK_out_kmnist), np.std(time_EMK_out_kmnist)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_fmnist), np.std(mmc_out_fmnist)))\n",
    "print(\"MMC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_notmnist), np.std(mmc_out_notmnist)))\n",
    "print(\"MMC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_kmnist), np.std(mmc_out_kmnist)))\n",
    "\n",
    "print(\"AUROC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_fmnist), np.std(auroc_out_fmnist)))\n",
    "print(\"AUROC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_notmnist), np.std(auroc_out_notmnist)))\n",
    "print(\"AUROC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_kmnist), np.std(auroc_out_kmnist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to Second-order Delta Posterior Predictive\n",
    "\n",
    "as detailed in Appendix D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_in_SODPP = predict_SODPP(mnist_model, mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_FMNIST_SODPP = predict_SODPP(mnist_model, FMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_notMNIST_SODPP = predict_SODPP(mnist_model, not_mnist_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()\n",
    "mnist_test_out_KMNIST_SODPP = predict_SODPP(mnist_model, KMNIST_test_loader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_SODPP, prob_correct_in_SODPP, ent_in_SODPP, MMC_in_SODPP = get_in_dist_values(mnist_test_in_SODPP, targets)\n",
    "acc_out_FMNIST_SODPP, prob_correct_out_FMNIST_SODPP, ent_out_FMNIST_SODPP, MMC_out_FMNIST_SODPP, auroc_out_FMNIST_SODPP = get_out_dist_values(mnist_test_in_SODPP, mnist_test_out_FMNIST_SODPP, targets_FMNIST)\n",
    "acc_out_notMNIST_SODPP, prob_correct_out_notMNIST_SODPP, ent_out_notMNIST_SODPP, MMC_out_notMNIST_SODPP, auroc_out_notMNIST_SODPP = get_out_dist_values(mnist_test_in_SODPP, mnist_test_out_notMNIST_SODPP, targets_notMNIST)\n",
    "acc_out_KMNIST_SODPP, prob_correct_out_KMNIST_SODPP, ent_out_KMNIST_SODPP, MMC_out_KMNIST_SODPP, auroc_out_KMNIST_SODPP = get_out_dist_values(mnist_test_in_SODPP, mnist_test_out_KMNIST_SODPP, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_in_dist_values(acc_in_SODPP, prob_correct_in_SODPP, ent_in_SODPP, MMC_in_SODPP, 'MNIST', 'SODPP')\n",
    "print_out_dist_values(acc_out_FMNIST_SODPP, prob_correct_out_FMNIST_SODPP, ent_out_FMNIST_SODPP, MMC_out_FMNIST_SODPP, auroc_out_FMNIST_SODPP, 'MNIST', test='fmnist', method='SODPP')\n",
    "print_out_dist_values(acc_out_notMNIST_SODPP, prob_correct_out_notMNIST_SODPP, ent_out_notMNIST_SODPP, MMC_out_notMNIST_SODPP, auroc_out_notMNIST_SODPP, 'MNIST', test='notMNIST', method='SODPP')\n",
    "print_out_dist_values(acc_out_KMNIST_SODPP, prob_correct_out_KMNIST_SODPP, ent_out_KMNIST_SODPP, MMC_out_KMNIST_SODPP, auroc_out_KMNIST_SODPP, 'MNIST', test='KMNIST', method='SODPP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SODPP\n",
    "#seeds are 123,124,125,126,127\n",
    "time_SODPP_in = [0.022600173950195312, 0.022752046585083008, 0.027388811111450195, 0.0277860164642334, 0.028126955032348633]\n",
    "time_SODPP_out_fmnist = [0.02283191680908203, 0.022715091705322266, 0.02962970733642578, 0.030440807342529297, 0.028356552124023438]\n",
    "time_SODPP_out_notmnist = [0.040780067443847656, 0.04070425033569336, 0.04935407638549805, 0.05025911331176758, 0.051819801330566406]\n",
    "time_SODPP_out_kmnist = [0.021996021270751953, 0.022888660430908203, 0.028764009475708008, 0.030774354934692383, 0.02728438377380371]\n",
    "\n",
    "acc_in = [0.989, 0.989, 0.990, 0.987, 0.990]\n",
    "mmc_in = [0.980, 0.977, 0.979, 0.980, 0.979]\n",
    "mmc_out_fmnist = [0.552, 0.565, 0.506, 0.561, 0.545]\n",
    "mmc_out_notmnist = [0.681, 0.719, 0.708, 0.724, 0.717]\n",
    "mmc_out_kmnist = [0.636, 0.648, 0.636, 0.648, 0.636]\n",
    "\n",
    "auroc_out_fmnist = [0.984, 0.979, 0.987, 0.982, 0.984]\n",
    "auroc_out_notmnist = [0.945, 0.910, 0.935, 0.928, 0.921]\n",
    "auroc_out_kmnist = [0.968, 0.960, 0.967, 0.966, 0.966]\n",
    "\n",
    "print(\"SODPP time in: {:.03f} with std {:.03f}\".format(np.mean(time_SODPP_in), np.std(time_SODPP_in)))\n",
    "print(\"SODPP time out fmnist: {:.03f} with std {:.03f}\".format(np.mean(time_SODPP_out_fmnist), np.std(time_SODPP_out_fmnist)))\n",
    "print(\"SODPP time out notmnist: {:.03f} with std {:.03f}\".format(np.mean(time_SODPP_out_notmnist), np.std(time_SODPP_out_notmnist)))\n",
    "print(\"SODPP time out kmnist: {:.03f} with std {:.03f}\".format(np.mean(time_SODPP_out_kmnist), np.std(time_SODPP_out_kmnist)))\n",
    "\n",
    "print(\"accuracy: {:.03f} with std {:.03f}\".format(np.mean(acc_in), np.std(acc_in)))\n",
    "\n",
    "print(\"MMC in: {:.03f} with std {:.03f}\".format(np.mean(mmc_in), np.std(mmc_in)))\n",
    "print(\"MMC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_fmnist), np.std(mmc_out_fmnist)))\n",
    "print(\"MMC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_notmnist), np.std(mmc_out_notmnist)))\n",
    "print(\"MMC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(mmc_out_kmnist), np.std(mmc_out_kmnist)))\n",
    "\n",
    "print(\"AUROC out fmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_fmnist), np.std(auroc_out_fmnist)))\n",
    "print(\"AUROC out notmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_notmnist), np.std(auroc_out_notmnist)))\n",
    "print(\"AUROC out kmnist: {:.03f} with std {:.03f}\".format(np.mean(auroc_out_kmnist), np.std(auroc_out_kmnist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Rotated MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "# rotate 15 degrees\n",
    "MNIST_transform_r15 = transforms.Compose([\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "MNIST_test_r15 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r15)\n",
    "\n",
    "mnist_test_loader_r15 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r15,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 30 degrees\n",
    "MNIST_transform_r30 = transforms.Compose([\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r30 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r30)\n",
    "\n",
    "mnist_test_loader_r30 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r30,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 45 degrees\n",
    "MNIST_transform_r45 = transforms.Compose([\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r45 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r45)\n",
    "\n",
    "mnist_test_loader_r45 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r45,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 60 degrees\n",
    "MNIST_transform_r60 = transforms.Compose([\n",
    "    transforms.RandomRotation(60),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r60 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r60)\n",
    "\n",
    "mnist_test_loader_r60 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r60,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 75 degrees\n",
    "MNIST_transform_r75 = transforms.Compose([\n",
    "    transforms.RandomRotation(75),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r75 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r75)\n",
    "\n",
    "mnist_test_loader_r75 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r75,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 90 degrees\n",
    "MNIST_transform_r90 = transforms.Compose([\n",
    "    transforms.RandomRotation(90),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r90 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r90)\n",
    "\n",
    "mnist_test_loader_r90 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r90,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 105 degrees\n",
    "MNIST_transform_r105 = transforms.Compose([\n",
    "    transforms.RandomRotation(105),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r105 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r105)\n",
    "\n",
    "mnist_test_loader_r105 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r105,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 120 degrees\n",
    "MNIST_transform_r120 = transforms.Compose([\n",
    "    transforms.RandomRotation(120),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r120 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r120)\n",
    "\n",
    "mnist_test_loader_r120 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r120,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 135 degrees\n",
    "MNIST_transform_r135 = transforms.Compose([\n",
    "    transforms.RandomRotation(135),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r135 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r135)\n",
    "\n",
    "mnist_test_loader_r135 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r135,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 150 degrees\n",
    "MNIST_transform_r150= transforms.Compose([\n",
    "    transforms.RandomRotation(150),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r150 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r150)\n",
    "\n",
    "mnist_test_loader_r150 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r150,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 165 degrees\n",
    "MNIST_transform_r165 = transforms.Compose([\n",
    "    transforms.RandomRotation(165),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r165 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r165)\n",
    "\n",
    "mnist_test_loader_r165 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r165,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# rotate 180 degrees\n",
    "MNIST_transform_r180 = transforms.Compose([\n",
    "    transforms.RandomRotation(180),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "MNIST_test_r180 = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform_r180)\n",
    "\n",
    "mnist_test_loader_r180 = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test_r180,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "## helper function: given a dataloader compute accuracy and brier score\n",
    "def get_acc_brier(dataloader, targets, num_samples=1000):\n",
    "    \n",
    "    # compute sampling results\n",
    "    mnist_rotated_D = predict_diagonal_sampling(mnist_model, dataloader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=False, n_samples=num_samples).cpu().numpy()\n",
    "    \n",
    "    # compute LB results\n",
    "    mnist_rotated_LB = predict_LB(mnist_model, dataloader, M_W_post_D, M_b_post_D, C_W_post_D, C_b_post_D, verbose=False, cuda=True, timing=False).cpu().numpy()\n",
    "    mnist_rotated_LBn = mnist_rotated_LB/mnist_rotated_LB.sum(1).reshape(-1,1)\n",
    "    \n",
    "    # accuracy for sampling and LB\n",
    "    acc_D = np.mean(np.argmax(mnist_rotated_D, 1) == targets)\n",
    "    acc_LB = np.mean(np.argmax(mnist_rotated_LBn, 1) == targets)\n",
    "    \n",
    "    # get brier score for sampling and LB\n",
    "    pred_at_true_D = np.array([mnist_rotated_D[i, j] for i, j in enumerate(targets)])\n",
    "    pred_at_true_LBn = np.array([mnist_rotated_LBn[i, j] for i, j in enumerate(targets)])\n",
    "    \n",
    "    brier_D = brier_score_loss(np.ones_like(pred_at_true_D), pred_at_true_D)\n",
    "    brier_LB = brier_score_loss(np.ones_like(pred_at_true_LBn), pred_at_true_LBn)\n",
    "    \n",
    "    return(acc_D, acc_LB, brier_D, brier_LB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a test\n",
    "get_acc_brier(mnist_test_loader_r15, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict on all distributions and compute accuracy and brier score\n",
    "\n",
    "dataloader_list = [mnist_test_loader, mnist_test_loader_r15, mnist_test_loader_r30, mnist_test_loader_r45,\n",
    "                  mnist_test_loader_r60, mnist_test_loader_r75, mnist_test_loader_r90, mnist_test_loader_r105,\n",
    "                  mnist_test_loader_r120, mnist_test_loader_r135, mnist_test_loader_r150, mnist_test_loader_r165, \n",
    "                  mnist_test_loader_r180]\n",
    "\n",
    "Acc_D_list = []\n",
    "Acc_LB_list = []\n",
    "Brier_D_list = []\n",
    "Brier_LB_list = []\n",
    "\n",
    "for i, loader_ in enumerate(dataloader_list):\n",
    "    print(i)\n",
    "    Acc_D_, Acc_LB_, Brier_D_, Brier_LB_ = get_acc_brier(loader_, targets)\n",
    "    Acc_D_list.append(Acc_D_)\n",
    "    Acc_LB_list.append(Acc_LB_)\n",
    "    Brier_D_list.append(Brier_D_)\n",
    "    Brier_LB_list.append(Brier_LB_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make inline plots vector graphics\n",
    "import matplotlib\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")\n",
    "\n",
    "matplotlib.rc(\"font\", **{\"family\": \"serif\", \"serif\": [\"Computer Modern\"]})\n",
    "plt.rcParams[\"text.usetex\"] = True\n",
    "plt.rcParams[\"text.latex.preamble\"] = r\"\\usepackage{amsfonts} \\usepackage{amsmath}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Acc_D_list)\n",
    "print(Acc_LB_list)\n",
    "print(Brier_D_list)\n",
    "print(Brier_LB_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### compare over 5 seeds: 123, 124, 125, 126, 127\n",
    "Acc_D_all = np.array([\n",
    "    [0.9906, 0.981, 0.9401, 0.8593, 0.7388, 0.6375, 0.5627, 0.5049, 0.4703, 0.4533, 0.4363, 0.4287, 0.4268],\n",
    "    [0.9889, 0.9814, 0.9433, 0.8518, 0.7337, 0.6403, 0.561, 0.5016, 0.4669, 0.4306, 0.4292, 0.4104, 0.4183],\n",
    "    [0.9904, 0.9829, 0.946, 0.8582, 0.747, 0.6495, 0.5602, 0.5075, 0.4684, 0.4437, 0.4334, 0.4245, 0.4217],\n",
    "    [0.9875, 0.9776, 0.9353, 0.8422, 0.7212, 0.6377, 0.55, 0.4896, 0.4497, 0.4328, 0.416, 0.4117, 0.4095],\n",
    "    [0.9894, 0.9816, 0.9451, 0.8522, 0.7403, 0.6391, 0.5565, 0.4994, 0.4656, 0.4308, 0.4284, 0.4221, 0.4152]\n",
    "])\n",
    "\n",
    "Acc_LB_all = np.array([\n",
    "    [0.9907, 0.983, 0.9394, 0.8526, 0.7419, 0.6386, 0.5703, 0.5152, 0.4702, 0.4484, 0.4335, 0.4236, 0.4182],\n",
    "    [0.9889, 0.9804, 0.9471, 0.8547, 0.7411, 0.6296, 0.5624, 0.4938, 0.4609, 0.4404, 0.4295, 0.4172, 0.4152],\n",
    "    [0.9905, 0.9831, 0.9436, 0.8616, 0.7461, 0.6517, 0.5647, 0.5127, 0.4673, 0.4454, 0.4273, 0.4291, 0.415],\n",
    "    [0.9876, 0.9799, 0.9398, 0.851, 0.7352, 0.6368, 0.5445, 0.4992, 0.4632, 0.4269, 0.42, 0.417, 0.4181],\n",
    "    [0.9893, 0.9808, 0.939, 0.8465, 0.7338, 0.6312, 0.5573, 0.505, 0.4539, 0.4458, 0.4158, 0.4189, 0.4145]\n",
    "])\n",
    "\n",
    "Brier_D_all = np.array([\n",
    "    [0.008377700125184932, 0.016529366393931315, 0.05263197122933036, 0.12622978433992207, 0.23428155265648049, 0.33001102154197504, 0.40364649390174306, 0.45833621095510935, 0.48925685879783176, 0.5084873524892697, 0.5265005890140535, 0.5351897862160594, 0.535655688821819],\n",
    "    [0.010744372283432188, 0.01861786528128727, 0.04921392785665237, 0.130553247122029, 0.23708335123859933, 0.3264322724129021, 0.4020700676185214, 0.45858313992431765, 0.49138653020376494, 0.5267105075863001, 0.5302083038117827, 0.5471840382483096, 0.5406172102829678],\n",
    "    [0.008842406659404037, 0.01589756693721347, 0.04813633596723046, 0.12450638545589118, 0.2251983306557004, 0.31596819055291797, 0.39958471677594976, 0.4511044612864316, 0.4908296869743894, 0.513979687459884, 0.5228012975599322, 0.5331324584577392, 0.5366587666968312],\n",
    "    [0.010730850229531219, 0.019535984307980356, 0.054532256239393395, 0.13633893796280153, 0.24614911889753252, 0.32741910714203704, 0.4095507809925442, 0.4699407662727923, 0.5086184210935191, 0.525650285971151, 0.544528220493102, 0.5479489062984142, 0.5516986011718078],\n",
    "    [0.009795412763015064, 0.01817015340220003, 0.04894315094823046, 0.12908701202114847, 0.23282846945381916, 0.3285278649762006, 0.4081318056690933, 0.46182442301821763, 0.4973149243671581, 0.5269344762781906, 0.5322562363481955, 0.5380640556117866, 0.5471252220414157]\n",
    "])\n",
    "\n",
    "Brier_LB_all = np.array([\n",
    "    [0.007844242911062461, 0.015166313465830961, 0.05119071505966292, 0.13220723722716274, 0.23246172960926015, 0.3326357106190896, 0.4017439850542811, 0.45393964088759164, 0.49749242841385766, 0.5211752780472316, 0.5353101111950346, 0.5431750248692432, 0.5509519585301749],\n",
    "    [0.009505463873917114, 0.016971207786010672, 0.0470778754224552, 0.12817579122361247, 0.2349776937700805, 0.339070595213455, 0.40568261118347, 0.4740733457035303, 0.5030623144832805, 0.5253332179429556, 0.5360191577762338, 0.549535112754868, 0.5511099660875561],\n",
    "    [0.0076984171595750415, 0.014756499162356471, 0.0469422462627717, 0.12101585517824154, 0.22767872259093588, 0.31762503643888157, 0.4011728288118217, 0.4516875142101383, 0.4954514924412558, 0.5178717196068767, 0.5371807295796656, 0.5349908080424384, 0.5497019253514567],\n",
    "    [0.01001831719153492, 0.016729483129191477, 0.05172484471829214, 0.13075484635707604, 0.23891760292945455, 0.3334325462508124, 0.42297503706889894, 0.4695227387634461, 0.5033610244309235, 0.5387111053297119, 0.5473753173717428, 0.5506346709903308, 0.5503556560492305],\n",
    "    [0.008755419081030589, 0.01618864623953837, 0.05156303842716825, 0.13417919609729662, 0.23963138466766964, 0.34131713744443276, 0.4110757904444741, 0.46371903749768173, 0.5107108175950072, 0.5222953217647085, 0.5486314035009504, 0.5470332878374046, 0.5531479907833964]   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_D_all_mean = np.mean(Acc_D_all, axis=0)\n",
    "Acc_D_all_std = np.std(Acc_D_all, axis=0)\n",
    "\n",
    "Acc_LB_all_mean = np.mean(Acc_LB_all, axis=0)\n",
    "Acc_LB_all_std = np.std(Acc_LB_all, axis=0)\n",
    "\n",
    "Brier_D_all_mean = np.mean(Brier_D_all, axis=0)\n",
    "Brier_D_all_std = np.std(Brier_D_all, axis=0)\n",
    "\n",
    "Brier_LB_all_mean = np.mean(Brier_LB_all, axis=0)\n",
    "Brier_LB_all_std = np.std(Brier_LB_all, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a figure for accuracy\n",
    "x_labels = ['test', '15$^{\\circ}$', '30$^{\\circ}$', '45$^{\\circ}$', '60$^{\\circ}$', '75$^{\\circ}$', '90$^{\\circ}$',\n",
    "            '105$^{\\circ}$', '120$^{\\circ}$', '135$^{\\circ}$', '150$^{\\circ}$', '165$^{\\circ}$', '180$^{\\circ}$']\n",
    "\n",
    "plt.figure(figsize=(5, 1.5))\n",
    "plt.plot(x_labels, Acc_D_all_mean, color='blue', label='MCMC')\n",
    "plt.plot(x_labels, Acc_LB_all_mean, color='firebrick', label='LB')\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('Accuracy', size=15)\n",
    "#plt.legend()\n",
    "plt.xticks(x_labels, x_labels, rotation='30', size=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/LB_vs_MCMC_Acc.pdf')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a figure for the brier score\n",
    "\n",
    "plt.figure(figsize=(5, 1.5))\n",
    "plt.plot(x_labels, Brier_D_list, color='blue', label='MCMC')\n",
    "plt.plot(x_labels, Brier_LB_list, color='firebrick', label='LB')\n",
    "plt.ylim(0,0.8)\n",
    "plt.ylabel('Brier', size=15)\n",
    "plt.legend()\n",
    "plt.xticks(x_labels, x_labels, rotation='30', size=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/LB_vs_MCMC_Brier.pdf')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a subplot\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(5, 3))\n",
    "\n",
    "ax[0].plot(x_labels, Acc_D_all_mean, color='blue', label='MCMC')\n",
    "ax[0].plot(x_labels, Acc_LB_all_mean, color='firebrick', label='LB')\n",
    "ax[0].set_ylim(0,1)\n",
    "ax[0].set_ylabel('Accuracy', size=15)\n",
    "ax[0].set_xticklabels([])\n",
    "\n",
    "ax[1].plot(x_labels, Brier_D_list, color='blue', label='MCMC')\n",
    "ax[1].plot(x_labels, Brier_LB_list, color='firebrick', label='LB')\n",
    "ax[1].set_ylim(0,0.8)\n",
    "ax[1].set_ylabel('Brier', size=15)\n",
    "ax[1].legend()\n",
    "ax[1].set_xticklabels(x_labels, rotation=30, size=13)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/LB_vs_MCMC_Acc_Brier.pdf')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
