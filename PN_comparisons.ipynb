{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692b151f",
   "metadata": {},
   "source": [
    "# PN networks for comparison with LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30ddcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Python packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy\n",
    "from utils.LB_utils import * \n",
    "from utils.load_not_MNIST import notMNIST\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from laplace import Laplace\n",
    "import utils.scoring as scoring\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# import libraries that we implemented\n",
    "#from utils import data, measures, models, plot, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "616b28e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "cuda status:  True\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cuda_status = torch.cuda.is_available()\n",
    "print(\"device: \", device)\n",
    "print(\"cuda status: \", cuda_status)\n",
    "\n",
    "s = 1\n",
    "np.random.seed(s)\n",
    "torch.manual_seed(s)\n",
    "torch.cuda.manual_seed(s)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ef502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TRAIN_MNIST = 128\n",
    "BATCH_SIZE_TEST_MNIST = 128\n",
    "MAX_ITER_MNIST = 6\n",
    "LR_TRAIN_MNIST = 10e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1dd2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "MNIST_train = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=MNIST_transform)\n",
    "\n",
    "MNIST_train_loader = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_train,\n",
    "    batch_size=BATCH_SIZE_TRAIN_MNIST,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "MNIST_test = torchvision.datasets.MNIST(\n",
    "        '~/data/mnist',\n",
    "        train=False,\n",
    "        download=False,\n",
    "        transform=MNIST_transform)\n",
    "\n",
    "MNIST_test_loader = torch.utils.data.dataloader.DataLoader(\n",
    "    MNIST_test,\n",
    "    batch_size=BATCH_SIZE_TEST_MNIST,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f8cdca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define network\n",
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=10, alpha_0=1.):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.alpha_0 = alpha_0\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.Conv2d(16, 32, 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2,2),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(4 * 4 * 32, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        \n",
    "        assert_no_nan_no_inf(logits)\n",
    "\n",
    "        concentrations = torch.exp(logits) + 1\n",
    "        assert_no_nan_no_inf(concentrations)\n",
    "\n",
    "        mean = concentrations / concentrations.sum(dim=1).unsqueeze(dim=1)\n",
    "        assert_no_nan_no_inf(mean)\n",
    "\n",
    "        precision = torch.sum(concentrations)\n",
    "        assert_no_nan_no_inf(precision)\n",
    "\n",
    "        y_pred = F.softmax(concentrations / self.alpha_0, dim=1)\n",
    "        assert_no_nan_no_inf(y_pred)\n",
    "\n",
    "        model_outputs = {\n",
    "            'logits': logits,\n",
    "            'mean': mean,\n",
    "            'concentrations': concentrations,\n",
    "            'precision': precision,\n",
    "            'y_pred': y_pred\n",
    "        }\n",
    "        return model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6b2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical, Dirichlet\n",
    "from torch.distributions.kl import _kl_dirichlet_dirichlet\n",
    "from torch.nn import NLLLoss\n",
    "from scipy import special\n",
    "\n",
    "\n",
    "def assert_no_nan_no_inf(x):\n",
    "    assert not torch.isnan(x).any()\n",
    "    assert not torch.isinf(x).any()\n",
    "\n",
    "\n",
    "def kl_divergence(model_concentrations,\n",
    "                  target_concentrations,\n",
    "                  mode='reverse'):\n",
    "    \"\"\"\n",
    "    Input: Model concentrations, target concentrations parameters.\n",
    "    Output: Average of the KL between the two Dirichlet.\n",
    "    \"\"\"\n",
    "    assert torch.all(model_concentrations > 0)\n",
    "    assert torch.all(target_concentrations > 0)\n",
    "\n",
    "    target_dirichlet = Dirichlet(target_concentrations)\n",
    "    model_dirichlet = Dirichlet(model_concentrations)\n",
    "    kl_divergences = _kl_dirichlet_dirichlet(\n",
    "        p=target_dirichlet if mode == 'forward' else model_dirichlet,\n",
    "        q=model_dirichlet if mode == 'forward' else target_dirichlet)\n",
    "    assert_no_nan_no_inf(kl_divergences)\n",
    "    mean_kl = torch.mean(kl_divergences)\n",
    "    assert_no_nan_no_inf(mean_kl)\n",
    "    return mean_kl\n",
    "\n",
    "\n",
    "def kl_loss_fn(loss_input,\n",
    "               mode='reverse'):\n",
    "\n",
    "    model_concentrations = loss_input['model_outputs']['concentrations']\n",
    "    target_concentrations = loss_input['y_concentrations_batch']\n",
    "    loss = kl_divergence(\n",
    "        model_concentrations=model_concentrations,\n",
    "        target_concentrations=target_concentrations,\n",
    "        mode=mode)\n",
    "    assert_no_nan_no_inf(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def neg_log_likelihood(input,\n",
    "                       target):\n",
    "\n",
    "    nll_fn = NLLLoss()\n",
    "    nll = nll_fn(input=input, target=target)\n",
    "    assert_no_nan_no_inf(nll)\n",
    "    return nll\n",
    "\n",
    "\n",
    "def nll_loss_fn(loss_inputs):\n",
    "    y_pred_batch = loss_inputs['model_outputs']['y_pred']\n",
    "    y_batch = loss_inputs['y_batch']\n",
    "    loss = neg_log_likelihood(\n",
    "        input=y_pred_batch,\n",
    "        target=y_batch)\n",
    "    assert_no_nan_no_inf(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def entropy_categorical(categorical_parameters):\n",
    "    entropy = Categorical(categorical_parameters).entropy()\n",
    "    # TODO: discuss whether we want numpy in these functions\n",
    "    assert_no_nan_no_inf(entropy)\n",
    "    entropy = entropy.detach().numpy()\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def entropy_dirichlet(dirichlet_concentrations):\n",
    "    entropy = Dirichlet(dirichlet_concentrations).entropy()\n",
    "    # TODO: discuss whether we want numpy in these functions\n",
    "    entropy = entropy.detach().numpy()\n",
    "    assert_no_nan_no_inf(entropy)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def mutual_info_dirichlet(dirichlet_concentrations):\n",
    "    # TODO: discuss whether we want numpy in these functions\n",
    "    dirichlet_concentrations = dirichlet_concentrations.detach().numpy()\n",
    "    dirichlet_concentrations_sum = dirichlet_concentrations.sum()\n",
    "    res = (1.0/dirichlet_concentrations_sum)*dirichlet_concentrations*(np.log(dirichlet_concentrations*1.0/dirichlet_concentrations_sum)-special.digamma(dirichlet_concentrations+1)+special.digamma(dirichlet_concentrations_sum+1))\n",
    "    final_res = res.sum() * (-1.0)\n",
    "    #assert_no_nan_no_inf(final_res)\n",
    "    return final_res\n",
    "\n",
    "\n",
    "def create_loss_fn(loss_fn_str,\n",
    "                   args):\n",
    "\n",
    "    if loss_fn_str == 'nll':\n",
    "        loss_fn = nll_loss_fn\n",
    "    elif loss_fn_str == 'kl':\n",
    "        loss_fn = kl_loss_fn\n",
    "    else:\n",
    "        raise NotImplementedError('Loss function {} not implemented!'.format(loss_fn_str))\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51be41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = ConvNet().to(device)\n",
    "\n",
    "mnist_optimizer = torch.optim.Adam(mnist_model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "MNIST_PATH = \"pretrained_weights/MNIST_pretrained_10_classes_last_layer_PN_s{}.pth\".format(s)\n",
    "loss_fn = create_loss_fn(loss_fn_str='kl', args={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aece238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training routine\n",
    "def get_accuracy(output, targets):\n",
    "    \"\"\"Helper function to print the accuracy\"\"\"\n",
    "    predictions = output.argmax(dim=1, keepdim=True).view_as(targets)\n",
    "    return predictions.eq(targets).float().mean().item()\n",
    "\n",
    "def assert_no_nan_no_inf(x):\n",
    "    assert not torch.isnan(x).any()\n",
    "    assert not torch.isinf(x).any()\n",
    "    \n",
    "def concentrations_from_labels(y, num_classes=10):\n",
    "    len_ = y.size(0)\n",
    "    #baseline 1\n",
    "    base = torch.ones((len_, num_classes))\n",
    "    \n",
    "    #add onehot vectors from labels\n",
    "    onehots = torch.zeros((len_, num_classes))\n",
    "    rows = np.arange(len_)\n",
    "    onehots[rows, y] = 1\n",
    "    return(base + onehots)\n",
    "    \n",
    "\n",
    "def train(model, train_loader, optimizer, max_iter, path, verbose=True):\n",
    "    max_len = len(train_loader)\n",
    "    model.train()\n",
    "\n",
    "    for iter in range(max_iter):\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            model_outputs = model(x)\n",
    "            y_concentrations_batch = concentrations_from_labels(y, num_classes=10).to(device)\n",
    "            #print(y)\n",
    "            #print(y_concentrations_batch)\n",
    "            \n",
    "            loss_inputs = {\n",
    "                'model_outputs': model_outputs,\n",
    "                'x_batch': x,\n",
    "                'y_batch': y,\n",
    "                'y_concentrations_batch': y_concentrations_batch,\n",
    "            }\n",
    "\n",
    "            batch_loss = loss_fn(loss_inputs)\n",
    "            assert_no_nan_no_inf(batch_loss)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            m = nn.Softmax(dim=1)\n",
    "            accuracy = get_accuracy(m(model_outputs[\"logits\"]), y)\n",
    "\n",
    "            if verbose and batch_idx % 50 == 0:\n",
    "                print(\n",
    "                    \"Iteration {}; {}/{} \\t\".format(iter, batch_idx, max_len) +\n",
    "                    \"Accuracy %.0f\" % (accuracy * 100) + \"%\"\n",
    "                )\n",
    "\n",
    "    print(\"saving model at: {}\".format(path))\n",
    "    torch.save(mnist_model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef159eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(mnist_model, MNIST_train_loader, mnist_optimizer, MAX_ITER_MNIST, MNIST_PATH, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ac8c533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from: pretrained_weights/MNIST_pretrained_10_classes_last_layer_PN_s1.pth\n",
      "Batch 0/79 \tAccuracy 99%\n",
      "Batch 10/79 \tAccuracy 98%\n",
      "Batch 20/79 \tAccuracy 98%\n",
      "Batch 30/79 \tAccuracy 95%\n",
      "Batch 40/79 \tAccuracy 100%\n",
      "Batch 50/79 \tAccuracy 99%\n",
      "Batch 60/79 \tAccuracy 100%\n",
      "Batch 70/79 \tAccuracy 96%\n",
      "overall test accuracy on MNIST: 98.56 %\n"
     ]
    }
   ],
   "source": [
    "#predict in distribution\n",
    "MNIST_PATH = \"pretrained_weights/MNIST_pretrained_10_classes_last_layer_PN_s{}.pth\".format(s)\n",
    "\n",
    "mnist_model = ConvNet().to(device)\n",
    "print(\"loading model from: {}\".format(MNIST_PATH))\n",
    "mnist_model.load_state_dict(torch.load(MNIST_PATH))\n",
    "mnist_model.eval()\n",
    "\n",
    "acc = []\n",
    "\n",
    "max_len = len(MNIST_test_loader)\n",
    "for batch_idx, (x, y) in enumerate(MNIST_test_loader):\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    model_outputs = mnist_model(x)\n",
    "\n",
    "    m = nn.Softmax(dim=1)\n",
    "    accuracy = get_accuracy(m(model_outputs[\"logits\"]), y)\n",
    "    \n",
    "    if batch_idx % 10 == 0:\n",
    "        print(\n",
    "            \"Batch {}/{} \\t\".format(batch_idx, max_len) + \n",
    "            \"Accuracy %.0f\" % (accuracy * 100) + \"%\"\n",
    "        )\n",
    "    acc.append(accuracy)\n",
    "\n",
    "avg_acc = np.mean(acc)\n",
    "print('overall test accuracy on MNIST: {:.02f} %'.format(avg_acc * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2b87cd",
   "metadata": {},
   "source": [
    "### OOD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "495804d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TEST_FMNIST = 128\n",
    "BATCH_SIZE_TEST_KMNIST = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73cdfd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "FMNIST_test = torchvision.datasets.FashionMNIST(\n",
    "        '~/data/fmnist', train=False, download=True,\n",
    "        transform=MNIST_transform)   #torchvision.transforms.ToTensor())\n",
    "\n",
    "FMNIST_test_loader = torch.utils.data.DataLoader(\n",
    "    FMNIST_test,\n",
    "    batch_size=BATCH_SIZE_TEST_FMNIST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8dae771",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMNIST_test = torchvision.datasets.KMNIST(\n",
    "        '~/data/kmnist', train=False, download=True,\n",
    "        transform=MNIST_transform)\n",
    "\n",
    "KMNIST_test_loader = torch.utils.data.DataLoader(\n",
    "    KMNIST_test,\n",
    "    batch_size=BATCH_SIZE_TEST_KMNIST, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94d4313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png is broken\n",
      "File A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png is broken\n"
     ]
    }
   ],
   "source": [
    "#root = os.path.abspath('~/data')\n",
    "root = os.path.expanduser('~/data')\n",
    "\n",
    "# Instantiating the notMNIST dataset class we created\n",
    "notMNIST_test = notMNIST(root=os.path.join(root, 'notMNIST_small'),\n",
    "                               transform=MNIST_transform)\n",
    "\n",
    "# Creating a dataloader\n",
    "notMNIST_test_loader = torch.utils.data.dataloader.DataLoader(\n",
    "                            dataset=notMNIST_test,\n",
    "                            batch_size=BATCH_SIZE_TEST_KMNIST,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a209e",
   "metadata": {},
   "source": [
    "# Predictions for PN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e563fd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = MNIST_test.targets.numpy()\n",
    "targets_FMNIST = FMNIST_test.targets.numpy()\n",
    "targets_notMNIST = notMNIST_test.targets.numpy().astype(int)\n",
    "targets_KMNIST = KMNIST_test.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89451702",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_PN(model, test_loader, device='cuda'):\n",
    "    py = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        py_ = torch.softmax(model(x)[\"logits\"], 1)\n",
    "\n",
    "        py.append(py_)\n",
    "    return torch.cat(py, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69dd4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_test_in_PN = predict_PN(mnist_model, MNIST_test_loader, device=device).cpu().numpy()\n",
    "MNIST_test_out_FMNIST_PN = predict_PN(mnist_model, FMNIST_test_loader, device=device).cpu().numpy()\n",
    "MNIST_test_out_notMNIST_PN = predict_PN(mnist_model, notMNIST_test_loader, device=device).cpu().numpy()\n",
    "MNIST_test_out_KMNIST_PN = predict_PN(mnist_model, KMNIST_test_loader, device=device).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b7dcd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24606139957904816\n",
      "2.659008264541626\n",
      "2.962023973464966\n",
      "3.141713857650757\n"
     ]
    }
   ],
   "source": [
    "# compute average log-likelihood for Diag\n",
    "MNIST_LLH_in_PN = -torch.distributions.Categorical(torch.tensor(MNIST_test_in_PN)).log_prob(torch.tensor(targets)).mean().item()\n",
    "MNIST_LLH_out_FMNIST_PN = -torch.distributions.Categorical(torch.tensor(MNIST_test_out_FMNIST_PN)).log_prob(torch.tensor(targets_FMNIST)).mean().item()\n",
    "MNIST_LLH_out_notMNIST_PN = -torch.distributions.Categorical(torch.tensor(MNIST_test_out_notMNIST_PN)).log_prob(torch.tensor(targets_notMNIST)).mean().item()\n",
    "MNIST_LLH_out_KMNIST_PN = -torch.distributions.Categorical(torch.tensor(MNIST_test_out_KMNIST_PN)).log_prob(torch.tensor(targets_KMNIST)).mean().item()\n",
    "\n",
    "print(MNIST_LLH_in_PN)\n",
    "print(MNIST_LLH_out_FMNIST_PN)\n",
    "print(MNIST_LLH_out_notMNIST_PN)\n",
    "print(MNIST_LLH_out_KMNIST_PN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b58834e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18369689898989894\n",
      "0.21209345454545458\n",
      "0.3142469341999357\n",
      "0.26133215151515155\n"
     ]
    }
   ],
   "source": [
    "#compute the Expected confidence estimate\n",
    "MNIST_ECE_in_PN = scoring.expected_calibration_error(targets, MNIST_test_in_PN)\n",
    "MNIST_ECE_out_FMNIST_PN = scoring.expected_calibration_error(targets_FMNIST, MNIST_test_out_FMNIST_PN)\n",
    "MNIST_ECE_out_notMNIST_PN = scoring.expected_calibration_error(targets_notMNIST, MNIST_test_out_notMNIST_PN)\n",
    "MNIST_ECE_out_KMNIST_PN = scoring.expected_calibration_error(targets_KMNIST, MNIST_test_out_KMNIST_PN)\n",
    "print(MNIST_ECE_in_PN)\n",
    "print(MNIST_ECE_out_FMNIST_PN)\n",
    "print(MNIST_ECE_out_notMNIST_PN)\n",
    "print(MNIST_ECE_out_KMNIST_PN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca596b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007529875263571739\n",
      "0.09756391495466232\n",
      "0.10524796694517136\n",
      "0.10414869338274002\n"
     ]
    }
   ],
   "source": [
    "## Brier score\n",
    "MNIST_brier_in_PN = get_brier(MNIST_test_in_PN, targets, n_classes=10)\n",
    "MNIST_brier_out_FMNIST_PN = get_brier(MNIST_test_out_FMNIST_PN, targets_FMNIST, n_classes=10)\n",
    "MNIST_brier_out_notMNIST_PN = get_brier(MNIST_test_out_notMNIST_PN, targets_notMNIST, n_classes=10)\n",
    "MNIST_brier_out_KMNIST_PN = get_brier(MNIST_test_out_KMNIST_PN, targets_KMNIST, n_classes=10)\n",
    "print(MNIST_brier_in_PN)\n",
    "print(MNIST_brier_out_FMNIST_PN)\n",
    "print(MNIST_brier_out_notMNIST_PN)\n",
    "print(MNIST_brier_out_KMNIST_PN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "770dff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_in_PN, prob_correct_in_PN, ent_in_PN, MMC_in_PN = get_in_dist_values(MNIST_test_in_PN, targets)\n",
    "acc_out_FMNIST_PN, prob_correct_out_FMNIST_PN, ent_out_FMNIST_PN, MMC_out_FMNIST_PN, auroc_out_FMNIST_PN = get_out_dist_values(MNIST_test_in_PN, MNIST_test_out_FMNIST_PN, targets_FMNIST)\n",
    "acc_out_notMNIST_PN, prob_correct_out_notMNIST_PN, ent_out_notMNIST_PN, MMC_out_notMNIST_PN, auroc_out_notMNIST_PN = get_out_dist_values(MNIST_test_in_PN, MNIST_test_out_notMNIST_PN, targets_notMNIST)\n",
    "acc_out_KMNIST_PN, prob_correct_out_KMNIST_PN, ent_out_KMNIST_PN, MMC_out_KMNIST_PN, auroc_out_KMNIST_PN = get_out_dist_values(MNIST_test_in_PN, MNIST_test_out_KMNIST_PN, targets_KMNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fac4844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[In, PN, MNIST] Accuracy: 0.985; average entropy: 0.785;     MMC: 0.802; Prob @ correct: 0.100\n",
      "[Out-FMNIST, PN, MNIST] Accuracy: 0.063; Average entropy: 2.014;    MMC: 0.273; AUROC: 0.995; Prob @ correct: 0.100\n",
      "[Out-notMNIST, PN, MNIST] Accuracy: 0.164; Average entropy: 1.600;    MMC: 0.447; AUROC: 0.938; Prob @ correct: 0.100\n",
      "[Out-KMNIST, PN, MNIST] Accuracy: 0.111; Average entropy: 1.757;    MMC: 0.372; AUROC: 0.976; Prob @ correct: 0.100\n"
     ]
    }
   ],
   "source": [
    "print_in_dist_values(acc_in_PN, prob_correct_in_PN, ent_in_PN, MMC_in_PN, 'MNIST', 'PN')\n",
    "print_out_dist_values(acc_out_FMNIST_PN, prob_correct_out_FMNIST_PN, ent_out_FMNIST_PN, MMC_out_FMNIST_PN, auroc_out_FMNIST_PN, 'MNIST', test='FMNIST', method='PN')\n",
    "print_out_dist_values(acc_out_notMNIST_PN, prob_correct_out_notMNIST_PN, ent_out_notMNIST_PN, MMC_out_notMNIST_PN, auroc_out_notMNIST_PN, 'MNIST', test='notMNIST', method='PN')\n",
    "print_out_dist_values(acc_out_KMNIST_PN, prob_correct_out_KMNIST_PN, ent_out_KMNIST_PN, MMC_out_KMNIST_PN, auroc_out_KMNIST_PN, 'MNIST', test='KMNIST', method='PN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c252e33",
   "metadata": {},
   "source": [
    "### create a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c138253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c906c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "MMC_PN = [MMC_in_PN, MMC_out_FMNIST_PN, MMC_out_notMNIST_PN, MMC_out_KMNIST_PN]\n",
    "AUROC_PN = [0, auroc_out_FMNIST_PN, auroc_out_notMNIST_PN, auroc_out_KMNIST_PN]\n",
    "ECE_PN = [MNIST_ECE_in_PN, MNIST_ECE_out_FMNIST_PN, MNIST_ECE_out_notMNIST_PN, MNIST_ECE_out_KMNIST_PN]\n",
    "LLH_PN = [MNIST_LLH_in_PN, MNIST_LLH_out_FMNIST_PN, MNIST_LLH_out_notMNIST_PN, MNIST_LLH_out_KMNIST_PN]\n",
    "Brier_PN = [MNIST_brier_in_PN, MNIST_brier_out_FMNIST_PN, MNIST_brier_out_notMNIST_PN, MNIST_brier_out_KMNIST_PN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27ad2e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PN = pd.DataFrame({\n",
    "    \"MMC\":MMC_PN,\n",
    "    \"AUROC\":AUROC_PN,\n",
    "    \"ECE\":ECE_PN,\n",
    "    \"LLH\":LLH_PN,\n",
    "    \"Brier\":Brier_PN\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dc347c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrrr}\n",
      "\\toprule\n",
      "  MMC &  AUROC &   ECE &   LLH &  Brier \\\\\n",
      "\\midrule\n",
      "0.802 &  0.000 & 0.184 & 0.246 &  0.008 \\\\\n",
      "0.273 &  0.995 & 0.212 & 2.659 &  0.098 \\\\\n",
      "0.447 &  0.938 & 0.314 & 2.962 &  0.105 \\\\\n",
      "0.372 &  0.976 & 0.261 & 3.142 &  0.104 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_PN.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3d5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
